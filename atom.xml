<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>沉潜飞动</title>
  
  <subtitle>君子藏器于身，待时而动。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.howardliu.cn/"/>
  <updated>2019-11-09T12:10:55.962Z</updated>
  <id>http://www.howardliu.cn/</id>
  
  <author>
    <name>Howard Liu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>微服务编程范式</title>
    <link href="http://www.howardliu.cn/the-paradigm-of-microservice/"/>
    <id>http://www.howardliu.cn/the-paradigm-of-microservice/</id>
    <published>2019-11-09T09:12:21.000Z</published>
    <updated>2019-11-09T12:10:55.962Z</updated>
    
    <content type="html"><![CDATA[<p>目前很多互联网公司都采用微服务架构，微服务的优点和缺点被反复说到，这里不在重复赘述，只结合工作中的一些实践，说说要用微服务要注意的点，厚颜写做编程范式，其实就是一些具体实践而已。</p><a id="more"></a><h1 id="原则（道）"><a href="#原则（道）" class="headerlink" title="原则（道）"></a>原则（道）</h1><p>原则是比较抽象的一个概念，简单说是一些指导意见，在不同的组之间可以共享，是为了实现一个共同的目的，必须准守的一些规则，或者叫做规矩。</p><p>这些规则（或规矩）对我们开发过程中有一定的约束作用，不容置疑，必须遵守。如果发现有那个团队或者个人没有准守，一定要及时纠正，否则，原则就没有任何存在的意义。</p><p>现在用的很广的一个原则是Heroku的 <strong><a href="http://www.12factor.net" target="_blank" rel="noopener">12-Factor</a></strong> 原则。具体内容如下：</p><blockquote><p>I. 基准代码<br>一份基准代码，多份部署<br>II. 依赖<br>显式声明依赖关系<br>III. 配置<br>在环境中存储配置<br>IV. 后端服务<br>把后端服务当作附加资源<br>V. 构建，发布，运行<br>严格分离构建和运行<br>VI. 进程<br>以一个或多个无状态进程运行应用<br>VII. 端口绑定<br>通过端口绑定提供服务<br>VIII. 并发<br>通过进程模型进行扩展<br>IX. 易处理<br>快速启动和优雅终止可最大化健壮性<br>X. 开发环境与线上环境等价<br>尽可能的保持开发，预发布，线上环境相同<br>XI. 日志<br>把日志当作事件流<br>XII. 管理进程<br>后台管理任务当作一次性进程运行</p></blockquote><p>在我们团队中有一些原则可以和大家分享：</p><ol><li>不要为了用而用，程序猿（或者叫攻城狮）是最喜欢尝鲜的人，有了新技术出来，就想用。这无疑是优点，但是在工作中这样，就有可能给系统带来灾难。所以如果想用某项技术，必须充分调研之后，经过一系列的验证，才能引入。</li><li>战略目标明确，业务部门愿景清晰。作为开发团队，可能很少关心业务团队的想法，这是很致命的。我们开发的东西，不是业务部门想要的，顾客想要一个吃饭的勺子，你给做了一个铁铲，铁铲做的再好又有什么用。</li><li>服务之间调用必须使用HTTP/RESTful方案，不能引入其他方案。不是说其他方案不好，而是最好协议一致，一致的协议能够减少系统的复杂性和沟通成本。</li></ol><h1 id="标准（术）"><a href="#标准（术）" class="headerlink" title="标准（术）"></a>标准（术）</h1><p>标准的定义会比原则更加具体，有点类似于道和术、战略和战术的关系，不同的技术栈、不同的团队可能会制定不同的标准。</p><ol><li>我们团队都是使用的是Java技术栈，所以标准大体上采用的是<a href="http://dd.ma/m5R1zqRj" target="_blank" rel="noopener">《阿里巴巴Java开发手册》</a>，有一部分内容作了修改，这里对孤尽表示感谢。这个手册脱胎于《Effective Java》和《代码简洁之道》，其中加入了孤尽在阿里的一些实践，所以对于Java栈的同学是比较适用的。</li><li>我们使用的是Spring Cloud，服务之间的调用，必须使用Feign Client形式，指定这个标准是为了以后使用K8s时改动最小。</li><li>页面与服务之间的调用，HTTP返回状态码都是200，在返回的数据中，定义具体的状态码，这个状态码参照HTTP状态码，同时定义一个子级状态码，用来具体定义业务情况。比如，状态码等于500标识服务异常，子级状态码等于5001，表示操作数据库异常等。</li><li>监控系统、日志系统、任务调度等，如果需要，也要指定明确是标准。比如打印日志时，日志开头必须以6位数字开头，因为我们的日志系统与监控系统对接，6位数字能够定位到不同的系统、模块、业务，直接定位问题位置。</li><li>不一而足（每个团队有每个团队具体的标准，这里就不一一列举了）。。。</li></ol><p>这些标准，最好形成文档，放在知识库中。这样，团队的成员可以随时查看，有新人加入时，也能避免老员工口口相传，传错了指令。</p><blockquote><p>有些架构师认为，原则和标准就是一个东西。就我来看，这两个粒度不同，对于大的团队，最好分开。因为大的团队，技术栈更加多样化，标准没有办法做到一致，但是原则可以做到不会脱离大框。对于小的团队，因为技术栈比较单一，有可能就是一个技术栈（就像我现在的团队），因为标准只有一个，就是对原则的细化，所以两者就是一个东西。</p></blockquote><p>标准的另外一个作用，就是告诉团队成员怎么做是对的，怎么做是更好的方案，更加直白的表述是，按照标准进行开发，bug更少。</p><p>有了开发标准之后，就需要将标准推广到所有人，但是每个人的理解力和执行力是有偏差的，所以需要一些手段来快速推广。具体的方法有：DEMO（示例）、代码模板（脚手架）。</p><h2 id="DEMO（示例）"><a href="#DEMO（示例）" class="headerlink" title="DEMO（示例）"></a>DEMO（示例）</h2><p>软件开发是一个比较有技术壁垒的行业，一个新人（没有经验或者有经验刚加入新团队的人）想要快速了解老系统所使用的标准，是比较困难的，毕竟每家团队采用的标准都不是百分之百一致。比较简单的做法就是，提供新人DEMO示例，然后告诉他，“就照着这个做”。</p><blockquote><p>对于有经验的新人，一定是先接受，然后在了解清楚之后，才会提出自己的一些看法，而不是一上来，就搬出自己以前的经验，全盘否定团队指定的标准。</p></blockquote><h2 id="代码模板（脚手架）"><a href="#代码模板（脚手架）" class="headerlink" title="代码模板（脚手架）"></a>代码模板（脚手架）</h2><p>代码模板的作用实现一个服务的集成方案，经过有效可靠的裁剪和定制。在需要新建服务时，就使用这个方案，直接进行业务代码开发即可，所以也被称为脚手架，比如SpringBoot的Starter和AutoConfiguration。</p><p>前面说过，我们团队使用的是Java技术栈，基于SpringCloud开发，所以我们对SpringCloud进行封装，定义了几根通用的组件，比如定义了一个<code>web-misc</code>组件，引入这个组件，就能够实现，引入实现WebMvc，并且提供了统一的获取Metric信息的接口，统一的异常处理的ExceptionHandler等。</p><h1 id="及时清理技术债务"><a href="#及时清理技术债务" class="headerlink" title="及时清理技术债务"></a>及时清理技术债务</h1><p>虽然我们都是代码洁癖，但是有时候迫于时间压力、业务压力，我们不得不背负一些技术债务。</p><p>比如我们知道硬编码会破坏系统灵活性，但是明天就要上线，根本没有时间做配置型服务，所以只能硬编码到系统中，这就是技术债务。第二天系统上线，正常运行，如果这个时候把硬编码事情抛之脑后，这个债务就会产生利息，不知道哪天就会变成炸弹。</p><p>对于技术债务，我们团队的做法是做一个技术债务清单，设置超时提醒功能，限期修复。这个清单是公开的，每一项都注明了，是谁，因为什么，于什么时间，产生的技术债务，需要在什么时候修复。如果是临时特性或功能产生的债务，也需要注明特性或功能过期时间，由专人检查债务是否已经没有了。</p><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a><br>个人博文: <a href="http://www.howardliu.cn/the-paradigm-of-microservice">微服务编程范式</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="https://blog.csdn.net/liuxinghao/article/details/102990899" target="_blank" rel="noopener">微服务编程范式</a></p>]]></content>
    
    <summary type="html">
    
      目前很多互联网公司都采用微服务架构，微服务的优点和缺点被反复说到，这里不在重复赘述，只结合工作中的一些实践，说说要用微服务要注意的点，厚颜写做编程范式，其实就是一些技巧而已。
    
    </summary>
    
    
      <category term="microservice" scheme="http://www.howardliu.cn/categories/microservice/"/>
    
    
      <category term="微服务" scheme="http://www.howardliu.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="方法论" scheme="http://www.howardliu.cn/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"/>
    
      <category term="范式" scheme="http://www.howardliu.cn/tags/%E8%8C%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>源码安装NGINX</title>
    <link href="http://www.howardliu.cn/build-nginx-from-sources/"/>
    <id>http://www.howardliu.cn/build-nginx-from-sources/</id>
    <published>2019-11-09T05:21:01.000Z</published>
    <updated>2019-11-09T06:41:21.414Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录一次从源码安装Nginx过程，参考的是<a href="http://nginx.org/en/docs/configure.html" target="_blank" rel="noopener">Nginx官网</a>。</p><a id="more"></a><p>安装过程比较简单，就是下载源码包，下载依赖包，打包编译安装就完事了。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><ol><li>安装依赖包<pre><code class="shell">yum -y install gcc automake autoconf libtool makeyum -y install openssl openssl-devel</code></pre>因为下面的安装过程会打包编译https模块，依赖于openssl，所以需要安装这个依赖，否则会出现<code>./configure: error: SSL modules require the OpenSSL library.</code>的异常。</li><li>下载源码包<br>直接从<a href="http://nginx.org/download/nginx-1.17.5.tar.gz" target="_blank" rel="noopener">官网</a>下载安装包即可</li><li>下载依赖包<br>需要的依赖<code>pcre</code>和<code>zlib</code>，从各自的官网下载即可：<a href="http://www.pcre.org/" target="_blank" rel="noopener">PCRE</a>，<a href="https://ftp.pcre.org/pub/pcre/pcre-8.41.tar.gz" target="_blank" rel="noopener">下载地址</a>，<a href="http://zlib.net/" target="_blank" rel="noopener">zlib</a>，<a href="http://zlib.net/zlib-1.2.11.tar.gz" target="_blank" rel="noopener">下载地址</a>。</li><li>配置<pre><code class="shell"> # 进入nginx源码目录中 cd /path/to/nginx-source-directory ./configure \     --sbin-path=/usr/local/nginx/sbin/nginx \     --conf-path=/usr/local/nginx/conf/nginx.conf \     --pid-path=/usr/local/nginx/nginx.pid \     --with-http_ssl_module -\     -with-pcre=../pcre-8.41 \     --with-zlib=../zlib-1.2.11</code></pre></li><li>编译<pre><code class="shell"># 进入nginx源码目录中cd /path/to/nginx-source-directorymake</code></pre></li><li>安装<pre><code class="shell"># 进入nginx源码目录中cd /path/to/nginx-source-directorymake install</code></pre></li></ol><p>到这里Nginx就安装完成了，下面给出一些nginx配置的建议。</p><h1 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h1><p>nginx安装完之后，就需要进行一些配置，下面是我的一些建议。</p><ol><li><p>目录结构</p><p>在<code>conf</code>目录中，创建<code>vhosts</code>和<code>upstreams</code>目录，两个目录分别存储<code>server</code>和<code>upstream</code>的定义。在<code>nginx.conf</code>中添加下面的代码引入配置：</p><pre><code class="conf"> include upstreams/*.conf; include vhosts/*.conf;</code></pre></li><li><p>文件命名</p><p>在<code>vhosts</code>定义文件格式为<code>*.vhost.conf</code>，如果监听服务是域名，以域名倒置格式命名，比如：<code>cn.howardliu.www.vhost.conf</code>，这样的好处是，相同一级域名，不同二级域名的配置文件，在文件列表展示的时候，会在一起，比较好区分隔离。</p><p><code>upstreams</code>定义文件格式为<code>*.upstream.conf</code>，以服务模块名进行区分，好处是能够在一个文件中定义相同服务模块的内容，进行服务或模块隔离，或者以<code>服务.模块.upstream.conf</code>的格式，但是这样的坏处是，比较散，文件比较多。</p></li><li><p>建议显示生命链接超时时间</p><p>在<code>nginx.conf</code>的<code>http</code>中定义超时时间，下面是我测试环境的一个定义，需要根据自己的情况：</p><pre><code class="conf"> fastcgi_connect_timeout 300; fastcgi_read_timeout 300; fastcgi_send_timeout 300; proxy_connect_timeout 300s; proxy_read_timeout 300s; proxy_send_timeout 300s;</code></pre></li></ol><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a><br>个人博文: <a href="http://www.howardliu.cn/build-nginx-from-sources">源码安装NGINX</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="https://blog.csdn.net/conansix/article/details/102986884" target="_blank" rel="noopener">源码安装NGINX</a></p>]]></content>
    
    <summary type="html">
    
      本文主要记录一次从源码安装Nginx过程，参考的是[Nginx官网](http://nginx.org/en/docs/configure.html)。
    
    </summary>
    
    
      <category term="devops" scheme="http://www.howardliu.cn/categories/devops/"/>
    
    
      <category term="DevOps" scheme="http://www.howardliu.cn/tags/DevOps/"/>
    
      <category term="部署" scheme="http://www.howardliu.cn/tags/%E9%83%A8%E7%BD%B2/"/>
    
      <category term="Nginx" scheme="http://www.howardliu.cn/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title>瞎说八道之更换手机的成本</title>
    <link href="http://www.howardliu.cn/the-cost-to-change-phone/"/>
    <id>http://www.howardliu.cn/the-cost-to-change-phone/</id>
    <published>2019-11-09T05:13:00.000Z</published>
    <updated>2019-11-09T06:51:09.844Z</updated>
    
    <content type="html"><![CDATA[<p>现在手机越来越便宜，换手机是比较常见的一件事，所以各大厂商为了降低更换手机的成本，也是各种手段费尽心思：一键换机、云账号。。。但是这些方式都建立在一个基础上，就是同品牌手机才能用。如果是跨品牌换机，那真是要经历九九八十一难，百转千回才能顺利使用新机。像我这种懒人，可能还要在很长一段时间继续使用旧手机。</p><a id="more"></a><p>特意查了一下，成本有很多定义：</p><blockquote><p>1、成本是生产和销售一定种类与数量产品以耗费资源用货币计量的经济价值。企业进行产品生产需要消耗生产资料和劳动力，这些消耗在成本中用货币计量，就表现为材料费用、折旧费用、工资费用等。企业的经营活动不仅包括生产，也包括销售活动，因此在销售活动中所发生的费用，也应计入成本。同时，为了管理生产所发生的费用，也应计入成本。同时，为了管理生产经营活动所发生的费用也具有形成成本的性质。</p><p>2、成本是为取得物质资源所需付出的经济价值。企业为进行生产经营活动，购置各种生产资料或采购商品，而<br>支付的价款和费用，就是购置成本或采购成本。随着生产经营活动的不断进行，这些成本就转化为生产成本和销售成本。</p><p>3、成本是为达到一定目的而付出或应付出资源的价值牺牲，它可用货币单位加以计量。</p><p>4、成本是为达到一种目的而放弃另一种目的所牺牲的经济价值。</p><p>。。。</p></blockquote><p>适用于更换手机的成本，应该属于第三种：<strong>成本是为达到一定目的而付出或应付出资源的价值牺牲，它可用货币单位加以计量。</strong></p><p>首先，我们更换手机的场景都有哪些呢？</p><p>因为我从诺基亚的砸核桃手机之后，用的一直是小米，所以对小米手机是比较了解的，下面就以小米手机来举例子。</p><ol><li>同品牌同款手机更新换代，比如从小米1到小米3</li><li>同品牌不同款手机更换，比如从红米手机到小米手机</li><li>相同操作系统不同品牌手机更换，比如从华为到小米</li><li>不同操作系统手机更换，比如从Android到IOS或从IOS到Android</li></ol><p>第一、二种情况，对于一家好的厂商，应该都能做到几乎0成本换机，只要登录账号，或者扫描二维码，连上网，手机放在边上，等待一段时间就可以使用新机了。这点小米的MIUI做的很好，不光是通讯录，包括照片、APP，甚至是桌面配置都是直接恢复。</p><p>第三种情况，就看不同厂商是否对自己有足够的自信了，目前来看，花费的成本和第四种几乎一样。</p><p>第四种情况，换机成本几乎是相当于最大。需要重新导入通讯录，重新安装APP。最痛苦的无疑是安装APP的过程，需要对照旧手机的APP列表，一个一个从应用商店找到，然后安装。当然，也可以选择用到的时候再安装，可能出现的情况就是，着急用了，才发现手机上没有这个应用，然后安装，安装好之后发现忘记账号密码了，还得再找回密码。如果是有好几个手机号的时候，可能连哪个手机号注册的都忘了。</p><p>这里不得不说一下，苹果提供了一个叫做Move To IOS的应用，功能是从Android安装之后，直接将Android机的数据拷贝到苹果机。想法很好，奈何，奈何，Android的移动市场都搜不到这个应用，从苹果官网下载后，装上了，结果发现根本没法用。</p><p>这些都是明面上的一些成本，还有隐形的成本。比如，我最近都在使用极客时间，旧手机中下载了很多视频课程，但是再更换新手机的过程中发现，新手机还得再下载一次。幸好现在宽带普及度很高，流量也是好几十G，要是以前，这些时间成本就都是需要转换成金钱成本了。</p><p>深思一下，既然第一、二种更换手机的场景，换机成本几乎为0，为什么第三、四种，成本就会无限大。如果说第四种情况，跨越操作系统了，成本大还能够接收，但是第三种情况，都是一样的操作系统，为什么不能一键换机呢？（接下来就是我的胡说八道的时间了）</p><p><strong>原因无非就是这些厂商不够自信。</strong> 他们心理活动应该是这个样子的，万一给用户提供了一键换机，让他们发现别人的手机更好用怎么办？这么轻松就能够换机，换过去不回来了，那我岂不是损失了一个客户，就让他们在我的这个圈子里玩吧。</p><p>在云应用的领域，有一个协议叫做OpenTracing，这个协议是规定了调用链数据格式，把各个调用链监控服务（比如Zipkin、TraceView、SkyWalking、DataDog、Jaeger等）接口数据做了统一定义。只要遵守协议了，就能像乐高一样，可以实现一定程度的自由组合，随意搭建自己的调用链监控服务。这样，原来各自为政的项目，就能够让用户自由选择，更换成本几乎为0。</p><p>从历史上看，所有的事物最终都会 <strong>趋于一致，又各有千秋</strong>。比如，秦始皇统一度量衡、书同文、车同轨，这是打破了各方诸侯国文化不同的屏障，减少隔阂。大家可以自由的交流。但是不是说秦老大的书同文政策一出，各方人民就都被格式化成相同的了，各自还是有自己的特色。车同轨了，但是车的外观和内饰还是各有不同。</p><p>那是不是可以这样展望，在之后的几年间，各大厂商意识到换机的高昂成本，达成一致，推出一项一键换机的服务，在接入这个服务的厂商之间更换手机，无论什么品牌、什么型号、什么系统，都能够0成本换机，包括通讯录、文件、应用、铃声等等。然后，各大厂商各自发展自己的特色服务，专心种植自己的梧桐树，提升用户体验。做好这些，剩下的就交给市场。</p><p>如果真能做到，那就是手机领域的天下大同社会了，岂不美哉。。。</p><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a><br>个人博文: <a href="http://www.howardliu.cn/the-cost-to-change-phone">瞎说八道之更换手机的成本</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="https://blog.csdn.net/conansix/article/details/102986989" target="_blank" rel="noopener">瞎说八道之更换手机的成本</a></p>]]></content>
    
    <summary type="html">
    
      现在手机越来越便宜，换手机是比较常见的一件事，所以各大厂商为了降低更换手机的成本，也是各种手段费尽心思：一键换机、云账号。。。但是这些方式都建立在一个基础上，就是同品牌手机才能用。如果是跨品牌换机，那真是要经历九九八十一难，百转千回才能顺利使用新机。像我这种懒人，可能还要在很长一段时间继续使用旧手机。
    
    </summary>
    
    
      <category term="raving" scheme="http://www.howardliu.cn/categories/raving/"/>
    
    
      <category term="瞎说八道" scheme="http://www.howardliu.cn/tags/%E7%9E%8E%E8%AF%B4%E5%85%AB%E9%81%93/"/>
    
      <category term="瞎想" scheme="http://www.howardliu.cn/tags/%E7%9E%8E%E6%83%B3/"/>
    
      <category term="成本" scheme="http://www.howardliu.cn/tags/%E6%88%90%E6%9C%AC/"/>
    
      <category term="生活" scheme="http://www.howardliu.cn/tags/%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
  <entry>
    <title>蓝绿部署、金丝雀发布（灰度发布）、AB测试</title>
    <link href="http://www.howardliu.cn/blue-green-deployments-a-b-testing-and-canary-releases/"/>
    <id>http://www.howardliu.cn/blue-green-deployments-a-b-testing-and-canary-releases/</id>
    <published>2019-10-26T09:00:00.000Z</published>
    <updated>2019-11-09T06:44:36.182Z</updated>
    
    <content type="html"><![CDATA[<p>随着微服务架构的普及，线上服务越来越多，随之而来的就是部署越来越频繁；随着互联网行业的兴旺，产品迭代的频率也是越来越快，服务上线速度逐步提升。有上线、有部署，就有风险。有风险，就对业务有影响，然后就有了一系列减少这种风险的部署方案：蓝绿部署、金丝雀发布（灰度发布），也有适应产品迭代频率的AB测试。</p><p>本文主要是简单解释下这几个概念，帮助自己理解，如果有错误，请大佬们斧正。</p><a id="more"></a><h1 id="蓝绿部署"><a href="#蓝绿部署" class="headerlink" title="蓝绿部署"></a>蓝绿部署</h1><p>蓝绿色部署是一种通过运行两个相同的称为 BLUE 和 GREEN 的生产环境来减少停机时间和降低风险的技术。</p><p>蓝绿部署，以颜色命名，简单的理解就是，线上有两套集群环境，在架构图中，一套标记成蓝色，称为蓝色集群BLUE；一套标记为绿色，称为绿色集群GREEN。通过将流量引入两个集群，完成系统升级切换。</p><p><img src="http://www.howardliu.cn/images/op/blue_green_deployments.png" alt="blue green deployments"></p><p>步骤一：部署绿色集群，这个时候是初始状态，蓝色集群承担全部责任，接收全部流量，等待被替换。绿色集群刚刚部署，还没有投入使用，流量为0，等待验证和上线。</p><p>步骤二：蓝色集群流量不变，向绿色集群引入流量。这个过程可以分成几个阶段完成。第一个阶段，引入少量非实时流量，仅用于数据测试；第二个阶段，引入全部实时流量，用于做系统验证。</p><p>步骤三：切断向蓝色集群引入流量，将全部流量引入绿色集群。这个时候，绿色集群已经承担全部责任，接收全部流量。这个过程也可以分阶段操作。第一个阶段，平衡蓝色和绿色集群流量，也就是蓝色和绿色集群一同承担职责；第二个阶段，切断蓝色集群流量，流量全部写入绿色集群。是否采用分阶段操作，完全看升级的功能是否是破坏性的，是否可兼容。</p><p>步骤四：监控系统运行，这个过程是必要的。因为没有人能够保证测试时100%的覆盖的，所以新集群可能会出现这样那样、或大或小的问题，如果评估需要回滚，就需要将全部流量切换到蓝色集群。也完成了版本回滚。</p><h1 id="金丝雀发布（灰度发布）"><a href="#金丝雀发布（灰度发布）" class="headerlink" title="金丝雀发布（灰度发布）"></a>金丝雀发布（灰度发布）</h1><p>金丝雀发布，与蓝绿部署不同的是，它不是非黑即白的部署方式，所以又称为灰度发布。它能够缓慢的将修改推广到一小部分用户，验证没有问题后，再推广到全部用户，以降低生产环境引入新功能带来的风险。</p><p><img src="http://www.howardliu.cn/images/op/canarydeployment.png" alt="canary releases"></p><p>步骤一：将流量从待部署节点移出，更新该节点服务到待发布状态，将该节点称为金丝雀节点；</p><p>步骤二：根据不同策略，将流量引入金丝雀节点。策略可以根据情况指定，比如随机样本策略（随机引入）、狗粮策略（就是内部用户或员工先尝鲜）、分区策略（不同区域用户使用不同版本）、用户特征策略（这种比较复杂，需要根据用户个人资料和特征进行分流，类似于千人千面）；</p><p>步骤三：金丝雀节点验证通过后，选取更多的节点称为金丝雀节点，重复步骤一和步骤二，直到所有节点全部更新</p><h1 id="AB测试"><a href="#AB测试" class="headerlink" title="AB测试"></a>AB测试</h1><p>AB测试和上面两种发布方式不是一个范围的概念，它是为了进行效果验证的手段，其他两种是为了实现线上平稳发布的手段，这里把他们放在一起说，是因为这三个概念很容易弄混。</p><p>AB测试是线上同时运行多个不同版本的服务，这些服务更多的是用户侧的体验不同，比如页面布局、按钮颜色，交互方式等，通常底层业务逻辑还是一样的，也就是通常说的换汤不换药。</p><p><img src="http://www.howardliu.cn/images/op/abtesting.png" alt="A/B Testing"></p><p>这个没有具体的步骤（也可以采用金丝雀部署的步骤，只不过不是全量更新），根据策略（这个策略可以是金丝雀分布中的策略一致），将一部分流量引入A版本，另外一部分流量引入B版本，也可能出现CDEF版本。然后相关人员通过分析不同版本的实际效果，选出最优解。最优解可能是一个版本获胜，取代另一个版本，也可能是催生出更多的版本，服务于用户，还有可能是多个版本在不同区域同时提供服务。</p><h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><p>这里总结一下：</p><table><thead><tr><th>名称</th><th>特点</th><th>优势</th><th>劣势</th></tr></thead><tbody><tr><td>蓝绿部署</td><td>同时存在两个集群，两个集群中只有一个集群真正提供服务，另外一个集群测试、验证或待命</td><td>服务文档，版本回退简单，适用于各种场景的升级，大版本不兼容升级的或迭代兼容升级</td><td>浪费硬件资源，需要同时有两个集群，如果集群比较大，比如有1000个节点，这种方式几乎不可用</td></tr><tr><td>金丝雀部署</td><td>逐点部署，逐步替换线上服务</td><td>小步快跑，快速迭代</td><td>只能适用于兼容迭代的方式，如果是大版本不兼容的场景，就没办法使用这种方式了</td></tr></tbody></table><p>AB测试和上面两个不是一个范畴，不做比较。但是需要说明的一点，AB测试可以采用上面两种部署方式的手法。</p><p>参考：<br><a href="https://docs.cloudfoundry.org/devguide/deploy-apps/blue-green.html" target="_blank" rel="noopener">Using Blue-Green Deployment to Reduce Downtime and Risk</a><br><a href="https://martinfowler.com/bliki/BlueGreenDeployment.html" target="_blank" rel="noopener">Blue Green Deployment</a><br><a href="https://blog.christianposta.com/deploy/blue-green-deployments-a-b-testing-and-canary-releases/" target="_blank" rel="noopener">Blue-green Deployments, A/B Testing, and Canary Releases</a><br><a href="https://martinfowler.com/bliki/CanaryRelease.html" target="_blank" rel="noopener">Canary Release</a></p><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a><br>个人博文: <a href="http://www.howardliu.cn/blue-green-deployments-a-b-testing-and-canary-releases">蓝绿部署、金丝雀发布（灰度发布）、AB测试</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="https://blog.csdn.net/liuxinghao/article/details/102758824" target="_blank" rel="noopener">蓝绿部署、金丝雀发布（灰度发布）、AB测试</a></p>]]></content>
    
    <summary type="html">
    
      随着微服务架构的普及，线上服务越来越多，随之而来的就是部署越来越频繁；随着互联网行业的兴旺，产品迭代的频率也是越来越快，服务上线速度逐步提升。有上线、有部署，就有风险。有风险，就对业务有影响，然后就有了一系列减少这种风险的部署方案：蓝绿部署、金丝雀发布（灰度发布），也有适应产品迭代频率的AB测试。
    
    </summary>
    
    
      <category term="devops" scheme="http://www.howardliu.cn/categories/devops/"/>
    
    
      <category term="DevOps" scheme="http://www.howardliu.cn/tags/DevOps/"/>
    
      <category term="系统运维" scheme="http://www.howardliu.cn/tags/%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4/"/>
    
      <category term="部署" scheme="http://www.howardliu.cn/tags/%E9%83%A8%E7%BD%B2/"/>
    
  </entry>
  
  <entry>
    <title>HTTP的响应头Vary</title>
    <link href="http://www.howardliu.cn/http/http-response-header-vary/"/>
    <id>http://www.howardliu.cn/http/http-response-header-vary/</id>
    <published>2019-10-22T12:15:52.000Z</published>
    <updated>2019-10-22T13:29:46.616Z</updated>
    
    <content type="html"><![CDATA[<p>写在前面：Vary 是一个HTTP响应头部信息，它决定了对于未来的一个请求头，应该用一个缓存的回复(response)还是向源服务器请求一个新的回复。它被服务器用来表明在 content negotiation algorithm（内容协商算法）中选择一个资源代表的时候应该使用哪些头部信息（headers）。</p><a id="more"></a><h2 id="描述问题"><a href="#描述问题" class="headerlink" title="描述问题"></a>描述问题</h2><p>今天碰到一个问题，需要根据不同的UA跳转不同的页面，实现手机访问网站跳转到Wap页面，PC端访问跳转到PC端的页面。根据之前的了解，在Nginx做了UA区分，实现了根据UA跳转页面。</p><p>结果，解决一个问题，同时引入一个更大问题。</p><p>CND中缓存的文档出现了错位，因为请求地址是相同的，结果有时候存储的是PC页面，有时候存储的是WAP页面。</p><h2 id="分析问题"><a href="#分析问题" class="headerlink" title="分析问题"></a>分析问题</h2><p>CDN缓存了错误的数据，是因为相同地址、不同UA，Nginx返回的内容是不一样的。</p><p>在CDN缓存过期时，UA是手机，Nginx就会返回Wap页面，缓存的就是Wap页面。</p><p>在CDN缓存过期时，UA是PC，Nginx就会返回PC页面，缓存的就是PC页面。</p><h2 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h2><p>针对这种架构，有两种解决方法，一种是，在PC页面中增加UA校验，如果是手机，就跳转页面，代码如下：</p><pre><code class="js">var originHref = window.location.href;var afterSchema = originHref.indexOf(&quot;://&quot;) + 3;var domainEnd = originHref.indexOf(&quot;/&quot;, afterSchema);var domain = originHref.substring(afterSchema, domainEnd);if (domain !== &#39;m.howardliu.cn&#39;) {    if (navigator) {        var ua = navigator.userAgent.toLowerCase();        if (/mobile|android|iphone|ipad|phone/i.test(ua)) {            window.location.href = originHref.substring(0, afterSchema) + &#39;m.howardliu.cn&#39; + originHref.substring(domainEnd);        }    }}</code></pre><p>这种方式可以解决问题，但是不够优雅，因为需要浏览器先加载PC的页面，然后再进行检查，也就是会加载两个页面。如果网速比较慢时，就会在PC站驻留比较长的时间，然后才跳转到wap站，用户体验差。</p><p>那就得只能在CDN进行处理，CDN服务商提供的解决方案是，在响应头中增加<code>vary: Accept-Encoding, User-Agent</code>，具体配置是在nginx中配置：</p><pre><code class="conf">add_header Vary &quot;Accept-Encoding, User-Agent&quot;;</code></pre><p>果然解决了问题。</p><p>学非探其花，要深拔其根。http响应头中的<code>Vary</code>是什么作用呢？为什么配置了它之后，CDN就能够区分不同的响应信息了？</p><p><a href="https://tools.ietf.org/html/rfc7231" target="_blank" rel="noopener">RFC 7231</a>中的<a href="https://tools.ietf.org/html/rfc7231#section-7.1.4" target="_blank" rel="noopener">7.1.4.  Vary</a>给出了解释：</p><blockquote><p>The “Vary” header field in a response describes what parts of a request message, aside from the method, Host header field, and request target, might influence the origin server’s process for selecting and representing this response. The value consists of either a single asterisk (“*”) or a list of header field names (case-insensitive).</p><p>Vary = “*” / 1#field-name</p><p>A Vary field value of “” signals that anything about the request might play a role in selecting the response representation, possibly including elements outside the message syntax (e.g., the client’s network address). A recipient will not be able to determine whether this response is appropriate for a later request without forwarding the request to the origin server. A proxy MUST NOT generate a Vary field with a “” value.</p><p>A Vary field value consisting of a comma-separated list of names indicates that the named request header fields, known as the selecting header fields, might have a role in selecting the representation. The potential selecting header fields are not limited to those defined by this specification.</p><p>For example, a response that contains</p><p>Vary: accept-encoding, accept-language</p><p>indicates that the origin server might have used the request’s Accept-Encoding and Accept-Language fields (or lack thereof) as determining factors while choosing the content for this response.</p><p>An origin server might send Vary with a list of fields for two purposes:</p><ol><li>To inform cache recipients that they MUST NOT use this response to satisfy a later request unless the later request has the same values for the listed fields as the original request (Section 4.1 of [RFC7234]). In other words, Vary expands the cache key required to match a new request to the stored cache entry.</li><li>To inform user agent recipients that this response is subject to content negotiation (Section 5.3) and that a different representation might be sent in a subsequent request if additional parameters are provided in the listed header fields (proactive negotiation).</li></ol><p>An origin server SHOULD send a Vary header field when its algorithm for selecting a representation varies based on aspects of the request message other than the method and request target, unless the variance cannot be crossed or the origin server has been deliberately configured to prevent cache transparency. For example, there is no need to send the Authorization field name in Vary because reuse across users is constrained by the field definition (Section 4.2 of [RFC7235]). Likewise, an origin server might use Cache-Control directives (Section 5.2 of [RFC7234]) to supplant Vary if it considers the variance less significant than the performance cost of Vary’s impact on caching.</p></blockquote><p>简单来说，<code>Vary</code>是http的一种约定，当客户端与服务端之间，针对相同的URL请求，服务端存在不同内容的响应，且响应内容是根据请求头的不同，返回不同时，就需要<code>Vary</code>指定需要区分的请求头了。</p><p>比如，上面提到的需要根据UA来区分，那响应信息里面就需要包括：<code>Vary: User-Agent</code>。如果需要根据<code>Accept-Encoding</code>和<code>Accept-Language</code>进行区分，响应头就需要包含<code>Vary: Accept-Encoding, Accept-Language</code>。这样做，其实是为了客户端能够很好的对结果缓存。</p><p>总结一下：</p><ol><li><code>Vary</code>是服务端添加在响应头的信息</li><li><code>Vary</code>的内容来源于请求头</li><li>实现完整协议的客户端（包括浏览器和缓存服务器）缓存数据时，会将<code>Vary</code>一起缓存。</li></ol>]]></content>
    
    <summary type="html">
    
      Vary 是一个HTTP响应头部信息，它决定了对于未来的一个请求头，应该用一个缓存的回复(response)还是向源服务器请求一个新的回复。它被服务器用来表明在 content negotiation algorithm（内容协商算法）中选择一个资源代表的时候应该使用哪些头部信息（headers）。
    
    </summary>
    
    
      <category term="http" scheme="http://www.howardliu.cn/categories/http/"/>
    
    
      <category term="概念" scheme="http://www.howardliu.cn/tags/%E6%A6%82%E5%BF%B5/"/>
    
      <category term="网络" scheme="http://www.howardliu.cn/tags/%E7%BD%91%E7%BB%9C/"/>
    
      <category term="http" scheme="http://www.howardliu.cn/tags/http/"/>
    
  </entry>
  
  <entry>
    <title>spring-cloud-config 非对称加密 keystore 文件加载异常</title>
    <link href="http://www.howardliu.cn/spring-cloud/spring-cloud-config-encrypt-keystore-path-error/"/>
    <id>http://www.howardliu.cn/spring-cloud/spring-cloud-config-encrypt-keystore-path-error/</id>
    <published>2017-10-12T01:35:44.000Z</published>
    <updated>2019-10-26T08:57:15.872Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Spring Cloud Config是Spring Cloud一个全新的项目，依赖版本仓库（比如Git、SVN）实现分布式系统外部配置的集中管理。<br>文中Spring Cloud的版本是Dalston.SR4，可能在其他之后的版本有修改。</p></blockquote><p>最近这段时间在学习Spring Cloud，准备在项目中使用。Spring Cloud不能简单的算是一个框架，而应该认为是一个微服务的整体解决方案，它集成了Spring Boot、Netflix等等很多非常优秀的框架，很多组件开箱即用。也正是因为它集成了这么多框架，致使其版本不够稳定，即使是SR的版本，也存在这样那样的问题。甚至有的上一个版本没有问题，这个版本就出问题了。</p><a id="more"></a><p>因为配置内容可能涉及到某些敏感信息，所以可以简单的在Spring Cloud Config中使用非对称加密实现敏感信息的安全存储。但是在使用过程中，却碰到一些奇葩的问题，无论是官方文档还是一些大神的博文，都没有明确解决。</p><p>当然，这个问题的出现与我的配置有关，很多人可能并没有碰到。这里把问题描述下，万一有人和我一样配置，也出现问题，就可以帮助别人节省点时间了。</p><h1 id="1-正确姿势"><a href="#1-正确姿势" class="headerlink" title="1. 正确姿势"></a>1. 正确姿势</h1><p>Spring Cloud Config的非对称加密可以使用RSA加密方式，通过JDK自带的keytool生产秘匙对，对敏感信息进行加密解密。</p><p>生产秘匙对：</p><pre><code class="sh">keytool -genkeypair -alias mytestkey -keyalg RSA -dname &quot;CN=Web Server,OU=China,O=www.howardliu.cn,L=Beijing,S=Beijing,C=China&quot; -keypass changeme -keystore config-service.jks -storepass letmein</code></pre><p>将生成的keystore文件拷贝到<code>src/main/resources</code>目录中，然后在配置文件<code>bootstrap.yml</code>中配置信息秘匙对信息：</p><pre><code>encrypt:  key-store:    location: classpath:/config-service.jks    alias: mytestkey    password: letmein    secret: changeme</code></pre><p>在<code>pom.xml</code>中加入了如下配置：</p><pre><code class="xml">&lt;build&gt;    &lt;resources&gt;        &lt;resource&gt;            &lt;directory&gt;src/main/resources&lt;/directory&gt;            &lt;filtering&gt;true&lt;/filtering&gt;            &lt;excludes&gt;                &lt;exclude&gt;**/*.jks&lt;/exclude&gt;            &lt;/excludes&gt;        &lt;/resource&gt;        &lt;resource&gt;            &lt;directory&gt;src/main/resources&lt;/directory&gt;            &lt;filtering&gt;false&lt;/filtering&gt;            &lt;includes&gt;                &lt;include&gt;**/*.jks&lt;/include&gt;            &lt;/includes&gt;        &lt;/resource&gt;    &lt;/resources&gt;&lt;/build&gt;</code></pre><p>然后启动应用，通过访问<a href="http://localhost:8888/encrypt/status" target="_blank" rel="noopener">http://localhost:8888/encrypt/status</a> ，返回<code>{&quot;status&quot;:&quot;OK&quot;}</code>，说明配置正确。这个使用就可以通过访问<code>/encrypt</code>和<code>decrypt</code>两个endpoints对指定信息进行加密解密了。</p><p>这个项目的代码在<a href="https://github.com/howardliu-cn/spring-learning/tree/master/micro-service/config-service" target="_blank" rel="noopener">https://github.com/howardliu-cn/spring-learning/tree/master/micro-service/config-service</a>。</p><h1 id="2-错误姿势"><a href="#2-错误姿势" class="headerlink" title="2. 错误姿势"></a>2. 错误姿势</h1><p>刚开始学习的时候并不是如上面那样顺利，碰到了一些问题。</p><h2 id="2-1-encrypt-key-store-配置位置引起的加密信息未解密"><a href="#2-1-encrypt-key-store-配置位置引起的加密信息未解密" class="headerlink" title="2.1 encrypt.key-store.*配置位置引起的加密信息未解密"></a>2.1 encrypt.key-store.*配置位置引起的加密信息未解密</h2><p>最开始是将<code>encrypt.key-store.*</code>配置在<code>application.yml</code>中，这个时候通过访问<a href="http://localhost:8888/encrypt/status" target="_blank" rel="noopener">http://localhost:8888/encrypt/status</a> ，返回<code>{&quot;status&quot;:&quot;OK&quot;}</code>。原本以为配置成功，结果在客户端获取加密信息的时候，返回的结果并没有解密，只是将<code>{cipher}</code>去掉了。</p><p>这个错误算是比较容易排查，Spring Cloud的主要配置文件就<code>application.yml</code>和<code>bootstrap.yml</code>两个，因为客户端中需要把Spring Cloud Config的配置信息放在<code>bootstrap.yml</code>中，就把<code>encrypt.key-store.*</code>配置<code>bootstrap.yml</code>试试，结果果然成功。</p><blockquote><p>这个错误出现的时候，<code>encrypt.key-store.location</code>使用的是绝对路径<code>file://${user.home}/work/projects/spring-learning/micro-service/config-service/src/main/resources/config-service.jks</code>。</p></blockquote><h2 id="2-2-maven-filter对keystore文件的污染"><a href="#2-2-maven-filter对keystore文件的污染" class="headerlink" title="2.2 maven filter对keystore文件的污染"></a>2.2 maven filter对keystore文件的污染</h2><p>因为对Spring Cloud Config服务端定义了<code>/info</code>这个endpoints这个信息，需要用到maven的构建内容，所以在pom.xml中加入了filter信息：</p><pre><code class="xml">&lt;build&gt;    &lt;resources&gt;        &lt;resource&gt;            &lt;directory&gt;src/main/resources&lt;/directory&gt;            &lt;filtering&gt;true&lt;/filtering&gt;        &lt;/resource&gt;    &lt;/resources&gt;&lt;/build&gt;</code></pre><p>结果在访问 <a href="http://localhost:8888/encrypt/status" target="_blank" rel="noopener">http://localhost:8888/encrypt/status</a> 的时候，返回错误信息：</p><pre><code>Thu Oct 12 13:55:51 CST 2017There was an unexpected error (type=Internal Server Error, status=500).Cannot load keys from store: class path resource [config-service.jks]</code></pre><p>服务端报错如下：</p><pre><code>2017-10-12 13:55:50.991 ERROR 14733 --- [nio-8888-exec-1] o.a.c.c.C.[.[.[/].[dispatcherServlet]    : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.IllegalStateException: Cannot load keys from store: class path resource [config-service.jks]] with root causejava.io.IOException: Invalid keystore format    at sun.security.provider.JavaKeyStore.engineLoad(JavaKeyStore.java:658) ~[na:1.8.0_144]    at sun.security.provider.JavaKeyStore$JKS.engineLoad(JavaKeyStore.java:56) ~[na:1.8.0_144]    at sun.security.provider.KeyStoreDelegator.engineLoad(KeyStoreDelegator.java:224) ~[na:1.8.0_144]    at sun.security.provider.JavaKeyStore$DualFormatJKS.engineLoad(JavaKeyStore.java:70) ~[na:1.8.0_144]    at java.security.KeyStore.load(KeyStore.java:1445) ~[na:1.8.0_144]    at org.springframework.security.rsa.crypto.KeyStoreKeyFactory.getKeyPair(KeyStoreKeyFactory.java:53) ~[spring-security-rsa-1.0.3.RELEASE.jar:na]    at org.springframework.cloud.config.server.encryption.KeyStoreTextEncryptorLocator.locate(KeyStoreTextEncryptorLocator.java:84) ~[spring-cloud-config-server-1.3.3.RELEASE.jar:1.3.3.RELEASE]    at org.springframework.cloud.config.server.encryption.EncryptionController.checkEncryptorInstalled(EncryptionController.java:167) ~[spring-cloud-config-server-1.3.3.RELEASE.jar:1.3.3.RELEASE]    at org.springframework.cloud.config.server.encryption.EncryptionController.status(EncryptionController.java:113) ~[spring-cloud-config-server-1.3.3.RELEASE.jar:1.3.3.RELEASE]    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_144]    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_144]    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144]    at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144]    at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:967) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at javax.servlet.http.HttpServlet.service(HttpServlet.java:635) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at javax.servlet.http.HttpServlet.service(HttpServlet.java:742) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) ~[tomcat-embed-websocket-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55) ~[spring-boot-1.5.4.RELEASE.jar:1.5.4.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:110) ~[spring-boot-actuator-1.5.4.RELEASE.jar:1.5.4.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilterInternal(BasicAuthenticationFilter.java:215) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:64) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:262) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:105) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:81) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:106) ~[spring-boot-actuator-1.5.4.RELEASE.jar:1.5.4.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:198) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:478) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:140) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:80) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:87) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:799) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:861) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1455) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) [tomcat-embed-core-8.5.15.jar:8.5.15]    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_144]    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_144]    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-embed-core-8.5.15.jar:8.5.15]    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]</code></pre><p>从错误堆栈信息中可以看到，是秘匙对文件格式错误。但是并没有对文件进行修改，何来错误一说。本着怀疑一切的精神，重新生成了一份keystore文件，然后执行，仍然报错。</p><p>那问题就来了，文件格式没有问题，结果却报文件格式有问题，那就只能是编译的时候文件被修改了。于是将<code>encrypt.key-store.location</code>使用绝对路径<code>file://${user.home}/work/projects/spring-learning/micro-service/config-service/src/main/resources/config-service.jks</code>进行实验，结果成功解析。然后将路径改为编译后target中的文件路径，就报上面的错误。果然是maven编译的时候报错。</p><p>就在<a href="https://stackoverflow.com/questions/30992424/tomcat-java-io-ioexception-invalid-keystore-format-when-loading-keystore-via-cl" target="_blank" rel="noopener">stackoverflow</a>上找到解决方式，是maven的filter污染了文件。就把pom.xml中的配置改为这样：</p><pre><code class="xml">&lt;build&gt;    &lt;resources&gt;        &lt;resource&gt;            &lt;directory&gt;src/main/resources&lt;/directory&gt;            &lt;filtering&gt;true&lt;/filtering&gt;            &lt;excludes&gt;                &lt;exclude&gt;**/*.jks&lt;/exclude&gt;            &lt;/excludes&gt;        &lt;/resource&gt;        &lt;resource&gt;            &lt;directory&gt;src/main/resources&lt;/directory&gt;            &lt;filtering&gt;false&lt;/filtering&gt;            &lt;includes&gt;                &lt;include&gt;**/*.jks&lt;/include&gt;            &lt;/includes&gt;        &lt;/resource&gt;    &lt;/resources&gt;&lt;/build&gt;</code></pre><p>再次测试，果然成功。不过遗憾的是，还不确定filter为什么会污染文件，后面慢慢研究，等找到原因再把结果放在下面。</p><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a></p><p>个人博文: <a href="http://www.howardliu.cn/spring-cloud/spring-cloud-config-encrypt-keystore-path-error">spring-cloud-config 非对称加密 keystore 文件加载异常</a></p><p>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a></p><p>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/78216201" target="_blank" rel="noopener">spring-cloud-config 非对称加密 keystore 文件加载异常</a></p>]]></content>
    
    <summary type="html">
    
      因为配置内容可能涉及到某些敏感信息，所以可以简单的在Spring Cloud Config中使用非对称加密实现敏感信息的安全存储。但是在使用过程中，却碰到一些奇葩的问题，无论是官方文档还是一些大神的博文，都没有明确解决。当然，这个问题的出现与我的配置有关，很多人可能并没有碰到。这里把问题描述下，万一有人和我一样配置，也出现问题，就可以帮助别人节省点时间了。
    
    </summary>
    
    
      <category term="spring-cloud" scheme="http://www.howardliu.cn/categories/spring-cloud/"/>
    
      <category term="spring-cloud-config" scheme="http://www.howardliu.cn/categories/spring-cloud/spring-cloud-config/"/>
    
      <category term="error" scheme="http://www.howardliu.cn/categories/spring-cloud/spring-cloud-config/error/"/>
    
    
      <category term="spring-cloud" scheme="http://www.howardliu.cn/tags/spring-cloud/"/>
    
      <category term="spring-cloud-config" scheme="http://www.howardliu.cn/tags/spring-cloud-config/"/>
    
      <category term="error" scheme="http://www.howardliu.cn/tags/error/"/>
    
      <category term="micro-service" scheme="http://www.howardliu.cn/tags/micro-service/"/>
    
  </entry>
  
  <entry>
    <title>代码质量管理：SonarQube + Jenkins Pipeline配置</title>
    <link href="http://www.howardliu.cn/sonarqube-jenkins-pipeline/"/>
    <id>http://www.howardliu.cn/sonarqube-jenkins-pipeline/</id>
    <published>2017-09-13T07:03:21.000Z</published>
    <updated>2019-11-18T14:41:35.792Z</updated>
    
    <content type="html"><![CDATA[<p>前段时间对自己的项目进行代码质量扫描，曾经以为自己的代码质量算是不错的，结果发现一堆的bug或者smell code，灵魂受到1w点伤害。</p><p>可以想到，在时间紧、任务中的情况下，代码质量绝对是不能够保证的，虽然功能算是完整，但是可能就在某个隐藏的角落，就有无数的bug在潜伏着，所以有时间的话都对自己的代码进行代码质量检查吧。虽然不能保证有完美的代码，但是可以把bug数降低，也可以根据扫描的结果养成良好的编程习惯。</p><p>身为程序员就得严谨。</p><p>闲言碎语不再讲。</p><p>本文主要是介绍通过Jenkins Pipeline与SonarQube集成，对代码进行扫描，这里使用的是Jenkins2.19.1，SonarQube6.4。</p><a id="more"></a><h1 id="1-基础工作"><a href="#1-基础工作" class="headerlink" title="1. 基础工作"></a>1. 基础工作</h1><h2 id="1-1-安装插件"><a href="#1-1-安装插件" class="headerlink" title="1.1 安装插件"></a>1.1 安装插件</h2><p>在Jenkins管理界面中的 系统管理-&gt;插件管理 安装最新的 <em>SonarQube plugin</em> 插件，并重启Jenkins：</p><p><img src="http://www.howardliu.cn/images/code_quality/jenkins_available-plugins_sonarqube.png" alt="SonarQube plugin"></p><h2 id="1-2-配置-安装SonarQube-Scanner"><a href="#1-2-配置-安装SonarQube-Scanner" class="headerlink" title="1.2 配置/安装SonarQube Scanner"></a>1.2 配置/安装SonarQube Scanner</h2><p>等待重启之后，在 系统管理-&gt;Global Tool Configuration 中配置/安装最新的SonarQube Scanner：</p><p><img src="http://www.howardliu.cn/images/code_quality/jenkins_global_tool_sonarqube_scanner_1.png" alt="SonarQube plugin"></p><p>可以选择自动安装，这样会在需要用到的时候自动从默认地址安装到默认路径，不需要手动其下载安装，非常方便。当然，也可以自己去网上下载，如果是自己下载的话，需要把<em>自动安装</em>的勾去掉，然后填上自己下载的SonarQube Scanner运行包路径：</p><p><img src="http://www.howardliu.cn/images/code_quality/jenkins_global_tool_sonarqube_scanner_2.png" alt="SonarQube plugin"></p><h2 id="1-3-配置SonarQube服务"><a href="#1-3-配置SonarQube服务" class="headerlink" title="1.3 配置SonarQube服务"></a>1.3 配置SonarQube服务</h2><p>因为SonarQube Scanner工具需要把扫描的代码及结果发送到SonarQube服务器上，所以需要配置SonarQube服务地址。</p><p>在 系统管理-&gt;系统设置 中增加 <em>SonarQube servers</em> 相关配置：</p><p><img src="http://www.howardliu.cn/images/code_quality/jenkins_system_setting_sonarqube_server.png" alt="SonarQube plugin"></p><p>至此，基础配置工作就结束了，下面可以开始配置扫描任务了。</p><h1 id="2-配置待扫描项目"><a href="#2-配置待扫描项目" class="headerlink" title="2. 配置待扫描项目"></a>2. 配置待扫描项目</h1><p>首先创建一个Jenkins Pipeline项目:</p><p><img src="http://www.howardliu.cn/images/code_quality/jenkins_new_project_pipeline.png" alt="Pipeline Project"></p><p>然后修改Pipeline脚本，比如：</p><pre><code class="Groovy">node {    stage(&#39;SCM&#39;) {        git([url: &#39;https://github.com/howardliu-cn/cynomys.git&#39;])    }    stage(&#39;SonarQube analysis&#39;) {        def sonarqubeScannerHome = tool name: &#39;SonarQube Scanner&#39;        withSonarQubeEnv(&#39;SonarQube&#39;) {            sh &quot;${sonarqubeScannerHome}/bin/sonar-scanner&quot;        }    }}</code></pre><p>如果需要指定从某个分支复制代码，可以增加<code>branch</code>参数；如果使用ssh方式复制代码，需要通过<code>credentialsId</code>参数配置Jenkins中配置好的秘钥ID。比如：</p><pre><code class="Groovy">node {    stage(&#39;SCM&#39;) {        git([url: &#39;git@10.6.3.213:RD/messenger.git&#39;, branch: &#39;develop&#39;, credentialsId: &#39;fae8b1b9-8818-48e9-a28a-24b928015a6c&#39;])    }    stage(&#39;SonarQube analysis&#39;) {        def sonarqubeScannerHome = tool name: &#39;SonarQube Scanner&#39;        withSonarQubeEnv(&#39;SonarQube&#39;) {            sh &quot;${sonarqubeScannerHome}/bin/sonar-scanner&quot;        }    }}</code></pre><p>这两种方式都需要在项目的根路径下面有一个<code>sonar-project.properties</code>文件，其内容如下：</p><pre><code># must be unique in a given SonarQube instancesonar.projectKey=cynomys:0.0.1# this is the name and version displayed in the SonarQube UI. Was mandatory prior to SonarQube 6.1.sonar.projectName=cynomyssonar.projectVersion=0.0.1# Path is relative to the sonar-project.properties file. Replace &quot;\&quot; by &quot;/&quot; on Windows.# This property is optional if sonar.modules is set.sonar.sources=.sonar.exclusions=**/test/**,**/target/**sonar.java.source=1.8sonar.java.target=1.8# Encoding of the source code. Default is default system encodingsonar.sourceEncoding=UTF-8</code></pre><p>除了通过Pipeline的方式之外，还可以创建普通项目进行扫描，可以参考<a href="https://support.cloudbees.com/hc/en-us/articles/204947684-How-can-I-Run-A-Sonar-Analysis-Without-Maven-" target="_blank" rel="noopener">这里</a>。</p><p>修改完成，保存，就可以开始扫描了。</p><h1 id="3-开始扫描"><a href="#3-开始扫描" class="headerlink" title="3. 开始扫描"></a>3. 开始扫描</h1><p>直接在项目页面点击立即构建，就会开始扫描。然后登录SonarQube服务，就能够看到代码质量检查的结果了。</p><p><img src="http://www.howardliu.cn/images/code_quality/sonarqube_dashboard_project.png" alt="SonarQube 项目 Dashboard页面"></p><h1 id="4-个人建议"><a href="#4-个人建议" class="headerlink" title="4. 个人建议"></a>4. 个人建议</h1><ol><li>代码质量检查是非常必要的，可以在代码运行之前就可以找到很多bug</li><li>smell code虽然不影响运行，但是在某些情况下对代码的重构、复用、修改造成不必要的影响</li><li>工具只是工具，可以减少低效劳动，但是绝对不会是万能的</li></ol><p>希望之后不会被吐槽是在写bug。。。</p><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a><br>个人博文: <a href="http://www.howardliu.cn/sonarqube-jenkins-pipeline">代码质量管理：SonarQube + Jenkins Pipeline配置</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/77967158" target="_blank" rel="noopener">代码质量管理：SonarQube + Jenkins Pipeline配置</a></p>]]></content>
    
    <summary type="html">
    
      本文主要是介绍通过Jenkins Pipeline与SonarQube集成，对代码进行扫描，这里使用的是Jenkins2.19.1，SonarQube6.4。
    
    </summary>
    
    
      <category term="cd" scheme="http://www.howardliu.cn/categories/cd/"/>
    
    
      <category term="DevOps" scheme="http://www.howardliu.cn/tags/DevOps/"/>
    
      <category term="CD" scheme="http://www.howardliu.cn/tags/CD/"/>
    
      <category term="code quality" scheme="http://www.howardliu.cn/tags/code-quality/"/>
    
      <category term="SonarQube" scheme="http://www.howardliu.cn/tags/SonarQube/"/>
    
      <category term="Jenkins Pipeline" scheme="http://www.howardliu.cn/tags/Jenkins-Pipeline/"/>
    
  </entry>
  
  <entry>
    <title>java.lang.OutOfMemoryError:GC overhead limit exceeded</title>
    <link href="http://www.howardliu.cn/java/gc-overhead-limit-exceeded/"/>
    <id>http://www.howardliu.cn/java/gc-overhead-limit-exceeded/</id>
    <published>2017-09-09T08:24:59.000Z</published>
    <updated>2019-11-18T14:59:03.852Z</updated>
    
    <content type="html"><![CDATA[<p><code>java.lang.OutOfMemoryError</code>这个错误是比较经典的错误了，经过JDK不断的迭代，服务器硬件的不断升级。。。总之，社会在发展，时代在进步。很多错误已经消失在时代的浪潮中。我也是很久没有见过这个错误了，以至于都以为在Java的世界中不会再碰到这个错误。结果，就在最疏忽的时候碰到了TA，真是，心中一万只神兽奔袭而过，狠狠的践踏了我这颗上了年纪的心脏啊。</p><p>吐槽完毕，言归正传。</p><p>Java刚刚出现的年代，有一个相比于其他语言的优势就是，内存回收机制。不需要明确的调用释放内存的API，java就自动完成，这个过程就是Garbage Collection，简称GC。这对以懒著称的程序猿们来说，绝对是重大利好。但是，凡事有利必有弊，可以肯定的是，Java语言是人创造的，GC也是人编写的代码，绝对不是机器自动完成的。也就是说，GC的过程是另外一群程序猿根据可能出现的情况，预设了GC条件，把符合回收条件的内存空间释放出来。一旦被占用的内存空间不符合释放的条件，GC没办法清理，那就会适时出现<code>java.lang.OutOfMemoryError</code>。这个错误就是提醒我们这群程序猿，写GC程序的程序猿不知道这种情况怎么处理，为了安全也不便处理，谁使用Java就自己看着解决吧。</p><p>说起来，<code>java.lang.OutOfMemoryError</code>有几种分类的，这次碰到的是<code>java.lang.OutOfMemoryError: GC overhead limit exceeded</code>，下面就来说说这种类型的内存溢出。</p><a id="more"></a><p>简单来说，<code>java.lang.OutOfMemoryError: GC overhead limit exceeded</code>发生的原因是，当前已经没有可用内存，经过多次GC之后仍然没能有效释放内存。</p><h1 id="1-原因"><a href="#1-原因" class="headerlink" title="1. 原因"></a>1. 原因</h1><p>众所周知，JVM的GC过程会因为STW，只不过停顿短到不容易感知。当引起停顿时间的98%都是在进行GC，但是结果只能得到小于2%的堆内存恢复时，就会抛出<code>java.lang.OutOfMemoryError: GC overhead limit exceeded</code>这个错误。<a href="https://plumbr.eu" target="_blank" rel="noopener">Plumbr</a>给出一个示意图：</p><p><img src="http://www.howardliu.cn/images/java/OOM-gc-overhead-limit-exceeded-1.png" alt="java.lang.OutOfMemoryError: GC overhead limit exceeded"></p><p>这个错误其实就是空闲内存与GC之间平衡的一个限制，当经过几次GC之后，只有少于2%的内存被释放，也就是很少的空闲内存，可能会再次被快速填充，这样就会触发再一次的GC。这就是一个恶性循环了，CPU大部分的时间在做GC操作，没有时间做具体的业务操作，可能几毫秒的任务需要几分钟都无法完成，整个应用程序就形同虚设了。</p><h1 id="2-示例"><a href="#2-示例" class="headerlink" title="2. 示例"></a>2. 示例</h1><p>从<a href="https://plumbr.eu" target="_blank" rel="noopener">Plumbr</a>上找的一个例子，这里直接给出。</p><pre><code class="java">class Wrapper {  public static void main(String args[]) throws Exception {    Map map = System.getProperties();    Random r = new Random();    while (true) {      map.put(r.nextInt(), &quot;value&quot;);    }  }}</code></pre><p>然后设定堆内存是100m，比如<code>java -Xmx100m -XX:+UseParallelGC Wrapper</code>。不同的系统环境可能需要设置不同的堆内存大小，否则会产生不同的OOM错误。其实也算是好理解，因为<code>java.lang.OutOfMemoryError: GC overhead limit exceeded</code>需要有两个条件：98%的时间和2%的内存。如果这两个条件有一个没有达到，结果Map对象扩容，那就可能出现<code>java.lang.OutOfMemoryError: Java heap space</code>这个错误。</p><h1 id="3-解决方法"><a href="#3-解决方法" class="headerlink" title="3. 解决方法"></a>3. 解决方法</h1><h2 id="3-1-JVM参数"><a href="#3-1-JVM参数" class="headerlink" title="3.1 JVM参数"></a>3.1 JVM参数</h2><p>JVM给出一个参数避免这个错误：<code>-XX:-UseGCOverheadLimit</code>。</p><p>但是，这个参数并不是解决了内存不足的问题，只是将错误发生时间延后，并且替换成<code>java.lang.OutOfMemoryError: Java heap space</code>。</p><h2 id="3-2-堆内存"><a href="#3-2-堆内存" class="headerlink" title="3.2 堆内存"></a>3.2 堆内存</h2><p>还有一个偷懒的方法是：增大堆内存。既然堆内存少了，那就增加堆内存即可。</p><p>但是，这个方法也不是万能的。因为程序里可能有内存泄露。这个时候即使再增大堆内存，也会有用完的时候。</p><p>所以前两个方法都只是治标不治本而已。</p><h2 id="3-3-终极方法"><a href="#3-3-终极方法" class="headerlink" title="3.3 终极方法"></a>3.3 终极方法</h2><p>其实还是有一个终极方法的，而且是治标治本的方法，就是找到占用内存大的地方，把代码优化了，就不会出现这个问题了。</p><p>怎么找到需要优化的代码呢？就是通过heap dump生产jvm快照，通过分析快照找到占用内存大的对象，从而找到代码位置。</p><p>通过设置<code>-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=heapdump</code>参数来生产快照，然后通过VisualVM或者<a href="http://www.eclipse.org/mat/" target="_blank" rel="noopener">MAT</a>等工具分析快照内容进行定位。通过这个参数是将发生OOM时的堆内存所有信息写入快照文件，也就是说，如果此时堆内存中有敏感信息的话，那就可能造成信息泄漏了。</p><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a></p><p>个人博文: <a href="http://www.howardliu.cn/java/gc-overhead-limit-exceeded">java.lang.OutOfMemoryError:GC overhead limit exceeded</a></p><p>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a></p><p>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/77934725" target="_blank" rel="noopener">java.lang.OutOfMemoryError:GC overhead limit exceeded</a></p>]]></content>
    
    <summary type="html">
    
      简单来说，java.lang.OutOfMemoryError\:\ GC overhead limit exceeded发生的原因是，当前已经没有可用内存，经过多次GC之后仍然没能有效释放内存。
    
    </summary>
    
    
      <category term="java" scheme="http://www.howardliu.cn/categories/java/"/>
    
      <category term="oom" scheme="http://www.howardliu.cn/categories/java/oom/"/>
    
    
      <category term="java" scheme="http://www.howardliu.cn/tags/java/"/>
    
      <category term="oom" scheme="http://www.howardliu.cn/tags/oom/"/>
    
  </entry>
  
  <entry>
    <title>中文字节长度引起的数据丢失</title>
    <link href="http://www.howardliu.cn/get-chinese-length/"/>
    <id>http://www.howardliu.cn/get-chinese-length/</id>
    <published>2017-08-30T07:43:05.000Z</published>
    <updated>2019-11-09T06:43:05.009Z</updated>
    
    <content type="html"><![CDATA[<p>最近在写一个应用监控的项目，使用netty作为数据传输。因为刚开始写，没有使用Protobuf之类的作为编码工具，只是使用的是netty自带的<code>LengthFieldBasedFrameDecoder</code>作为报文解析工具，自定义编码解码类，实现数据传输。</p><p>本来一切正常，结果在昨天测试的过程中，传输的数据体总是少16个字符，甚是奇怪。</p><a id="more"></a><p>翻来覆去查问题，后来仔细查看报文内容，才发现报文中有8个汉字。这才想到，中文字节长度不能使用<code>java.lang.String</code>的<code>length()</code>方法获取。应该使用的是<code>getBytes()</code>方法转成字节数组，在通过数组的<code>length</code>属性获取长度。</p><p>比如：</p><p><code>&quot;abcd&quot;.length()</code>的结果是：4<br><code>&quot;abcd&quot;.getBytes().length</code>的结果是：4<br><code>&quot;中国威武&quot;.length()</code>的结果是：4<br><code>&quot;中国威武&quot;.getBytes().length</code>的结果是：12<br><code>&quot;中国v5&quot;.length()</code>的结果是：4<br><code>&quot;中国v5&quot;.getBytes().length</code>的结果是：8</p><p>例子简单，但也能说明问题。在这里每个中文字节长度是3，英文字母、数字、英文标点是1。</p><p>所以在我的测试代码中，存在的8个汉字使用<code>length()</code>方法获取的长度是8，比<code>getBytes()</code>方法的字节数组长度少了16，所以在传输过程中总是少了16个字符（英文字符长度是1）。</p><p>总的来说，在对中文进行转换字节的时候一定要注意，千万不要想当然的使用<code>length()</code>方法。还是要根据具体情况多试试。<strong>特立文标记此错误</strong></p><p>下面附上代码：</p><p>编码器：MessageEncoder</p><pre><code class="java">import cn.howardliu.monitor.cynomys.net.struct.Message;import io.netty.buffer.ByteBuf;import io.netty.channel.ChannelHandlerContext;import io.netty.handler.codec.MessageToByteEncoder;import io.netty.util.CharsetUtil;public class MessageEncoder extends MessageToByteEncoder&lt;Message&gt; {    @Override    protected void encode(ChannelHandlerContext ctx, Message msg, ByteBuf out) throws Exception {        if (msg == null || msg.getHeader() == null) {            throw new IllegalArgumentException(&quot;the encode message is null.&quot;);        }        out.writeInt(msg.getHeader().getCrcCode());        out.writeInt(msg.getHeader().getLength());        out.writeInt(msg.getHeader().getOpaque());        out.writeInt(msg.getHeader().getTag().length());        out.writeCharSequence(msg.getHeader().getTag(), CharsetUtil.UTF_8);        out.writeInt(msg.getHeader().getSysName().length());        out.writeCharSequence(msg.getHeader().getSysName(), CharsetUtil.UTF_8);        out.writeInt(msg.getHeader().getSysCode().length());        out.writeCharSequence(msg.getHeader().getSysCode(), CharsetUtil.UTF_8);        out.writeByte(msg.getHeader().getType());        out.writeByte(msg.getHeader().getCode());        out.writeByte(msg.getHeader().getFlagPath());        if (msg.getBody() == null) {            out.writeInt(0);        } else {            out.writeInt(msg.getBody().getBytes().length);            out.writeCharSequence(msg.getBody(), CharsetUtil.UTF_8);        }        out.setInt(4, out.readableBytes() - 8);    }}</code></pre><p>解码器：MessageDecoder</p><pre><code class="java">import cn.howardliu.monitor.cynomys.net.struct.Header;import cn.howardliu.monitor.cynomys.net.struct.Message;import io.netty.buffer.ByteBuf;import io.netty.channel.ChannelHandlerContext;import io.netty.handler.codec.LengthFieldBasedFrameDecoder;import io.netty.util.CharsetUtil;public class MessageDecoder extends LengthFieldBasedFrameDecoder {    public MessageDecoder(int maxFrameLength, int lengthFieldOffset, int lengthFieldLength) {        super(maxFrameLength, lengthFieldOffset, lengthFieldLength);    }    @Override    protected Object decode(ChannelHandlerContext ctx, ByteBuf in) throws Exception {        ByteBuf frame = (ByteBuf) super.decode(ctx, in);        if (frame == null) {            return null;        }        Message message = new Message()                .setHeader(                        new Header()                                .setCrcCode(frame.readInt())                                .setLength(frame.readInt())                                .setOpaque(frame.readInt())                                .setTag(frame.readCharSequence(frame.readInt(), CharsetUtil.UTF_8).toString())                                .setSysName(frame.readCharSequence(frame.readInt(), CharsetUtil.UTF_8).toString())                                .setSysCode(frame.readCharSequence(frame.readInt(), CharsetUtil.UTF_8).toString())                                .setType(frame.readByte())                                .setCode(frame.readByte())                                .setFlagPath(frame.readByte())                );        if (frame.readableBytes() &gt; 4) {            message.setBody(frame.readCharSequence(frame.readInt(), CharsetUtil.UTF_8).toString());        }        return message;    }}</code></pre><p>解码器使用方式是<code>new MessageDecoder(1024 * 1024 * 100, 4, 4)</code>。</p><p>厚颜的贴上这个项目地址：</p><ul><li>项目名称：cynomys</li><li>项目地址：<a href="https://github.com/howardliu-cn/cynomys" target="_blank" rel="noopener">https://github.com/howardliu-cn/cynomys</a></li></ul><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a><br>个人博文: <a href="http://www.howardliu.cn/get-chinese-length">中文字节长度引起的数据丢失</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/77717976" target="_blank" rel="noopener">中文字节长度引起的数据丢失</a></p>]]></content>
    
    <summary type="html">
    
      在对中文进行转换字节的时候一定要注意，千万不要想当然的使用`length()`方法。还是要根据具体情况多试试。
    
    </summary>
    
    
      <category term="bug" scheme="http://www.howardliu.cn/categories/bug/"/>
    
    
      <category term="bug" scheme="http://www.howardliu.cn/tags/bug/"/>
    
      <category term="中文字符" scheme="http://www.howardliu.cn/tags/%E4%B8%AD%E6%96%87%E5%AD%97%E7%AC%A6/"/>
    
  </entry>
  
  <entry>
    <title>ResourceManager HA 配置</title>
    <link href="http://www.howardliu.cn/hadoop/rm-ha/"/>
    <id>http://www.howardliu.cn/hadoop/rm-ha/</id>
    <published>2017-07-11T06:34:05.000Z</published>
    <updated>2019-09-11T14:08:23.879Z</updated>
    
    <content type="html"><![CDATA[<p>陆续的把<a href="http://www.howardliu.cn/hadoop/hadoop-environment-cluster-setup">Hadoop集群部署</a>、<a href="(http://www.howardliu.cn/hadoop/hdfs-ha-using-qjm)">HDFS的HA配置</a>完成，把<a href="http://www.howardliu.cn/hadoop/rm-ha">ResourceManager的HA配置</a>好之后，Hadoop集群配置也算是完整了，可以满足小型中型生产环境Hadoop集群搭建的需要。如果真要搭建超大型的Hadoop集群，这些只能算是参考，还需要修改很多其他参数，使性能更好一些。</p><p>ResourceManager（RM）负责跟踪集群中资源使用情况，调度应用程序（比如MapReduce作业）。在Hadoop 2.4之前，ResourceManager存在单点故障，需要通过其他方式实现HA。官方给出的HA方案是Active/Standby两种状态ResourceManager的冗余方式，类似于HDFS的HA方案，也就是通过冗余消除单点故障。</p><a id="more"></a><h1 id="HA架构"><a href="#HA架构" class="headerlink" title="HA架构"></a>HA架构</h1><p>下图是ResourceManager HA方案架构图：</p><p><img src="http://www.howardliu.cn/images/hadoop/rm-ha-overview.png" alt="RM HA"></p><h2 id="RM故障转移"><a href="#RM故障转移" class="headerlink" title="RM故障转移"></a>RM故障转移</h2><p>ResourceManager HA是通过Active/Standby冗余架构实现的，在任何时间点，其中一个RM处于Active状态，其他RM处于Standby状态，Standby状态的RM就等着Active扑街或被撤。通过管理员命令或自动故障转移（需要开启自动故障转移配置），Standby就会转为Active状态，对外提供服务。</p><ul><li>手动转换和故障转移：当未启用自动故障转移时，就需要管理员手动转换。首先将Active状态的RM转为Standby状态，然后将一个Standby状态的转为Active状态。这些操作都需要通过<code>yarn rmadmin</code>命令来操作。</li><li>自动故障转移：RM可以通过内嵌的基于Zookeeper的Active/Standby选择器决定哪个RM应该是Active状态的。当Active性能下降或无响应时，一个Standby状态的RM就被推举出来，转为Active状态接管。这里不需要像HDFS的HA配置需要一个单独的ZKFS守护进程辅助完成切换，因为这个功能已经内嵌在RM中。</li></ul><p>客户端、ApplicationMaster和NodeManager在故障转移时，会轮训这些RM节点，知道找到Active状态的RM。如果Active节点性能下降，他们会重新轮训查找新的Active状态的RM。默认的轮训扩展是<code>org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider</code>。可以通过实现<code>org.apache.hadoop.yarn.client.RMFailoverProxyProvider</code>，并配置<code>yarn.client.failover-proxy-provider</code>来实现自己的逻辑。</p><h2 id="恢复RM状态"><a href="#恢复RM状态" class="headerlink" title="恢复RM状态"></a>恢复RM状态</h2><p>当启用<a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html" target="_blank" rel="noopener">ResourceManger重启状态恢复</a>之后，新的Active状态的RM会加载上一个RM状态，并根据状态尽可能的恢复之前的操作。应用程序会定期检查，以避免丢失数据。状态存储需要对Active状态和Standby状态的RM都可见。目前，<code>RMStateStore</code>有两个持久化实现，<code>FileSystemRMStateStore</code>和<code>ZKRMStateStore</code>。<code>ZKRMStateStore</code>隐式的只允许一个RM写入操作，可以没有单独的防护机制就能够避免闹裂问题，所以是HA集群推荐的状态存储方式。使用<code>ZKRMStateStore</code>时，建议不要在zookeeper集群上设置<code>zookeeper.DigestAuthenticationProvider.superDigest</code>配置，以确保zk管理员无法访问YARN的信息。</p><h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>大多数的故障转移功能可以使用各种配置进行调整，下表是必须的和重要的参数项。完整的配置和默认值参见<a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" target="_blank" rel="noopener">yarn-default.xml</a>。关于状态存储参见<a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html" target="_blank" rel="noopener">ResourceManger状态存储</a>。</p><table><thead><tr><th>配置项</th><th>描述</th></tr></thead><tbody><tr><td>yarn.resourcemanager.zk-address</td><td>zookeeper集群地址，用于状态存储和内部的leader选举</td></tr><tr><td>yarn.resourcemanager.ha.enabled</td><td>开启RM的HA</td></tr><tr><td>yarn.resourcemanager.ha.rm-ids</td><td>RM逻辑id列表，以逗号分割，比如：rm1,rm2。</td></tr><tr><td>yarn.resourcemanager.hostname.[rm-id]</td><td>对于每个rm-id，需要给出hostname或ip地址。</td></tr><tr><td>yarn.resourcemanager.address.[rm-id]</td><td>对于每个rm-id，指定host:port地址，该配置会覆盖yarn.resourcemanager.hostname.rm-id。</td></tr><tr><td>yarn.resourcemanager.scheduler.address.[rm-id]</td><td>对于每个rm-id，指定ApplicationMasters申请资源的Scheduler的host:port地址，该配置会覆盖yarn.resourcemanager.hostname.rm-id。</td></tr><tr><td>yarn.resourcemanager.resource-tracker.address.[rm-id]</td><td>对于每个rm-id，指定NodeManager连接的host:port地址，该配置会覆盖yarn.resourcemanager.hostname.rm-id。</td></tr><tr><td>yarn.resourcemanager.admin.address.[rm-id]</td><td>对于每个rm-id，指定管理命令操作的host:port地址，该配置会覆盖yarn.resourcemanager.hostname.rm-id。</td></tr><tr><td>yarn.resourcemanager.webapp.address.[rm-id]</td><td>对于每个rm-id，指定RM的web应用host:port地址，如果设置yarn.http.policy是HTTPS_ONLY，就没必要设置该参数。该参数会覆盖yarn.resourcemanager.hostname.rm-id。</td></tr><tr><td>yarn.resourcemanager.webapp.https.address.[rm-id]</td><td>对于每个rm-id，指定RM的web应用host:port地址，如果设置yarn.http.policy是HTTP_ONLY，就没必要设置该参数。该参数会覆盖yarn.resourcemanager.hostname.rm-id。</td></tr><tr><td>yarn.resourcemanager.ha.id</td><td>用于识别HA的RM，可选配置。如果设置，需要确定所有的RM都有自己的ID。</td></tr><tr><td>yarn.resourcemanager.ha.automatic-failover.enabled</td><td>启用自动故障转移; 默认情况下，仅当HA被启用时才启用。</td></tr><tr><td>yarn.resourcemanager.ha.automatic-failover.embedded</td><td>启用自动故障转移时，使用嵌入式leader选举选择Active RM。 默认情况下，仅当HA被启用时才启用。</td></tr><tr><td>yarn.resourcemanager.cluster-id</td><td>集群标志，用于保证RM不会成为另一个集群的Active节点。</td></tr><tr><td>yarn.client.failover-proxy-provider</td><td>客户端使用，用于客户端、ApplicationMaster、NodeManager连接到新的Active的RM。</td></tr><tr><td>yarn.client.failover-max-attempts</td><td>FailoverProxyProvider应该尝试的最大次数。</td></tr><tr><td>yarn.client.failover-sleep-base-ms</td><td>用于计算故障转移的休眠基准（单位是毫秒）。</td></tr><tr><td>yarn.client.failover-sleep-max-ms</td><td>故障转移休眠最长时间（单位是毫秒）。</td></tr><tr><td>yarn.client.failover-retries</td><td>尝试连接到RM的重试次数。</td></tr><tr><td>yarn.client.failover-retries-on-socket-timeouts</td><td>尝试连接到RM中可允许超时连接的次数。</td></tr></tbody></table><p>配置示例（配置承接<a href="http://www.howardliu.cn/hadoop/hadoop-environment-cluster-setup">hadoop集群部署(yarn)</a>中的，使用s107和s108作为RM双节点）：</p><pre><code class="xml">&lt;!--Configurations for the state-store of ResourceManager--&gt;&lt;property&gt;  &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;  &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;  &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;  &lt;value&gt;10.6.3.109:2181,10.6.3.110:2181,10.6.3.111:2181&lt;/value&gt;  &lt;description&gt;ZooKeeper服务的地址，多个地址使用逗号隔开&lt;/description&gt;&lt;/property&gt;&lt;!--Configurations for HA of ResourceManager--&gt;&lt;property&gt;  &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;  &lt;value&gt;true&lt;/value&gt;  &lt;description&gt;是否启用HA，默认false&lt;/description&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;  &lt;value&gt;rm1,rm2&lt;/value&gt;  &lt;description&gt;最少2个&lt;/description&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;  &lt;value&gt;s107&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;  &lt;value&gt;s108&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;  &lt;value&gt;yarn-ha&lt;/value&gt;  &lt;description&gt;集群HA的id，用于在ZooKeeper上创建节点，区分使用同一个ZooKeeper集群的不同Hadoop集群&lt;/description&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;    &lt;value&gt;s107:8088&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;    &lt;value&gt;s108:8088&lt;/value&gt;&lt;/property&gt;</code></pre><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>可以在s107上直接通过<code>start-yarn.sh</code>启动YARN，这样在s107上会启动ResourceManager，在其他节点上会启动NodeManager。</p><p>需要注意的是s108上不会自己启动ResourceManager，需要手动启动。通过命令<code>yarn-daemon.sh start resourcemanager</code>手动启动。</p><h2 id="管理命令"><a href="#管理命令" class="headerlink" title="管理命令"></a>管理命令</h2><p>对于YARN的管理前面又说到，用的命令是<code>yarn rmadmin</code>，可以检查RM的健康状态、转换Active/Standby状态等，需要使用<code>yarn.resourcemanager.ha.rm-ids</code>参数配置的RM的id作为参数。比如，查看RM状态：</p><pre><code>$ yarn rmadmin -getServiceState rm1active$ yarn rmadmin -getServiceState rm2standby</code></pre><p>其他的命令可以通过<code>yarn rmadmin -help</code>获取。</p><h2 id="Web管理页面"><a href="#Web管理页面" class="headerlink" title="Web管理页面"></a>Web管理页面</h2><p>管理界面就是yarn.resourcemanager.webapp.address.[rm-id]配置的地址，如果访问的是Standby的RM地址，会自动重定向到Active状态的RM地址。About页面除外，可以访问About页面查看当前哪个节点是Active状态，哪个是Standby状态的。</p><hr><p>参考文章</p><ol><li><a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html" target="_blank" rel="noopener">ResourceManager High Availability</a></li></ol><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a></p><p>个人博文: <a href="http://www.howardliu.cn/hadoop/rm-ha">ResourceManager HA 配置</a></p><p>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a></p><p>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/74984123" target="_blank" rel="noopener">ResourceManager HA 配置</a></p>]]></content>
    
    <summary type="html">
    
      ResourceManager（RM）负责跟踪集群中资源使用情况，调度应用程序（比如MapReduce作业）。在Hadoop 2.4之前，ResourceManager存在单点故障，需要通过其他方式实现HA。官方给出的HA方案是Active/Standby两种状态ResourceManager的冗余方式，类似于HDFS的HA方案，也就是通过冗余消除单点故障。
    
    </summary>
    
    
      <category term="hadoop" scheme="http://www.howardliu.cn/categories/hadoop/"/>
    
      <category term="rm" scheme="http://www.howardliu.cn/categories/hadoop/rm/"/>
    
    
      <category term="大数据" scheme="http://www.howardliu.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="hadoop" scheme="http://www.howardliu.cn/tags/hadoop/"/>
    
      <category term="开源" scheme="http://www.howardliu.cn/tags/%E5%BC%80%E6%BA%90/"/>
    
      <category term="ha" scheme="http://www.howardliu.cn/tags/ha/"/>
    
      <category term="rm" scheme="http://www.howardliu.cn/tags/rm/"/>
    
  </entry>
  
  <entry>
    <title>YARN架构</title>
    <link href="http://www.howardliu.cn/hadoop/yarn-architecture/"/>
    <id>http://www.howardliu.cn/hadoop/yarn-architecture/</id>
    <published>2017-07-11T01:41:10.000Z</published>
    <updated>2019-09-11T14:08:23.878Z</updated>
    
    <content type="html"><![CDATA[<p>对Hadoop有过了解的都知道，Hadoop经历过很长一段时间的版本号混乱和架构调整，YARN是Hadoop 2.0（或者早期的0.23.x）提出的资源管理、任务调度框架。解决了很多Hadoop 1.0（或者0.21.x、0.22.x）时代的痛点。</p><p>随着发展，YARN不仅仅是Hadoop的资源调度框架，还成为一个通用的资源调度管理器，可以将各种各样的计算框架通过YARN管理起来，比如Strom、Spark等。</p><p>YARN的基本思想是将资源管理和作业调度/监控的功能分为独立的守护进程。分别是一个全局的 ResourceManager（RM） 和每个应用程序的 ApplicationMaster（AM）。应用程序可以是一个job作业或者一组job作业的有向无环图（DAG）。</p><p>ResourceManager负责系统中的所有应用程序的资源分配。NodeManager负责每台机器中容器代理、资源监控（cpu，内存，磁盘，网络），并将这些情况报告给ResourceManager或Scheduler。</p><p>每个应用的ApplicationMaster是一个框架特定的库，从ResourceManager协商资源，并与NodeManager共同执行监听任务。</p><p>从结构上看，YARN是主/从架构，一个ResourceManager，多个NodeManager，共同构成了数据计算框架。</p><a id="more"></a><p><img src="http://www.howardliu.cn/images/hadoop/yarn_architecture.gif" alt="YARN架构" title="http://www.howardliu.cn YARN架构"></p><p>从上面的结构图来看，YARN主要的组件包括ResourceManager、NodeManager、ApplicationMaster和Container。</p><h1 id="1-ResourceManager（RM）"><a href="#1-ResourceManager（RM）" class="headerlink" title="1. ResourceManager（RM）"></a>1. ResourceManager（RM）</h1><p>ResourceManager负责整个集群的资源管理和分配，包括处理客户端请求、启动和监控ApplicationMaster、监控NodeManager、资源的分配和调度。</p><p>ResourceManager由两个主要组件组成：Scheduler和ApplicationsManager（ASM）。</p><ul><li><p>Scheduler：Scheduler根据容量、队列等限制条件（每个队列分配多少资源、最多执行多少个作业等），向运行的应用程序分配资源。Scheduler是一个单纯的调度器，不负责监控功能或跟踪应用程序状态。另外，如果因为应用程序错误或硬件故障任务失败，它不保证重新启动任务。这些都交给ApplicationMaster完成。</p><p>资源分配单位用一个抽象概念“容器”（Container）表示，容器是一个动态资源分配单位，将诸如内存、cpu、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量</p><p>Scheduler是一个可插拔组件，可以根据自己的需要重新定义。YARN提供了已经实现了多种Scheduler，比如<a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html" target="_blank" rel="noopener">CapacityScheduler</a>和<a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/FairScheduler.html" target="_blank" rel="noopener">FairScheduler</a>。</p></li><li><p>ApplicationsManager：ApplicationsManager负责接收提交的作业，与第一个容器协商来执行应用程序对应的ApplicationMaster，并在容器失败时重启ApplicationMaster。每个应用程序对应的ApplicationMaster负责从Scheduler协商资源容器，并跟踪应用程序状态、监控执行进度。</p></li></ul><h1 id="2-NodeManager（NM）"><a href="#2-NodeManager（NM）" class="headerlink" title="2. NodeManager（NM）"></a>2. NodeManager（NM）</h1><p>NodeManager是YARN集群中每个节点上资源和任务管理器，负责当前节点程序的运行、资源的管理和监控。</p><p>NodeManager定时向ResourceManager发送本节点资源使用情况、容器运行状态等信息。同时，NodeManager需要执行ResourceManager和ApplicationMaster的命令。</p><h1 id="3-ApplicationMaster（AM）"><a href="#3-ApplicationMaster（AM）" class="headerlink" title="3. ApplicationMaster（AM）"></a>3. ApplicationMaster（AM）</h1><p>YARN运行的每个应用程序都会有一个ApplicationMaster。负责协调来自ResourceManager的资源，并通过NodeManager监控容器和资源使用（包括内存、CPU等）。</p><h1 id="4-YARN工作流程"><a href="#4-YARN工作流程" class="headerlink" title="4. YARN工作流程"></a>4. YARN工作流程</h1><ol><li>客户端向ResourceManager提交应用程序，其中包括ApplicationMaster、启动ApplicationMaster的命令、用户程序等；</li><li>ResourceManager为该应用程序分配第一个Container，并与对应NodeManager通信，要求它在这个Container中启动应用程序的ApplicationMaster；</li><li>ApplicationMaster向ResourceManager注册自己，启动成功后与ResourceManager保持心跳；</li><li>ApplicationMaster向ResourceManager申请资源；</li><li>申请资源成功后，由ApplicationMaster进行初始化，然后与NodeManager通信，要求NodeManager启动Container。然后ApplicationMaster与NodeManager保持心跳，从而对NodeManager上运行的任务进行监控和管理；</li><li>Container运行期间，向ApplicationMaster汇报自己的进度和状态信息，以便ApplicationMaster掌握任务运行状态，从而在任务失败是可以重新启动；</li><li>应用运行结束后，ApplicationMaster向ResourceManager注销自己，允许其所属的Container回收。</li></ol><hr><p>参考</p><ol><li><a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">Apache Hadoop YARN</a></li><li><a href="https://hortonworks.com/blog/introducing-apache-hadoop-yarn/" target="_blank" rel="noopener">INTRODUCING APACHE HADOOP YAR</a></li></ol><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a></p><p>个人博文: <a href="http://www.howardliu.cn/hadoop/yarn-architecture">YARN架构</a></p><p>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a></p><p>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/74939382" target="_blank" rel="noopener">YARN架构</a></p>]]></content>
    
    <summary type="html">
    
      YARN的基本思想是将资源管理和作业调度/监控的功能分为独立的守护进程。这样就出现了一个全局的 ResourceManager（RM） 和每个应用程序的 ApplicationMaster（AM）。应用程序可以是一个job作业或者一组job作业的有向无环图（DAG）。ResourceManager负责系统中的所有应用程序的资源分配。NodeManager负责每台机器中容器代理、资源监控（cpu，内存，磁盘，网络），并将这些情况报告给ResourceManager或Scheduler。每个应用的ApplicationMaster是一个框架特定的库，从ResourceManager协商资源，并与NodeManager共同执行监听任务。
    
    </summary>
    
    
      <category term="hadoop" scheme="http://www.howardliu.cn/categories/hadoop/"/>
    
      <category term="yarn" scheme="http://www.howardliu.cn/categories/hadoop/yarn/"/>
    
    
      <category term="大数据" scheme="http://www.howardliu.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="hadoop" scheme="http://www.howardliu.cn/tags/hadoop/"/>
    
      <category term="yarn" scheme="http://www.howardliu.cn/tags/yarn/"/>
    
      <category term="开源" scheme="http://www.howardliu.cn/tags/%E5%BC%80%E6%BA%90/"/>
    
  </entry>
  
  <entry>
    <title>使用QJM实现HDFS的HA</title>
    <link href="http://www.howardliu.cn/hadoop/hdfs-ha-using-qjm/"/>
    <id>http://www.howardliu.cn/hadoop/hdfs-ha-using-qjm/</id>
    <published>2017-07-07T08:24:23.000Z</published>
    <updated>2019-09-11T14:08:23.878Z</updated>
    
    <content type="html"><![CDATA[<p>本文是在<a href="http://www.howardliu.cn/hadoop/hadoop-environment-cluster-setup">hadoop集群部署(yarn)</a>基础上增加的配置内容，因为那篇缺少HDFS的HA配置，在生产环境不够完整。</p><p>hadoop官方提供了两种HDFS的HA配置方案，两种方案殊途同归，但是需要的钱、精力和技术不同。</p><p>如果对HDFS架构熟悉的话（如果不熟悉，可以通过<a href="http://www.howardliu.cn/hadoop/hdfs-architecture">HDFS架构</a>了解），就应该知道，NameNode通过FsImage和EditLog两个文件管理DataNode的数据，Secondary NameNode会定期合并EditLog，以减少NameNode启动时的安全检查。EditLog文件存储的是对文件的一条条的操作，也就是说，只要保证有另外一个NameNode的EditLog文件一直与当前正在运行的NameNode的EditLog文件是一样的，那就可以随时使用新的NameNode替换老的NameNode。官方目前给出的两种HA方案也大体是这样：</p><ul><li>QJM：the Quorum Journal Manager，翻译是法定经济管理人，实在没法想象，所以大家都亲切的称之为QJM。这种方案是通过JournalNode共享EditLog的数据，使用的是Paxos算法（没错，zookeeper就是使用的这种算法），保证活跃的NameNode与备份的NameNode之间EditLog日志一致。</li><li>NFS：Network File System 或 Conventional Shared Storage，传统共享存储，其实就是在服务器挂载一个网络存储（比如NAS），活跃NameNode将EditLog的变化写到NFS，备份NameNode检查到修改就读取过来，是两个NameNode数据一致。</li></ul><blockquote><p>客观的说，Secondary NameNode也算是对NameNode的备份，但是使用Secondary NameNode需要手动处理，不如QJM和NFS两种可以自动处理简单，所以没有被列入HA解决方案中。</p></blockquote><p>但是，这两种方案在部署方式上差别比较大。QJM需要启动几个JournalNode即可，NFS需要挂在一个共享存储。因为条件限制，我只能通过QJM的方式实现HDFS的HA，如果想看NFS方案，可以直接看<a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html" target="_blank" rel="noopener">官方文档</a>。</p><a id="more"></a><h1 id="1-hdfs-site-xml"><a href="#1-hdfs-site-xml" class="headerlink" title="1. hdfs-site.xml"></a>1. hdfs-site.xml</h1><ul><li><p>dfs.nameservices：指定nameservice的名称，这个需要自定义，可以是任意的名称。这个值需要用在后面的配置和HDFS集群管理路径中。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.nameservices&lt;/name&gt;&lt;value&gt;mycluster&lt;/value&gt;&lt;/property&gt;</code></pre></li><li><p>dfs.ha.namenodes.[nameservice ID]：指定集群中两个NameNode的id，目前只能支持最多两个NameNode，所以就需要两个id，以逗号分隔。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;&lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;</code></pre></li><li><p>dfs.namenode.rpc-address.[nameservice ID].[namenode ID]：指定NameNode的rpc地址，用于数据传输。因为有两个NameNode，所以需要给出两个节点。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;&lt;value&gt;s108:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;&lt;value&gt;s109:8020&lt;/value&gt;&lt;/property&gt;</code></pre></li><li><p>dfs.name.http-address.[nameservice ID].[namenode ID]：同3，还需要http地址。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;&lt;value&gt;s108:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;&lt;value&gt;s109:50070&lt;/value&gt;&lt;/property&gt;</code></pre></li><li><p>dfs.namenode.shared.edits.dir：需要提供JournalNode的配置地址，用于活跃NameNode向该位置写变化数据，备份NameNode从该位置读取数据应用与自身。如果配置过Kafka就应该可以理解这个。配置地址格式是：qjournal://host1:port1;hots2:port2;host3:port3/journalId，地址端口为一对，每对之间通过分号隔开，最后的journalId是为了区分不同的nameservice的。也就是说，一组JournalNode可以支撑多个NameNode的HA配置。所以，比较好的配置方式是，journalId与nameservice的名称一致。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;&lt;value&gt;qjournal://s108:8485;s109:8485;s110:8485/mycluster&lt;/value&gt;&lt;/property&gt;</code></pre></li><li><p>dfs.client.failover.proxy.provider.[nameservice ID]：HDFS客户端连接活跃NameNode的方式，配置一个Java类。因为NameNode只有一个是活跃的，也就是只有一个提供服务，另一个是备份。所以客户端需要知道哪个是活跃节点。所以需要某种方式找到这个活跃节点。这里提供一个代理类，目前Hadoop只实现了一个代理类<code>ConfiguredFailoverProxyProvider</code>，也可以自己定义：</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt;</code></pre></li><li><p>dfs.ha.fencing.methods：用于故障转移过程中，在活跃节点执行的一组脚本或Java类。HDFS集群有一条原则是：只能有一个NameNode处于活跃状态。QJM只允许一个NameNode写入JournalNode集群，所以可以避免闹裂的发生。但是故障转移过程中，还可能会有其他的问题，所以需要提供一些防护方法。需要注意的是，如果不想使用具体的防护方法，也必须提供一个脚本，比如<code>shell(/bin/true)</code>。</p><ul><li><p>sshfence：通过ssh方式连接活跃NameNode，并kill掉进程。所以还需要通过<code>dfs.ha.fencing.ssh.private-key-files</code>配置ssh key，还可以通过<code>dfs.ha.fencing.ssh.connect-timeout</code>配置ssh连接超时时间。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;&lt;value&gt;sshfence&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;&lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;&lt;value&gt;30000&lt;/value&gt;&lt;/property&gt;</code></pre><p>如果对于不是标准ssh端口或相同用户的，可以在sshfence后添加用户名和端口，格式为<code>sshfence([[username][:port]])</code>。</p></li><li><p>shell：运行任意的脚本来进行防护。我是使用sshfence方式配置的，所以下面就列出配置格式，具体信息查看<a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="noopener">官网</a>。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;&lt;value&gt;shell(/path/to/my/script.sh arg1 arg2 ...)&lt;/value&gt;&lt;/property&gt;</code></pre></li></ul></li><li><p>dfs.journalnode.edits.dir：JournalNode守护进程存储数据的本地路径。这是启动JournalNode需要配置的配置项。当然整个集群配置相同也不会有不好的影响，需要是本地绝对路径。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;&lt;value&gt;/data/hadoop/journal&lt;/value&gt;&lt;/property&gt;</code></pre></li><li><p>dfs.ha.automatic-failover.enabled：自动故障转移，该配置向需要与core-site.xml中的<code>ha.zookeeper.quorum</code>配合使用。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;</code></pre></li></ul><h1 id="2-core-site-xml"><a href="#2-core-site-xml" class="headerlink" title="2. core-site.xml"></a>2. core-site.xml</h1><ul><li><p>fs.defaultFS：这个在单点NameNode的时候配置过，这里需要再次配置，需要使用hdfs-site.xml中的nameservice名称。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://mycluster&lt;/value&gt;&lt;/property&gt;</code></pre></li><li><p>ha.zookeeper.quorum：这个就是前面提到hdfs-site.xml中配置自动故障转移配合使用的配置项，需要提供zookeeper集群地址</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;&lt;value&gt;s109:2181,s110:2181,s111:2181&lt;/value&gt;&lt;/property&gt;</code></pre></li></ul><h1 id="3-开始启动"><a href="#3-开始启动" class="headerlink" title="3. 开始启动"></a>3. 开始启动</h1><h2 id="3-1-JournalNode"><a href="#3-1-JournalNode" class="headerlink" title="3.1 JournalNode"></a>3.1 JournalNode</h2><p>需要首先启动JournalNode，如上面配置的，需要s108/s109/s110三个节点启动JournalNode，默认端口就是8045。启动命令是<code>hadoop-daemon.sh start journalnode</code>。</p><h2 id="3-2-NameNode数据准备"><a href="#3-2-NameNode数据准备" class="headerlink" title="3.2 NameNode数据准备"></a>3.2 NameNode数据准备</h2><p>JournalNode启动完成后，因为有两个NameNode节点，就需要先同步两个NameNode节点的数据。</p><ol><li>如果是全新的HDFS集群，这个时候直接<code>hdfs namenode -format</code>格式化即可</li><li>已经格式化或是从非HA设置为HA的集群，需要把格式化后的NameNode节点的数据拷贝到为格式化节点上。未格式化NameNode节点执行<code>hdfs namenode -bootstrapStandby</code>命令。</li><li>如果是从非HA到HA的配置，需要执行<code>hdfs namenode -initializeSharedEdits</code>将原有的NameNode日志写入JournalNode中。</li></ol><h2 id="3-3-Zookeeper中的HA状态"><a href="#3-3-Zookeeper中的HA状态" class="headerlink" title="3.3 Zookeeper中的HA状态"></a>3.3 Zookeeper中的HA状态</h2><p>因为上面配置了自动故障转移，所以需要在Zookeeper中初始化HA状态。执行命令<code>hdfs zkfc -formatZK</code>。</p><h2 id="3-4-启动"><a href="#3-4-启动" class="headerlink" title="3.4 启动"></a>3.4 启动</h2><p>直接使用<code>start-dfs.sh</code>命令启动NameNode、DataNode，以及ZKFS进程，启动成功之后就可以通过s108:50070和s109:50070访问web页面查看具体哪个NameNode是Active或Standby状态的了。</p><blockquote><p>启动的时候可以注意到，启动过程没有启动Secondary NameNode，这是用为HA不会启动Secondary NameNode。也就是master配置文件配置内容无效了。</p></blockquote><h1 id="4-管理"><a href="#4-管理" class="headerlink" title="4. 管理"></a>4. 管理</h1><p>可以通过<code>hdfs haadmin</code>命令进行管理。具体查看<a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="noopener">官网</a>说明。</p><hr><p>参考</p><ol><li><a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="noopener">HDFS High Availability Using the Quorum Journal Manager</a></li><li><a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html" target="_blank" rel="noopener">HDFS High Availability</a></li></ol><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a></p><p>个人博文: <a href="http://www.howardliu.cn/hadoop/hdfs-ha-using-qjm">使用QJM实现HDFS的HA</a></p><p>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a></p><p>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/74910417" target="_blank" rel="noopener">使用QJM实现HDFS的HA</a></p>]]></content>
    
    <summary type="html">
    
      hadoop官方提供了两种HDFS的HA配置方案，两种方案殊途同归，但是需要的钱、精力和技术不同。如果对HDFS架构熟悉的话，就应该知道，NameNode通过FsImage和EditLog两个文件管理DataNode的数据，Secondary NameNode会定期合并EditLog，以减少NameNode启动时的安全检查。EditLog文件存储的是对文件的一条条的操作，也就是说，只要保证有另外一个NameNode的EditLog文件一直与当前正在运行的NameNode的EditLog文件是一样的，那就可以随时使用新的NameNode替换老的NameNode。官方目前给出的两种HA方案也大体是这样。QJM：the Quorum Journal Manager，翻译是法定经济管理人，实在没法想象，所以大家都亲切的称之为QJM。这种方案是通过JournalNode共享EditLog的数据，使用的是Paxos算法（没错，zookeeper就是使用的这种算法），保证活跃的NameNode与备份的NameNode之间EditLog日志一致。NFS：Network File System 或 Conventional Shared Storage，传统共享存储，其实就是在服务器挂载一个网络存储（比如NAS），活跃NameNode将EditLog的变化写到NFS，备份NameNode检查到修改就读取过来，是两个NameNode数据一致。
    
    </summary>
    
    
      <category term="hadoop" scheme="http://www.howardliu.cn/categories/hadoop/"/>
    
      <category term="hdfs" scheme="http://www.howardliu.cn/categories/hadoop/hdfs/"/>
    
    
      <category term="大数据" scheme="http://www.howardliu.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="hadoop" scheme="http://www.howardliu.cn/tags/hadoop/"/>
    
      <category term="开源" scheme="http://www.howardliu.cn/tags/%E5%BC%80%E6%BA%90/"/>
    
      <category term="hdfs" scheme="http://www.howardliu.cn/tags/hdfs/"/>
    
      <category term="ha" scheme="http://www.howardliu.cn/tags/ha/"/>
    
      <category term="QJM" scheme="http://www.howardliu.cn/tags/QJM/"/>
    
  </entry>
  
  <entry>
    <title>HDFS架构</title>
    <link href="http://www.howardliu.cn/hadoop/hdfs-architecture/"/>
    <id>http://www.howardliu.cn/hadoop/hdfs-architecture/</id>
    <published>2017-07-05T08:46:33.000Z</published>
    <updated>2019-09-11T14:08:23.879Z</updated>
    
    <content type="html"><![CDATA[<p>前段时间搭建了一套Hadoop集群的测试环境，因为服务器故障，废了。这几天闲来无事，想着把Storm用Yarn管理起来，于是再来一遍，同时也梳理下Hadoop组件中的一些概念。所谓书读百遍其义自见，不熟的系统多搭几遍，总会熟悉了，也就是所谓的刻意练习吧。</p><p>先简单的说下。</p><p>Hadoop文件存储的基础是HDFS(Hadoop Distributed File System)，HDFS的实现依赖于NameNode和DataNode，DataNode用来存储具体数据，NameNode用来管理多个DataNode中分别存储的是什么。</p><p>理解起来也不难，因为HDFS是分布式的文件系统，也就是有很多机器用来存储数据，一个大文件可能分布在多个机器上，也可能是一台机器上，具体分布在哪些或哪个机器上，每块数据块的副本在哪，得需要一个总管来管理，这个总管就是NameNode，具体存储机器的就是DataNode。</p><p>简单的说完了，接下来就复杂的说。</p><a id="more"></a><h1 id="1-HDFS架构"><a href="#1-HDFS架构" class="headerlink" title="1. HDFS架构"></a>1. HDFS架构</h1><p>HDFS是主/从架构，一个HDFS只有一个NameNode（这里的一个是指起作用的只有一个，在NameNode的HA状态下，也是只有一个NameNode起作用，其他的NameNode节点只是备份，家有千口，主事一人嘛）。这个NameNode管理整个文件系统的命名空间（The File System Namespace），并且调节客户端对文件的访问。DataNode用来存储数据，还有一些DataNode用来管理连接到运行节点的存储。HDFS公开文件系统的命名空间，允许用户在文件系统中存储数据。</p><p>在HDFS内部，一个文件被分割为一个或多个数据块，这些数据块存储在一组DataNode上。NameNode负责文件系统的命名空间操作，比如对文件或文件夹执行打开、关闭，以及重命名操作，同时，也负责将数据块映射到DataNode上。DataNode负责向客户端提供对文件系统的读取和写入请求的处理。DataNode还能根据NameNode的指示，执行数据块的创建、删除和复制操作。</p><p><img src="http://www.howardliu.cn/images/hadoop/hdfsarchitecture.gif" alt="HDFS架构"></p><p>NameNode和DataNode通常运行在普通商用服务器上（这里是针对小型机这种又贵很又特别贵的机器说的），这些机器一般装的是Linux。HDFS使用Java实现的，而且Java号称是一处编译到处运行，也就是只要有Java环境，就能运行NameNode和DataNode。典型的部署方式是，一台机器运行NameNode，其他机器运行DataNode，每台机器一个进程。（当然也可能一台机器上有多个DataNode，但是生产环境应该不会有这种情况，毕竟没有任何好处）</p><p>HDFS的这种主从架构保证了，所有的数据都会从NameNode经过，这样也简化了系统的体系结构，NameNode就是老大，所有数据都经过他。当然老大也可能出现意外，就得需要其他HA的部署方式（找几个小弟时刻准备接替老大），这个后面再说。</p><h1 id="2-文件系统"><a href="#2-文件系统" class="headerlink" title="2. 文件系统"></a>2. 文件系统</h1><p>HDFS文件系统设计与传统分层文件系统类似，有文件夹，文件夹里可以有文件夹或文件。客户端可以对这些文件夹或文件进行操作，比如创建、删除、移动、重命名等。不过目前还不支持软连接和硬连接，后面可能会支持。</p><p>如前面所说，这个命名空间由NameNode维护。所有的文件/文件夹的名字、属性、复制因子都由NameNode记录。</p><h1 id="3-数据复制"><a href="#3-数据复制" class="headerlink" title="3. 数据复制"></a>3. 数据复制</h1><p>HDFS设计用于在大型集群中存储非常大的文件，它将每个文件存储为一组数据块块中，除了最后一块外，所有文件块大小相同（默认是64MB），HDFS通过复制数据块块进行容错，每个文件的数据块大小、复制因子都是可配的。客户端可以指定文件的副本数，可以在创建时指定，也可以在之后修改。HDFS中的文件是一次写入的，并且在任何时候都有一个明确的作者。</p><p>NameNode来管理所有数据块复制操作，它定期与集群中每个DataNode进行心跳和文件块报告。收到心跳意味着DataNode正常运行，文件块报告是DataNode上所有文件块的列表。</p><p><img src="http://www.howardliu.cn/images/hadoop/hdfsdatanodes.gif" alt="HDFS DataNodes"></p><h2 id="3-1-副本存储"><a href="#3-1-副本存储" class="headerlink" title="3.1 副本存储"></a>3.1 副本存储</h2><p>副本存储的位置对于HDFS的可靠性和性能是至关重要的，优化副本存储是HDFS与大多数其他分布式文件系统区分开来的优点之一，这是一个经过大量调整和体验的功能。机架感知副本存储策略的目的是提高数据可靠性、可用性和网络带宽的高利用率。目前的副本存储策略是朝这个方向努力的第一步。实施这一政策的短期目标是对生产系统进行验证，更多的了解其行为，并为更加复杂的策略进行测试和研究奠定基础。</p><p>大型的HDFS实例通常运行在多个机架上，不同机架中的两个节点之间通信必须通过交换机。通产，同一机架上的机器之间的网络速率会优于不同机架间的速率。</p><p>NameNode通过<a href="https://hadoop.apache.org/docs/r1.2.1/cluster_setup.html#Hadoop+Rack+Awareness" target="_blank" rel="noopener">Hadoop 机架感知</a>中介绍的方式确定每个DataNode所属的机架ID。一个简单但不是最优策略是将副本存储在不同的机架上，这样可以防止整个机架故障导致数据丢失，并允许读取数据时使用多个机架的带宽。该策略保证可以在集群中均匀分配副本，可以轻松平衡组件故障时的负载。但是，这种策略会因为需要将副本数据写到多个机架，而增加写入成本。</p><p>常见情况中，复制因子是3，HDFS的副本存储策略是将一个副本放置在当前机架上的一个节点上，另一个位于另一个机架上的某个节点上，最后一个放置第二个机架上的另一个节点上。这种策略减少了机架间的通信，从而提高写入性能。机架故障的概率远小于节点故障的概率，这种策略不会影响数据可靠性和可用性。但是，它会降低读取数据的网络带宽，因为数据块放置在两个机架上，而不是三个。使用这种策略，副本不会均匀的分布在机架中。还有一种比较好的策略，就是三分之一的副本在一个节点上，三分之一的副本在另一个机架上，其余的均匀分布在其他机器上，这种方式可以改善写入策略，不会影响数据可靠性或读取性能。不过这种方式还在开发中。</p><h2 id="3-2-副本选择"><a href="#3-2-副本选择" class="headerlink" title="3.2 副本选择"></a>3.2 副本选择</h2><p>为了最大限度的减少全局带宽消耗和读取延迟，HDFS尝试让读取器读取最接近的副本。如果在同一机架上存在副本，就优先读取该副本。如果HDFS跨越多个数据中心，则优先读取当前数据中心的副本。</p><h2 id="3-3-安全模式（SafeMode）"><a href="#3-3-安全模式（SafeMode）" class="headerlink" title="3.3 安全模式（SafeMode）"></a>3.3 安全模式（SafeMode）</h2><p>在启动的时候，NameNode进入一个成为安全模式的特殊状态。NameNode处于安全模式时，不会复制数据块。NameNode从DataNode节点接收心跳和数据块报告，数据块报告包含DataNode存储的数据块列表信息。每个数据块有指定的最小数量副本。当使用NameNode检入该数据块的最小副本数量时，数据块被认为是安全复制的。在NameNode检查数据块的安全复制结束后（可配的时间段，再加上30秒），NameNode退出安全模式。然后它开始检查是否有少于指定数量的数据块列表，将这些数据块复制到其他DataNode。</p><h1 id="4-文件系统元数据"><a href="#4-文件系统元数据" class="headerlink" title="4. 文件系统元数据"></a>4. 文件系统元数据</h1><p>HDFS由NameNode存储命名空间信息，NameNode使用名为EditLog的事务日志持续记录文件系统元数据发生的每个更改。比如，在HDFS中创建一个新的文件会导致NameNode在EditLog中插入一条记录，用于记录该操作。同样的，修改文件的复制因子也会新增一条记录。NameNode使用本机文件系统存储EditLog文件。整个文件系统命名空间（包括块映射文件和文件系统属性等信息）存储在一个名为FsImage的文件中，该文件也是存储在NameNode所在的本机文件系统中。</p><p>NameNode在内存中存储整个文件系统的命名空间和文件数据块映射关系，这个元数据存储模块设计为紧凑存储，可以在4GB内存的NameNode中可以支持大量的文件和文件夹。在NameNode启动时，它从磁盘读取FsImage和EditLog，将所有从EditLog的事务应用到FsImage的内存表示中，并将这个新版本的FsImage文件中。该操作会截断旧的EditLog，因为其中的事务操作已经应用到FsImage文件中。这个过程成为检查点（checkpoint）。在当前版本中，检查点操作只会发生在NameNode启动时，正在增加定期检查点检查操作。</p><p>DataNode将HDFS数据存储在其本地文件系统的文件中，DataNode不知道HDFS的文件数据，它只是将HDFS文件数据的数据块存储在本地文件系统中的单独文件中。DataNode不会存储同一文件夹下的所有文件，相反，它会确定每个文件夹的最佳文件数量，并适当的创建子文件夹。在同一文件夹中创建所有本地文件是不合适的，因为本地文件系统可能无法在单个文件夹中有效支持存储大量文件。当DataNode启动时，它会扫描其本地文件系统，生产与每个文件对应的所有HDFS数据块数控报告，并发送给NameNode。</p><h1 id="5-通信协议"><a href="#5-通信协议" class="headerlink" title="5. 通信协议"></a>5. 通信协议</h1><p>所有HDFS的通信协议是基于TCP/IP协议的。客户端通过客户端协议与NameNode建立连接，并通信。DataNode通过DataNode协议与NameNode通信。NameNode从不发起通信请求，它只响应客户端和DataNode发出的请求。</p><h1 id="6-稳健性"><a href="#6-稳健性" class="headerlink" title="6. 稳健性"></a>6. 稳健性</h1><p>HDFS的主要目标是可靠性存储，即使发生故障，也能够很好的支撑存储。比较常见的故障类型有：NameNode故障、DataNode故障、网络分区异常。</p><h2 id="6-1-数据磁盘故障，心跳和重新复制"><a href="#6-1-数据磁盘故障，心跳和重新复制" class="headerlink" title="6.1 数据磁盘故障，心跳和重新复制"></a>6.1 数据磁盘故障，心跳和重新复制</h2><p>每个DataNode会定期向NameNode发送心跳请求。网络分区可能导致一部分DataNode与NameNode失去连接，NameNode通过心跳检测这种情况，如果发现长时间没有心跳的DataNode，则标记为丢弃，而且不再转发任何新的IO请求，这些DataNode的数据也不在作用于HDFS。DataNode的死亡可能导致某些数据块的复制因子低于设定值，NameNode会不断扫描哪些数据块需要复制，并在必要的时候进行复制。重复复制可能是因为DataNode不可用、副本文件损坏、DataNode硬盘损坏，或者文件复制因子增加。</p><h2 id="6-2-集群重新平衡"><a href="#6-2-集群重新平衡" class="headerlink" title="6.2 集群重新平衡"></a>6.2 集群重新平衡</h2><p>HDFS架构与数据重新平衡方案兼容，如果DataNode上的可用空间低于阀值，数据会从一个DataNode移动到另一个DataNode。在对特定文件的需求突然增加的情况下，可能会动态创建其他副本来重新平衡集群中的数据。（这种类型的数据重新平衡还未实现）。</p><h2 id="6-3-数据的完整性"><a href="#6-3-数据的完整性" class="headerlink" title="6.3 数据的完整性"></a>6.3 数据的完整性</h2><p>DataNode存储的数据块可能会被破坏，这可能是因为存储设备故障、网络故障或软件错误导致的。HDFS客户端会对HDFS文件内容进行校验和检查。当客户端创建一个HDFS文件时，它会计算文件的每个块的检验码，然后将该校验码存储在同一HDFS命名空间中的隐藏文件中。当客户端操作文件时，会将从DataNode接收到的数据与存储的校验文件进行校验和匹配。如果没有成功，客户端会从该数据块的其他副本读取数据。</p><h2 id="6-4-元数据磁盘故障"><a href="#6-4-元数据磁盘故障" class="headerlink" title="6.4 元数据磁盘故障"></a>6.4 元数据磁盘故障</h2><p>FsImage和EditLog是HDFS的核心数据结构，如果这些文件被损坏，可能导致HDFS实例不可用。因此，NameNode可以配置维护FsImage和EditLog的多个副本。对FsImage或EditLog的任何更新，这些副本也跟着更新。当时这样可能会导致NameNode的TPS。但是，这种性能的降低是可以接受的，因为即使HDFS应用程序本质上是数据密集型，但却不是元数据密集型的。当NameNode重启时，它会选择最新的FsImage和EditLog使用。</p><p>比较常见的多备份存储是一份存在本地文件系统中，一份存储在远程网络文件系统中。</p><p>因为NameNode单点的，所以可能会出现HDFS集群的单点故障，需要手动干预。这个时候就需要NameNode的HA方案，这个后面再说。</p><h2 id="6-5-Secondary-NameNode"><a href="#6-5-Secondary-NameNode" class="headerlink" title="6.5 Secondary NameNode"></a>6.5 Secondary NameNode</h2><p>前面说过，NameNode是单点的，如果NameNode宕机，系统中的文件会丢失，所以Hadoop提供还提供了一种机制：Secondary NameNode。</p><p>Secondary NameNode比较有意思，它并不能直接作为NameNode使用，它只是定期将FsImage和EditLog合并，防止EditLog文件过大。通常Secondary NameNode也是运行在单独的机器上，因为日志合并操作需要占用大量的内存和CPU。Secondary NameNode会保存合并后的元数据，一旦NameNode宕机，而且元数据不可用，就可以使用Secondary NameNode备份的数据。</p><p>但是，Secondary NameNode总是落后与NameNode，所以在NameNode宕机后，数据丢失是不可避免的。一般这种情况就会使用元数据的远程备份与Secondary NameNode中的合并数据共同作为NameNode的元数据文件。</p><h2 id="6-6-快照"><a href="#6-6-快照" class="headerlink" title="6.6 快照"></a>6.6 快照</h2><p><a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html" target="_blank" rel="noopener">快照功能</a>支持在特定时刻存储数据副本，可以在出现数据损坏的情况下，将HDFS回滚到之前的时间点。（目前还不支持，之后会增加）</p><h1 id="7-数据"><a href="#7-数据" class="headerlink" title="7 数据"></a>7 数据</h1><h2 id="7-1-数据块"><a href="#7-1-数据块" class="headerlink" title="7.1 数据块"></a>7.1 数据块</h2><p>HDFS是为了存储大文件的。与HDFS兼容的程序是处理大型数据集的，这些应用通常是一次写入，多次读取，并且要求在读取速率上满足要求。HDFS恰好满足“一次写入、多次读取”的要求。在HDFS中，数据块大小默认是64MB。所以，大文件在HDFS内存切成64MB的数据块，每个数据块会存储在不同的DataNode上。</p><h2 id="7-2-阶段"><a href="#7-2-阶段" class="headerlink" title="7.2 阶段"></a>7.2 阶段</h2><p>客户端创建文件的请求不会立即到达NameNode。一开始，客户端将文件缓存到临时文件中，写入操作透明的重定向到该临时文件中。当临时文件超过一个HDFS数据块的大小时，客户端将连接NameNode，NameNode会把文件名插入到文件系统中，为其分配一个数据块。NameNode使用DataNode和目标数据块响应客户端请求。然后客户端将数据块信息从本地临时文件刷新到指定的DataNode中。关闭文件时，临时文件中剩余的未刷新数据将传输到DataNode，然后，客户端通知NameNode文件关闭，此时，NameNode将文件创建操作提交到持久存储。如果NameNode在文件关闭前宕机， 则文件丢失。</p><p>HDFS采用上述方式是经过仔细考虑的，这些应用需要流式写入文件，如果客户端写入远程文件时没有使用客户端缓冲，网速和网络情况将会大大影响吞吐率。这种做法也是有先例的，早起的分布式文件系统也是采用这种方式，比如ASF。POSIX要求已经放宽，以便实现高性能的数据上传。</p><h2 id="7-3-复制流水线"><a href="#7-3-复制流水线" class="headerlink" title="7.3 复制流水线"></a>7.3 复制流水线</h2><p>如上所述，当客户端数据写入HDFS时，首先将数据写入本地缓存文件。如果HDFS文件的复制因子是3，当本地缓存文件大小累积到数据块大小时，客户端从NameNode查询写入副本的DataNode列表。然后，客户端将数据块刷新到第一个DataNode中，这个DataNode以小数据（4KB）接收数据，将数据写入本地存储库，并将该部分数据传输到第二个DataNode，第二个DataNode存储后，将数据发送给第三个DataNode。因此，DataNode可以在流水线中接收来自前一个的数据，同时将数据发送给流水线的下一个，以此类推。</p><h1 id="8-易使用"><a href="#8-易使用" class="headerlink" title="8 易使用"></a>8 易使用</h1><p>可以以多种方式访问HDFS，HDFS提供了一个<a href="http://hadoop.apache.org/core/docs/current/api/" target="_blank" rel="noopener">Java API</a>，C语言包装器也可用。也可以使用浏览器浏览HDFS实例文件。（目前正在通过使用WebDAV协议公开HDFS）。</p><h2 id="8-1-FS-Shell"><a href="#8-1-FS-Shell" class="headerlink" title="8.1 FS Shell"></a>8.1 FS Shell</h2><p>HDFS允许以文件和文件夹的形式组织数据，提供了一个名为“FS shell”的命令行界面，可让用户与HDFS中的数据进行交互。该命令集语法类似与用户熟悉的其他shell（比如bash，csh）。以下是一些实例：</p><table><thead><tr><th>操作</th><th>命令</th></tr></thead><tbody><tr><td>创建/foodir文件夹</td><td>bin/hadoop dfs -mkdir /foodir</td></tr><tr><td>删除/foodir文件夹</td><td>bin/hadoop fs -rm -R /foodir</td></tr><tr><td>查看文件/foodir/myfile.txt的内容</td><td>bin/hadoop dfs -cat /foodir/myfile.txt</td></tr></tbody></table><p>FS shell针对需要脚本语言与存储数据进行交互的应用场景。</p><h2 id="8-2-DFSAdmin"><a href="#8-2-DFSAdmin" class="headerlink" title="8.2 DFSAdmin"></a>8.2 DFSAdmin</h2><p>DFSAdmin命令集用于管理HDFS集群，这些应该是由HDFS管理员使用的命令，以下是一些实例：</p><table><thead><tr><th>操作</th><th>命令</th></tr></thead><tbody><tr><td>集群置为安全模式</td><td>bin/hadoop dfsadmin -safemode enter</td></tr><tr><td>生成DataNode列表</td><td>bin/hadoop dfsadmin -report</td></tr><tr><td>启动或停止DataNode</td><td>bin/hadoop dfsadmin -refreshNodes</td></tr></tbody></table><h2 id="8-3-浏览器接口"><a href="#8-3-浏览器接口" class="headerlink" title="8.3 浏览器接口"></a>8.3 浏览器接口</h2><p>典型的HDFS安装会配置web服务器，以配置TCP端口公开HDFS命名空间，这允许用户浏览HDFS命名空间，并使用web浏览器查看文件内容。</p><h1 id="9-空间复用"><a href="#9-空间复用" class="headerlink" title="9 空间复用"></a>9 空间复用</h1><h2 id="9-1-文件删除"><a href="#9-1-文件删除" class="headerlink" title="9.1 文件删除"></a>9.1 文件删除</h2><p>如果回收站功能可用，通过<a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html#rm" target="_blank" rel="noopener">FS Shell</a>删除的文件不会立马删除，而是移动到回收站中（每个用户的回收文件夹不同，是在各自的用户目录的.Trash中，即/user/<username>/.Trash）。只要文件还保留在回收站中，就可以很快恢复。</username></p><p>大多数的删除文件会移动到回收站的current目录（即/user/<username>/.Trash/Current）中，在配置的时间内，HDFS创建会创建检查点（/user/<username>/.Trash/<date>），并在旧的检查点过期后删除。具体可以查看<a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html#expunge" target="_blank" rel="noopener">删除操作</a>。</date></username></username></p><p>文件在回收站的时间过期后，NameNode会从HDFS命名空间中删除文件，同时，释放相关联的数据块空间。</p><p>文件删除后，客户端可以取消删除。可以通过浏览/trash文件夹，查找文件。/trash文件夹中包含被删除文件的最新副本。目前HDFS会从/trash删除超过6小时的文件。（/trash类似于文件系统中的回收站、垃圾桶等功能）。</p><p>下面是使用FS Shell删除文件的操作：</p><p>在当前目录中创建两个文件：test1、test2</p><pre><code>$ hadoop fs -mkdir -p delete/test1$ hadoop fs -mkdir -p delete/test2$ hadoop fs -ls delete/Found 2 itemsdrwxr-xr-x   - hadoop hadoop          0 2015-05-08 12:39 delete/test1drwxr-xr-x   - hadoop hadoop          0 2015-05-08 12:40 delete/test2</code></pre><p>删除test1：</p><pre><code>$ hadoop fs -rm -r delete/test1Moved: hdfs://localhost:8020/user/hadoop/delete/test1 to trash at: hdfs://localhost:8020/user/hadoop/.Trash/Current</code></pre><p>使用<code>skipTrash</code>选项删除test2：</p><pre><code>$ hadoop fs -rm -r -skipTrash delete/test2Deleted delete/test2</code></pre><p>查看回收站中的文件只有test1：</p><pre><code>$ hadoop fs -ls .Trash/Current/user/hadoop/delete/Found 1 items\drwxr-xr-x   - hadoop hadoop          0 2015-05-08 12:39 .Trash/Current/user/hadoop/delete/test1</code></pre><p>也就是test1进入回收站，test2之间删除了。</p><h2 id="9-2-减少复制因子"><a href="#9-2-减少复制因子" class="headerlink" title="9.2 减少复制因子"></a>9.2 减少复制因子</h2><p>当文件的复制因子减少后，NameNode选择可以删除的多余副本，在下一个心跳时将信息传输给DataNode，DataNode删除相应的数据块，并释放空间。调用setReplication API和集群使用可用空间可能会有延迟。</p><hr><p>参考</p><ol><li><a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html" target="_blank" rel="noopener">HDFS Architecture</a></li></ol><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a></p><p>个人博文: <a href="http://www.howardliu.cn/hadoop/hdfs-architecture">HDFS架构</a></p><p>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a></p><p>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/74456146" target="_blank" rel="noopener">HDFS架构</a></p>]]></content>
    
    <summary type="html">
    
      Hadoop文件存储的基础是HDFS(Hadoop Distributed File System)，HDFS的实现依赖于NameNode和DataNode，DataNode用来存储具体数据，NameNode用来管理多个DataNode中分别存储的是什么。理解起来也不难，因为HDFS是分布式的文件系统，也就是有很多机器用来存储数据，一个大文件可能分布在多个机器上，也可能是一台机器上，具体分布在哪些或哪个机器上，主文件在哪，备份文件在哪，得需要一个总管来管理，这个总管就是NameNode，具体存储机器的就是DataNode。
    
    </summary>
    
    
      <category term="hadoop" scheme="http://www.howardliu.cn/categories/hadoop/"/>
    
      <category term="hdfs" scheme="http://www.howardliu.cn/categories/hadoop/hdfs/"/>
    
    
      <category term="大数据" scheme="http://www.howardliu.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="hadoop" scheme="http://www.howardliu.cn/tags/hadoop/"/>
    
      <category term="开源" scheme="http://www.howardliu.cn/tags/%E5%BC%80%E6%BA%90/"/>
    
      <category term="hdfs" scheme="http://www.howardliu.cn/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>JVM参数优化（基础篇）</title>
    <link href="http://www.howardliu.cn/java/jvm-tuning-basic/"/>
    <id>http://www.howardliu.cn/java/jvm-tuning-basic/</id>
    <published>2017-06-27T08:17:23.000Z</published>
    <updated>2019-09-11T14:08:23.873Z</updated>
    
    <content type="html"><![CDATA[<p>这几天压测预生产环境，发现TPS各种不稳。因为是重构的系统，据说原来的系统在高并发的时候一点问题没有，结果重构的系统被几十个并发压一下就各种不稳定。虽然测试的同事没有说啥，但自己感觉被啪啪的打脸。。。</p><p>于是各种排查，最先想到的就是JVM参数，于是优化一番，希望能够出一个好的结果。尽管后来证明不稳定的原因是安装LoadRunner的压测服务器不稳定，不关我的系统的事，不过也是记录一下，一是做个备份，二是可以给别人做个参考。</p><a id="more"></a><h1 id="写在前面的话"><a href="#写在前面的话" class="headerlink" title="写在前面的话"></a>写在前面的话</h1><p>因为Hotspot JDK提供的参数默认值，在小版本之间不断变化，参数之间也会互相影响。而且，服务器配置不同，都可能影响最后的效果。所以千万不要迷信网上的某篇文章（包括这篇）里面的参数配置，一切的配置都需要自己亲身测试一番才能用。针对于JVM参数默认值不断变化，可以使用<code>-XX:+PrintFlagsFinal</code>打印当前环境JVM参数默认值，比如：<code>java -XX:PrintFlagsFinal -version</code>，也可以用<code>java [生产环境参数] -XX:+PrintFlagsFinal –version | grep [待查证的参数]</code>查看具体的参数数据。</p><p><a href="http://www.howardliu.cn/files/java/global-flags.txt">这里</a>是一个8G服务器的参数，JDK版本信息如下：</p><pre><code>java version &quot;1.8.0_73&quot;Java(TM) SE Runtime Environment (build 1.8.0_73-b02)Java HotSpot(TM) 64-Bit Server VM (build 25.73-b02, mixed mode)</code></pre><h1 id="堆设置"><a href="#堆设置" class="headerlink" title="堆设置"></a>堆设置</h1><p>堆内存设置应该算是一个Java程序猿的基本素养，最少也得修改过Xms、Xmx、Xmn这三个参数了。但是一个2G堆大小的JVM，可能总共占用多少内存的？</p><blockquote><p>堆内存 ＋ 线程数 ＊ 线程栈 ＋ 永久代 ＋ 二进制代码 ＋ 堆外内存</p><p>2G + 1000 * 1M + 256M + 48/240M + (～2G) = 5.5G (3.5G)</p><ul><li>堆内存： 存储Java对象，默认为物理内存的1/64</li></ul></blockquote><ul><li>线程栈： 存储局部变量（原子类型，引用）及其他，默认为1M</li><li>永久代： 存储类定义及常量池，注意JDK7/8的区别</li><li>二进制代码：JDK7与8，打开多层编译时的默认值不一样，从48到240M</li><li>堆外内存： 被Netty，堆外缓存等使用，默认最大值约为堆内存大小</li></ul><p>也就是说，堆内存设置为2G，那一个有1000个线程的JVM可能需要占5.5G，在考虑系统占用、IO占用等等各种情况，一台8G的服务器，也就启动一个服务了。当然，如果线程数少、并发不高、压力不大，还是可以启动多个，而且也可以把堆内存降低。</p><ol><li>-Xms2g 与 -Xmx2g：堆内存大小，第一个是最小堆内存，第二个是最大堆内存，比较合适的数值是2-4g，再大就得考虑GC时间</li><li>-Xmn1g 或 （-XX:NewSize=1g 和 -XX:MaxNewSize=1g） 或 -XX:NewRatio=1：设置新生代大小，JDK默认新生代占堆内存大小的1/3，也就是<code>-XX:NewRatio=2</code>。这里是设置的1g，也就是<code>-XX:NewRatio=1</code>。可以根据自己的需要设置。</li><li>-XX:MetaspaceSize=128m 和 -XX:MaxMetaspaceSize=512m，JDK8的永生代几乎可用完机器的所有内存，为了保护服务器不会因为内存占用过大无法连接，需要设置一个128M的初始值，512M的最大值保护一下。</li><li>-XX:SurvivorRatio：新生代中每个存活区的大小，默认为8，即1/10的新生代， 1/(SurvivorRatio+2)，有人喜欢设小点省点给新生代，但要避免太小使得存活区放不下临时对象而要晋升到老生代，还是从GC Log里看实际情况了。</li><li>-Xss256k：在堆之外，线程占用栈内存，默认每条线程为1M。存放方法调用出参入参的栈、局部变量、标量替换后的局部变量等，有人喜欢设小点节约内存开更多线程。但反正内存够也就不必要设小，有人喜欢再设大点，特别是有JSON解析之类的递归调用时不能设太小。</li><li>-XX:MaxDirectMemorySize：堆外内存/直接内存的大小，默认为堆内存减去一个Survivor区的大小。</li><li>-XX:ReservedCodeCacheSize：JIT编译后二进制代码的存放区，满了之后就不再编译。默认开多层编译240M，可以在JMX里看看CodeCache的大小。</li></ol><h1 id="GC设置"><a href="#GC设置" class="headerlink" title="GC设置"></a>GC设置</h1><p>目前比较主流的GC是CMS和G1，有大神建议以8G为界。（据说JDK 9默认的是G1）。因为应用设置的内存都比较小，所以选择CMS收集器。下面的参数也是针对CMS收集器的，等之后如果有需要，再补充G1收集器的参数。</p><h2 id="CMS设置"><a href="#CMS设置" class="headerlink" title="CMS设置"></a>CMS设置</h2><ol><li>-XX:+UseConcMarkSweepGC：启用CMS垃圾收集器</li><li>-XX:CMSInitiatingOccupancyFraction=80 与 -XX:+UseCMSInitiatingOccupancyOnly：两个参数需要配合使用，否则第一个参数的75只是一个参考值，JVM会重新计算GC的时间。</li><li>-XX:MaxTenuringThreshold=15：对象在Survivor区熬过多少次Young GC后晋升到年老代，默认是15。Young GC是最大的应用停顿来源，而新生代里GC后存活对象的多少又直接影响停顿的时间，所以如果清楚Young GC的执行频率和应用里大部分临时对象的最长生命周期，可以把它设的更短一点，让其实不是临时对象的新生代长期对象赶紧晋升到年老代。</li><li>-XX:-DisableExplicitGC：允许使用System.gc()主动调用GC。这里需要说明下，有的JVM优化建议是设置-XX:-DisableExplicitGC，关闭手动调用System.gc()。这是应为System.gc()是触发Full GC，频繁的Full GC会严重影响性能。但是很多NIO框架，比如Netty，会使用堆外内存，如果没有Full GC的话，堆外内存就无法回收。如果不主动调用System.gc()，就需要等到JVM自己触发Full GC，这个时候，就可能引起长时间的停顿（STW），而且机器负载也会升高。所以不能够完全禁止System.gc()，又得缩短Full GC的时间，那就使用<code>-XX:+ExplicitGCInvokesConcurrent</code>或<code>-XX:+ExplicitGCInvokesConcurrentAndUnloadsClasses</code>选项，使用CMS收集器来触发Full GC。这两个选项需要配合<code>-XX:+UseConcMarkSweepGC</code>使用。</li><li>-XX:+ExplicitGCInvokesConcurrent：使用System.gc()时触发CMS GC，而不是Full GC。默认是不开启的，只有使用-XX:+UseConcMarkSweepGC选项的时候才能开启这个选项。</li><li>-XX:+ExplicitGCInvokesConcurrentAndUnloadsClasses：使用System.gc()时，永久代也被包括进CMS范围内。只有使用-XX:+UseConcMarkSweepGC选项的时候才能开启这个选项。</li><li>-XX:+ParallelRefProcEnabled：默认为false，并行的处理Reference对象，如WeakReference，除非在GC log里出现Reference处理时间较长的日志，否则效果不会很明显。</li><li>-XX:+ScavengeBeforeFullGC：在Full GC执行前先执行一次Young GC。</li><li>-XX:+UseGCOverheadLimit： 限制GC的运行时间。如果GC耗时过长，就抛OOM。</li><li>-XX:+UseParallelGC：设置并行垃圾收集器</li><li>-XX:+UseParallelOldGC：设置老年代使用并行垃圾收集器</li><li>-XX:-UseSerialGC：关闭串行垃圾收集器</li><li>-XX:+CMSParallelInitialMarkEnabled 和 -XX:+CMSParallelRemarkEnabled：降低标记停顿</li><li>-XX:+CMSScavengeBeforeRemark：默认为关闭，在CMS remark前，先执行一次minor GC将新生代清掉，这样从老生代的对象引用到的新生代对象的个数就少了，停止全世界的CMS remark阶段就短一些。如果看到GC日志里remark阶段的时间超长，可以打开此项看看有没有效果，否则还是不要打开了，白白多了次YGC。</li><li>-XX:CMSWaitDuration=10000：设置垃圾收集的最大时间间隔，默认是2000。</li><li>-XX:+CMSClassUnloadingEnabled：在CMS中清理永久代中的过期的Class而不等到Full GC，JDK7默认关闭而JDK8打开。看自己情况，比如有没有运行动态语言脚本如Groovy产生大量的临时类。它会增加CMS remark的暂停时间，所以如果新类加载并不频繁，这个参数还是不开的好。</li></ol><h2 id="GC日志"><a href="#GC日志" class="headerlink" title="GC日志"></a>GC日志</h2><p>GC过程可以通过GC日志来提供优化依据。</p><ol><li>-XX:+PrintGCDetails：启用gc日志打印功能</li><li>-Xloggc:/path/to/gc.log：指定gc日志位置</li><li>-XX:+PrintHeapAtGC：打印GC前后的详细堆栈信息</li><li>-XX:+PrintGCDateStamps：打印可读的日期而不是时间戳</li><li>-XX:+PrintGCApplicationStoppedTime：打印所有引起JVM停顿时间，如果真的发现了一些不知什么的停顿，再临时加上<code>-XX:+PrintSafepointStatistics -XX: PrintSafepointStatisticsCount=1</code>找原因。</li><li>-XX:+PrintGCApplicationConcurrentTime：打印JVM在两次停顿之间正常运行时间，与<code>-XX:+PrintGCApplicationStoppedTime</code>一起使用效果更佳。</li><li>-XX:+PrintTenuringDistribution：查看每次minor GC后新的存活周期的阈值</li><li>-XX:+UseGCLogFileRotation 与 -XX:NumberOfGCLogFiles=10 与 -XX:GCLogFileSize=10M：GC日志在重启之后会清空，但是如果一个应用长时间不重启，那GC日志会增加，所以添加这3个参数，是GC日志滚动写入文件，但是如果重启，可能名字会出现混乱。</li><li>-XX:PrintFLSStatistics=1：打印每次GC前后内存碎片的统计信息</li></ol><h1 id="其他参数设置"><a href="#其他参数设置" class="headerlink" title="其他参数设置"></a>其他参数设置</h1><ol><li>-ea：启用断言，这个没有什么好说的，可以选择启用，或这选择不启用，没有什么大的差异。完全根据自己的系统进行处理。</li><li>-XX:+UseThreadPriorities：启用线程优先级，主要是因为我们可以给予周期性任务更低的优先级，以避免干扰客户端工作。在我当前的环境中，是默认启用的。</li><li>-XX:ThreadPriorityPolicy=42：允许降低线程优先级</li><li>-XX:+HeapDumpOnOutOfMemoryError：发生内存溢出是进行heap-dump</li><li>-XX:HeapDumpPath=/path/to/java_pid<pid>.hprof：这个参数与<code>-XX:+HeapDumpOnOutOfMemoryError</code>共同作用，设置heap-dump时内容输出文件。</pid></li><li>-XX:ErrorFile=/path/to/hs_err_pid<pid>.log：指定致命错误日志位置。一般在JVM发生致命错误时会输出类似hs_err_pid<pid>.log的文件，默认是在工作目录中（如果没有权限，会尝试在/tmp中创建），不过还是自己指定位置更好一些，便于收集和查找，避免丢失。</pid></pid></li><li>-XX:StringTableSize=1000003：指定字符串常量池大小，默认值是60013。对Java稍微有点常识的应该知道，字符串是常量，创建之后就不可修改了，这些常量所在的地方叫做字符串常量池。如果自己系统中有很多字符串的操作，且这些字符串值比较固定，在允许的情况下，可以适当调大一些池子大小。</li><li>-XX:+AlwaysPreTouch：在启动时把所有参数定义的内存全部捋一遍。使用这个参数可能会使启动变慢，但是在后面内存使用过程中会更快。可以保证内存页面连续分配，新生代晋升时不会因为申请内存页面使GC停顿加长。通常只有在内存大于32G的时候才会有感觉。</li><li>-XX:-UseBiasedLocking：禁用偏向锁（在存在大量锁对象的创建且高度并发的环境下(即非多线程高并发应用)禁用偏向锁能够带来一定的性能优化）</li><li>-XX:AutoBoxCacheMax=20000：增加数字对象自动装箱的范围，JDK默认-128～127的int和long，超出范围就会即时创建对象，所以，增加范围可以提高性能，但是也是需要测试。</li><li>-XX:-OmitStackTraceInFastThrow：不忽略重复异常的栈，这是JDK的优化，大量重复的JDK异常不再打印其StackTrace。但是如果系统是长时间不重启的系统，在同一个地方跑了N多次异常，结果就被JDK忽略了，那岂不是查看日志的时候就看不到具体的StackTrace，那还怎么调试，所以还是关了的好。</li><li>-XX:+PerfDisableSharedMem：启用标准内存使用。JVM控制分为标准或共享内存，区别在于一个是在JVM内存中，一个是生成/tmp/hsperfdata_{userid}/{pid}文件，存储统计数据，通过mmap映射到内存中，别的进程可以通过文件访问内容。通过这个参数，可以禁止JVM写在文件中写统计数据，代价就是jps、jstat这些命令用不了了，只能通过jmx获取数据。但是在问题排查是，jps、jstat这些小工具是很好用的，比jmx这种很重的东西好用很多，所以需要自己取舍。<a href="http://www.evanjones.ca/jvm-mmap-pause.html" target="_blank" rel="noopener">这里</a>有个GC停顿的例子。</li><li>-Djava.net.preferIPv4Stack=true：这个参数是属于网络问题的一个参数，可以根据需要设置。在某些开启ipv6的机器中，通过<code>InetAddress.getLocalHost().getHostName()</code>可以获取完整的机器名，但是在ipv4的机器中，可能通过这个方法获取的机器名不完整，可以通过这个参数来获取完整机器名。</li></ol><h1 id="大神给出的例子"><a href="#大神给出的例子" class="headerlink" title="大神给出的例子"></a>大神给出的例子</h1><p>下面贴上大神给出的例子，可以参考使用，不过还是建议在自己的环境中有针对的验证之后再使用，毕竟大神的环境和自己的环境还是不同。</p><h2 id="性能相关"><a href="#性能相关" class="headerlink" title="性能相关"></a>性能相关</h2><p>-XX:-UseBiasedLocking -XX:-UseCounterDecay -XX:AutoBoxCacheMax=20000<br>-XX:+PerfDisableSharedMem(可选) -XX:+AlwaysPreTouch -Djava.security.egd=file:/dev/./urandom</p><h2 id="内存大小相关-JDK7"><a href="#内存大小相关-JDK7" class="headerlink" title="内存大小相关(JDK7)"></a>内存大小相关(JDK7)</h2><p>-Xms4096m -Xmx4096m -Xmn2048m -XX:MaxDirectMemorySize=4096m<br>-XX:PermSize=128m -XX:MaxPermSize=512m -XX:ReservedCodeCacheSize=240M</p><blockquote><p>如果使用jdk8，就把-XX:PermSize=128m -XX:MaxPermSize=512m换成-XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=512m，正如前面所说的，这两套参数是为了保证安全的，建议还是加上。</p></blockquote><h2 id="CMS-GC相关"><a href="#CMS-GC相关" class="headerlink" title="CMS GC相关"></a>CMS GC相关</h2><p>-XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75<br>-XX:+UseCMSInitiatingOccupancyOnly -XX:MaxTenuringThreshold=6<br>-XX:+ExplicitGCInvokesConcurrent -XX:+ParallelRefProcEnabled</p><h2 id="GC日志相关"><a href="#GC日志相关" class="headerlink" title="GC日志相关"></a>GC日志相关</h2><p>-Xloggc:/dev/shm/app-gc.log -XX:+PrintGCApplicationStoppedTime<br>-XX:+PrintGCDateStamps -XX:+PrintGCDetails</p><h2 id="异常日志相关"><a href="#异常日志相关" class="headerlink" title="异常日志相关"></a>异常日志相关</h2><p>-XX:-OmitStackTraceInFastThrow -XX:ErrorFile=${LOGDIR}/hs_err_%p.log<br>-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${LOGDIR}/</p><h2 id="JMX相关"><a href="#JMX相关" class="headerlink" title="JMX相关"></a>JMX相关</h2><p>-Dcom.sun.management.jmxremote.port=${JMX_PORT} -Dcom.sun.management.jmxremote<br>-Djava.rmi.server.hostname=127.0.0.1 -Dcom.sun.management.jmxremote.authenticate=false<br>-Dcom.sun.management.jmxremote.ssl=false</p><p>参考文章：</p><ol><li><a href="http://calvin1978.blogcn.com/articles/javatuning.html" target="_blank" rel="noopener">Java性能优化指南1.8版，及唯品会的实战</a></li><li><a href="http://blog.csdn.net/yangzl2008/article/details/43202969" target="_blank" rel="noopener">Java中的逃逸分析和TLAB以及Java对象分配</a></li><li><a href="http://www.evanjones.ca/jvm-mmap-pause.html" target="_blank" rel="noopener">The Four Month Bug: JVM statistics cause garbage collection pauses</a></li></ol><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a></p><p>个人博文: <a href="http://www.howardliu.cn/java/jvm-tuning-basic">JVM参数优化（基础篇）</a></p><p>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a></p><p>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/73963399" target="_blank" rel="noopener">JVM参数优化（基础篇）</a></p>]]></content>
    
    <summary type="html">
    
      文中内容主要是自己关于JVM参数优化的一些总结，参考了网上很多大神的文章，有参数有说明，这里是为了记录下，也是为了可以给别人做个垫脚石。刚入坑不久，很多地方还不是很透彻，之后会继续研究。
    
    </summary>
    
    
      <category term="java" scheme="http://www.howardliu.cn/categories/java/"/>
    
      <category term="jvm" scheme="http://www.howardliu.cn/categories/java/jvm/"/>
    
    
      <category term="java" scheme="http://www.howardliu.cn/tags/java/"/>
    
      <category term="jvm" scheme="http://www.howardliu.cn/tags/jvm/"/>
    
  </entry>
  
  <entry>
    <title>storm笔记：Trident状态</title>
    <link href="http://www.howardliu.cn/storm/trident-state/"/>
    <id>http://www.howardliu.cn/storm/trident-state/</id>
    <published>2017-06-08T07:35:01.000Z</published>
    <updated>2019-09-11T14:08:23.883Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="http://www.howardliu.cn/storm/trident-tutorial/">storm笔记：Trident应用</a>中说了下Trident的使用，这里说下Trident几种状态的变化及其对应API的使用。</p><a id="more"></a><p>本文内容来自<a href="http://storm.apache.org/releases/current/Trident-state.html" target="_blank" rel="noopener">Trident State</a>，部分内容根据实际情况做出修改。</p><p>Trident中有对状态数据进行读取和写入操作的一流抽象工具。状态既可以保存在拓扑内部，比如保存在内容中并由HDFS存储，也可以通过外部存储（比如Memcached或Cassandra）存储在数据库中。而对于Trident的API而言，这两种机制没有任何区别。</p><p>Trident以容错的方式管理状态，以便在重试或失败时的状态更新是幂等的。在大数据处理中，数据处理的幂等性是非常重要的一个指标，这样能够保证每个消息即使处理了多次，结果也像是只处理了一次一样。</p><p>在进行状态更新时可能需要各种级别的容错能力，在这之前，我们来看一个例子说明实现“恰好一次”语义所需的技巧。比如，正在对流中的数据进行计数聚合操作，每次处理新的元组时，都会将运行的计数结果存储在数据库中。</p><p>如果发生故障时，元组将重新执行计数操作。这就会在执行状态更新时出现问题，因为这个时候不知道是不是已经更新过该元组状态。也许还没有处理该元组数据，这个时候就需要增加计数。也许已经处理该元组，并成功增加计数，但是在下一步的时候出现问题，这种情况下，就不应该增加计数。也有可能是处理元组正常，更新计数是异常，这个时候就需要更新计数。</p><p>所以说，如果只是在数据库中存储计数信息，就不知道元组是否已经处理过。因此，就需要更多的信息作为辅助。Trident提供了下面三个性质，来实现“恰好一次”的处理：</p><ol><li>元组都是以小批次处理</li><li>每批元组都会给出一个唯一ID，称为事务ID（transaction id，txid）。如果批次重复处理，txid也会相同。</li><li>状态的更新操作是按照元组批次的顺序执行的。也就是说，在批次2状态更新成功之前，不会进行批次3的状态更新。</li></ol><p>根据这些特性，就可以通过检查到该元组的批次是否已被处理，并根据检测结果采取适当的操作更新状态了。采取的具体操作取决于Spout的类型。Spout有三种类型：“非事务型（non-transactional）”，“事务型（transactional）”和“不透明事务型（opaque transactional）”。对应的容错能力也是三种：“非事务”，“事务”和“不透明事务”。下面来看看Spout的各个类型及对应的容错能力。</p><h1 id="事务型Spout"><a href="#事务型Spout" class="headerlink" title="事务型Spout"></a>事务型Spout</h1><p>Trident是按照批次发送元组进行处理的，每个批次的元组被赋予唯一的事务ID。Spout的特性根据他们所提供容错性保证机制来决定的，而且这种机制也会对每个批次发生作用。事务型Spout有如下特性：</p><ol><li>每个批次的txid不变，对于一个特定的txid，重复执行时，它所包含的元组数据与第一次完全相同。</li><li>元组只会在一个批次出现，不会重复（某个元组只会出现在一个批次中，不会出现在多个批次中）。</li><li>每个元组都会出现一次（不会遗漏任何的元组数据）</li></ol><p>这是最简单最容易理解的一种Spout类型，数据流被分割成固定的批次。storm中有与Kafka集成的事务型Spout的扩展，代码在<a href="http://github.com/apache/storm/tree/v1.1.0/external/storm-kafka/src/jvm/org/apache/storm/kafka/trident/TransactionalTridentKafkaSpout.java" target="_blank" rel="noopener">这里</a>。</p><p>既然事务型Spout这么简单易懂，为什么不在Trident中完全使用事务型Spout呢？其实就在于它的容错能力。比如，<code>TransactionalTridentKafkaSpout</code>的工作方式是，同一个txid的批次中将包含kafka所有分区的元组。一旦某个批次发出后，出现异常，需要重新发出，就需要完全相同的元组集合才能满足事务型Spout要求的语义。但是这个时候，kafka某个节点异常（节点关闭或分区不可用），就无法获取完全相同的的一批元组，那整个拓扑就会应为第3条语义（批次按顺序执行）停止。</p><p>这就是要有“不透明事务型”Spout的原因了，它能够容忍数据源节点丢失，而且又能保证数据恰好被操作一次。</p><blockquote><p>注：对kafka比较熟悉的应该会想到，如果某一个topic支持复制，那即使一个节点不可用，还会有其他复制节点顶上，那TransactionalTridentKafkaSpout也能够避免上面的问题。</p></blockquote><p>下面继续看看如何设计一个支持恰好一次特性的“事务型”Spout语义（简单的说就是同一个txid对应的批次元组数据完全一致）的状态实现，这种状态称为“事务型状态”。</p><p>比如，现在有一个单词计数的拓扑，需要将单词计数存储在key/value数据库中。key是单词，value中包含单词数量。另外，为了确定同一批次元组是否已经被执行，需要将txid也一同存储在value中。这样，当需要更新单词数量的时候，先比较txid是否相同，如果相同，就跳过更新。如果不同，就更新计数。</p><p>考虑这个为什么它工作的例子。 假设您正在处理由以下批次元组组成的txid 3：</p><p>比如，要处理一个txid是3的一批元组：</p><pre><code>[&quot;man&quot;][&quot;man&quot;][&quot;dog&quot;]</code></pre><p>目前数据库中存储的数据为：</p><pre><code>man =&gt; [count=3, txid=1]dog =&gt; [count=4, txid=3]apple =&gt; [count=10, txid=2]</code></pre><p>在这个时候，发现“man”对应的txid是1，当前的txid是3，就可以更新了。然后“dog”对应的txid是3，说明同一批次的元组数据已经发送过了，就不需要更新。从这点可以看出，txid是3的批次元组是重复发送的，在更新“dog”数量后，在更新“man”数量前，出现了错误。最后的结果就是：</p><pre><code>man =&gt; [count=5, txid=3]dog =&gt; [count=4, txid=3]apple =&gt; [count=10, txid=2]</code></pre><h1 id="不透明事务型Spout"><a href="#不透明事务型Spout" class="headerlink" title="不透明事务型Spout"></a>不透明事务型Spout</h1><p>前面已经提过，不透明事务型Spout不能保证相同txid对应的批次中的元组数据完全一致。其特点如下：</p><ol><li>每个元组都会在有且仅有一个批次中处理成功。</li></ol><p><code>[OpaqueTridentKafkaSpout](http://github.com/apache/storm/tree/v1.1.0/external/storm-kafka/src/jvm/org/apache/storm/kafka/trident/OpaqueTridentKafkaSpout.java)</code>具有这种特性，同时对kafka节点异常有很好的容错性。<code>OpaqueTridentKafkaSpout</code>在发送一个批次元组的时候，会从上次成功之后的位置开始发送，这样就能够保证元组不会漏发或重发。</p><p>基于上面的特点，不透明事务型Spout就不同通过txid来直接判断是否可以跳过状态更新，因为具有相同txid的批次中元组可能发生了变化。</p><p>这就需要存储更多的状态信息了，而不仅仅是一个结果和一个txid了，还需要存储前一个结果值。</p><p>比如，当前批次的计数是2，需要进行一次状态更新，数据库中的数据如下：</p><pre><code class="javascript">{  &quot;value&quot;: 4,  &quot;prevValue&quot;: 1,  &quot;txid&quot;: 2}</code></pre><p>如果当前的txid是3，与数据库中的不同。在这种情况下，需要将<code>prevValue</code>的值该为<code>value</code>的值，<code>value</code>的值增加2，更新<code>txid</code>为3，最后的结果就是：</p><pre><code class="javascript">{  &quot;value&quot;: 6,  &quot;prevValue&quot;: 4,  &quot;txid&quot;: 3}</code></pre><p>如果当前的txid是2，等于数据库中的txid。因为txid相同，说明上一次txid为2的批次处理失败，但是本次的元组可能与上一次不同了。这个时候，就需要使用本次数据覆盖上次处理结果。也就是说，<code>prevValue</code>值不变，<code>value</code>的值改为<code>prevValue</code>加2，<code>txid</code>不变，最后的结果如下：</p><pre><code class="javascript">{  &quot;value&quot;: 3,  &quot;prevValue&quot;: 1,  &quot;txid&quot;: 2}</code></pre><p>这种方式的可行性依赖于Trident的强顺序性。也就是说，一旦开始处理一个新的批次，就不会重复执行上一个批次。不透明事务型Spout保证了不同批次之间没有重复的情况，也就是每个元组只会在一个批次中处理成功，所以就可以放心的使用前一个值与当前值覆盖已存数据了。</p><h1 id="非事务型Spout"><a href="#非事务型Spout" class="headerlink" title="非事务型Spout"></a>非事务型Spout</h1><p>非事务型Spout不能为批次提供任何保证。所以可能出现”至多一次”的处理，即在某个批次处理过程中失败了，但是不会在重新处理；也可能提供“至少一次”的处理，即可能会有多个批次分别处理某个元组。也就是没有办法实现“恰好一次”的语义。</p><h1 id="不同类型spout和状态总结"><a href="#不同类型spout和状态总结" class="headerlink" title="不同类型spout和状态总结"></a>不同类型spout和状态总结</h1><p>下面是不同的spout/状态组合是否支持“恰好一次”处理语义：</p><p><img src="http://www.howardliu.cn/images/storm/spout-vs-state.png" alt="Spouts vs States"></p><p>不透明事务状态有最强的容错性，但是因为存储txid和两个结果带来更大的开销。事务型状态只需要存储一个状态结果，但是只对事务型Spout有效。非事务型状态要求存储的数据更少，但是不能实现“恰好一次”的处理语义。</p><p>所以在选择容错与存储空间中，需要根据具体的需要选择合适的组合。</p><h1 id="状态API"><a href="#状态API" class="headerlink" title="状态API"></a>状态API</h1><p>根据前面来看，“恰好一次”语义的原理有些复杂，但是作为用户，并不需要了解这些txid对比、多值存储，因为Trident已经在State中封装了所有容错处理逻辑，只需要想下面着用携带码就行：</p><pre><code class="java">TridentTopology topology = new TridentTopology();TridentState wordCounts =        topology.newStream(&quot;spout1&quot;, spout)                .each(new Fields(&quot;sentence&quot;), new Split(), new Fields(&quot;word&quot;))                .groupBy(new Fields(&quot;word&quot;))                .persistentAggregate(MemcachedState.opaque(serverLocations), new Count(), new Fields(&quot;count&quot;))                .parallelismHint(6);</code></pre><p>所有的不透明事务状态逻辑已经封装在<code>MemcachedState.opaque</code>中，另外，状态更新会自动调整为批次操作，这样可以减少与数据库之间反复交互带来的资源浪费。</p><p>基本的<code>State</code>接口只有两个方法：</p><pre><code class="java">public interface State {    void beginCommit(Long txid); // 对于像DRPC流发生的partitionPersist这样的事情，可以是null    void commit(Long txid);}</code></pre><p>前面已经说过，状态更新开始和结束时都会获取txid。Trident并不关心状态如何操作，使用哪种方式更新，使用哪种方式读取。</p><p>假如有一个包含用户地址信息的定制数据库，需要使用Trident与数据库交互，<code>State</code>扩展类中包含对于用户信息的getter和setter方法：</p><pre><code class="java">public class LocationDB implements State {    public void beginCommit(Long txid) {    }    public void commit(Long txid) {    }    public void setLocation(long userId, String location) {        // 向数据库设置地址信息    }    public String getLocation(long userId) {        // 从数据库中获取地址信息    }}</code></pre><p>然后就需要一个<code>StateFactory</code>来创建Trident所需的<code>State</code>对象，<code>LocationDB</code>所需的<code>StateFactory</code>大体结构如下：</p><pre><code class="java">public class LocationDBFactory implements StateFactory {    public State makeState(Map conf, int partitionIndex, int numPartitions) {        return new LocationDB();    }}</code></pre><p>Trident提供了用于查询状态源的<code>QueryFunction</code>接口，以及更新状态源的<code>StateUpdater</code>接口。比如，查询<code>LocationDB</code>中用户信息的<code>QueryLocation</code>：</p><pre><code class="java">TridentTopology topology = new TridentTopology();TridentState locations = topology.newStaticState(new LocationDBFactory());topology.newStream(&quot;myspout&quot;, spout)        .stateQuery(locations, new Fields(&quot;userid&quot;), new QueryLocation(), new Fields(&quot;location&quot;));</code></pre><p><code>QueryLocation</code>的代码如下：</p><pre><code class="java">public class QueryLocation extends BaseQueryFunction&lt;LocationDB, String&gt; {    public List&lt;String&gt; batchRetrieve(LocationDB state, List&lt;TridentTuple&gt; inputs) {        List&lt;String&gt; ret = new ArrayList();        for (TridentTuple input : inputs) {            ret.add(state.getLocation(input.getLong(0)));        }        return ret;    }    public void execute(TridentTuple tuple, String location, TridentCollector collector) {        collector.emit(new Values(location));    }}</code></pre><p><code>QueryFunction</code>操作分为两步：首先，Trident会将收集到的数据放在一个批次中，发送给<code>batchRetrieve</code>方法。在这个例子中，<code>batchRetrieve</code>方法收到的是一些用户id。<code>batchRetrieve</code>会返回一组与输入元组长度相同的结果。输入元组与输出结果中各个元素是彼此对应的。</p><p>从这点来看，上面的<code>LocationDB</code>类并没有发挥Trident批处理优势，所以需要尽心改造：</p><pre><code class="java">public class LocationDB implements State {    public void beginCommit(Long txid) {    }    public void commit(Long txid) {    }    public void setLocationsBulk(List&lt;Long&gt; userIds, List&lt;String&gt; locations) {        // set locations in bulk    }    public List&lt;String&gt; bulkGetLocations(List&lt;Long&gt; userIds) {        // get locations in bulk    }}</code></pre><p>对应的<code>QueryLocation</code>类如下：</p><pre><code class="java">public class QueryLocation extends BaseQueryFunction&lt;LocationDB, String&gt; {    public List&lt;String&gt; batchRetrieve(LocationDB state, List&lt;TridentTuple&gt; inputs) {        List&lt;Long&gt; userIds = new ArrayList&lt;Long&gt;();        for (TridentTuple input : inputs) {            userIds.add(input.getLong(0));        }        return state.bulkGetLocations(userIds);    }    public void execute(TridentTuple tuple, String location, TridentCollector collector) {        collector.emit(new Values(location));    }}</code></pre><p>这段代码大幅减少了数据库操作。</p><p>对于更新状态，可以使用<code>StateUpdater</code>接口。比如下面的更新操作：</p><pre><code class="java">public class LocationUpdater extends BaseStateUpdater&lt;LocationDB&gt; {    public void updateState(LocationDB state, List&lt;TridentTuple&gt; tuples, TridentCollector collector) {        List&lt;Long&gt; ids = new ArrayList&lt;Long&gt;();        List&lt;String&gt; locations = new ArrayList&lt;String&gt;();        for (TridentTuple t : tuples) {            ids.add(t.getLong(0));            locations.add(t.getString(1));        }        state.setLocationsBulk(ids, locations);    }}</code></pre><p>对应的更新操作拓扑中就可以是这样：</p><pre><code class="java">TridentTopology topology = new TridentTopology();TridentState locations =        topology.newStream(&quot;locations&quot;, locationsSpout)                .partitionPersist(new LocationDBFactory(), new Fields(&quot;userid&quot;, &quot;location&quot;), new LocationUpdater());</code></pre><p><code>partitionPersist</code>方法会更新状态，<code>StateUpdater</code>接口接收一批元组和状态信息，然后更新状态。上面的<code>LocationUpdater</code>类中仅仅是从元组中抓取用户id和地址信息，然后对状态执行批量处理。然后，<code>partitionPersist</code>会返回一个表示更新状态后的<code>TridentState</code>对象。随后就可以在拓扑的其他地方使用<code>stateQuery</code>方法查询状态。</p><p>在<code>StateUpdater</code>的<code>updateState</code>方法中有一个<code>TridentCollector</code>参数，这个对象是可以将发送进来的元组发送到一个新的数据流中。在这个例子中没有用到。如果需要进行比如向数据库更新计数值的后续操作，可以通过<code>TridentState#newValuesStream</code>方法获取新的数据流数据。</p><h1 id="persistentAggregate"><a href="#persistentAggregate" class="headerlink" title="persistentAggregate"></a>persistentAggregate</h1><p>Trident使用一个名为<code>persistentAggregate</code>的方法更新状态。前面已经出现过，这里再写一遍：</p><pre><code class="java">TridentTopology topology = new TridentTopology();TridentState wordCounts =        topology.newStream(&quot;spout1&quot;, spout)                .each(new Fields(&quot;sentence&quot;), new Split(), new Fields(&quot;word&quot;))                .groupBy(new Fields(&quot;word&quot;))                .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields(&quot;count&quot;));</code></pre><p><code>partitionPersist</code>是一个接收Trident聚合器作为参数并对状态数据进行更新的方法，<code>persistentAggregate</code>就是构建于<code>partitionPersist</code>上层的一个编程抽象。在这个例子中，通过<code>groupBy</code>返回一个分组数据，Trident需要一个实现<code>MapState</code>接口的对象。分组字段是状态的key，聚合结果是状态的value。<code>MapState</code>接口如下：</p><pre><code class="java">public interface MapState&lt;T&gt; extends State {    List&lt;T&gt; multiGet(List&lt;List&lt;Object&gt;&gt; keys);    List&lt;T&gt; multiUpdate(List&lt;List&lt;Object&gt;&gt; keys, List&lt;ValueUpdater&gt; updaters);    void multiPut(List&lt;List&lt;Object&gt;&gt; keys, List&lt;T&gt; vals);}</code></pre><p>如果你需要在未分组的数据流上执行聚合操作时，Trident需要一个实现<code>Snapshottable</code>接口的对象：</p><pre><code class="java">public interface Snapshottable&lt;T&gt; extends State {    T get();    T update(ValueUpdater updater);    void set(T o);}</code></pre><p><a href="http://github.com/apache/storm/blob/v1.1.0/storm-core/src/jvm/org/apache/storm/trident/testing/MemoryMapState.java" target="_blank" rel="noopener">MemoryMapState</a> 和 <a href="https://github.com/nathanmarz/trident-memcached/blob/master/src/jvm/trident/memcached/MemcachedState.java" target="_blank" rel="noopener">MemcachedState</a> 都实现了这些接口.</p><h1 id="实现MapState接口"><a href="#实现MapState接口" class="headerlink" title="实现MapState接口"></a>实现MapState接口</h1><p>实现<code>MapState</code>接口非常简单，Trident几乎把所有事都做完了。<code>OpaqueMap</code>、<code>TransactionalMap</code>和<code>NonTransactionalMap</code>都分别实现了各自的容错语义。只需要为这些类提供一个用于对不同key/value进行批量获取、批量修改的<code>IBackingMap</code>实现就行。<code>IBackingMap</code>接口如下：</p><pre><code class="java">public interface IBackingMap&lt;T&gt; {    List&lt;T&gt; multiGet(List&lt;List&lt;Object&gt;&gt; keys);    void multiPut(List&lt;List&lt;Object&gt;&gt; keys, List&lt;T&gt; vals);}</code></pre><p><code>OpaqueMap</code>会使用<a href="http://github.com/apache/storm/blob/v1.1.0/storm-core/src/jvm/org/apache/storm/trident/state/OpaqueValue.java" target="_blank" rel="noopener">OpaqueValue</a>作为vals参数调用<code>multiPut</code>方法；<code>TransactionalMap</code>会使用<a href="http://github.com/apache/storm/blob/v1.1.0/storm-core/src/jvm/org/apache/storm/trident/state/TransactionalValue.java" target="_blank" rel="noopener">TransactionalValue</a>作为参数；<code>NonTransactionalMaps</code>会直接把拓扑对象传入。</p><p>Trident还提供了<a href="http://github.com/apache/storm/blob/v1.1.0/storm-core/src/jvm/org/apache/storm/trident/state/map/CachedMap.java" target="_blank" rel="noopener">CachedMap</a>类来实现key/value的自动LRU缓存操作。</p><p>最后，Trident还提供了<a href="http://github.com/apache/storm/blob/v1.1.0/storm-core/src/jvm/org/apache/storm/trident/state/map/SnapshottableMap.java" target="_blank" rel="noopener">SnapshottableMap</a>类，该类通过将全局聚合的结果存入一个固定key中的方法将<code>MapState</code>对象转化为<code>Snapshottable</code>对象。</p><p>可以参考<a href="https://github.com/nathanmarz/trident-memcached/blob/master/src/jvm/trident/memcached/MemcachedState.java" target="_blank" rel="noopener">MemcachedState</a>的实现来了解如何将这些工具结合在一起来提供一个高性能的<code>MapState</code>实现。<code>MemcachedState</code>支持不透明事务、事务和非事务语义。</p><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a></p><p>个人博文: <a href="http://www.howardliu.cn/storm/trident-state/">storm笔记：Trident状态</a></p><p>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a></p><p>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/72972674" target="_blank" rel="noopener">storm笔记：Trident状态</a></p>]]></content>
    
    <summary type="html">
    
      Trident中有对状态数据进行读取和写入操作的一流抽象工具。状态既可以保存在拓扑内部，比如保存在内容中并由HDFS存储，也可以通过外部存储（比如Memcached或Cassandra）存储在数据库中。而对于Trident的API而言，这两种机制没有任何区别。Trident以容错的方式管理状态，以便在重试或失败时的状态更新是幂等的。在大数据处理中，数据处理的幂等性是非常重要的一个指标，这样能够保证每个消息即使处理了多次，结果也像是只处理了一次一样。
    
    </summary>
    
    
      <category term="storm" scheme="http://www.howardliu.cn/categories/storm/"/>
    
    
      <category term="大数据" scheme="http://www.howardliu.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="storm" scheme="http://www.howardliu.cn/tags/storm/"/>
    
      <category term="实时计算" scheme="http://www.howardliu.cn/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Trident" scheme="http://www.howardliu.cn/tags/Trident/"/>
    
  </entry>
  
  <entry>
    <title>JDK 工具一览</title>
    <link href="http://www.howardliu.cn/java/jdk-tools-and-utilities/"/>
    <id>http://www.howardliu.cn/java/jdk-tools-and-utilities/</id>
    <published>2017-04-26T05:38:45.000Z</published>
    <updated>2019-09-11T14:08:23.874Z</updated>
    
    <content type="html"><![CDATA[<p>Java 坑如此大，需要慢慢填。</p><p>本文是列出JDK自带的一些工具，介于篇幅，简单列出工具列表及工具的作用。至少先做到知道有哪些工具，然后才能在实际中用到。</p><blockquote><p>本文参考了<a href="http://docs.oracle.com/javase/8/docs/technotes/tools/" target="_blank" rel="noopener">官方介绍</a>和本机<code>man</code>命令的介绍。</p></blockquote><a id="more"></a><h1 id="1-标准工具"><a href="#1-标准工具" class="headerlink" title="1. 标准工具"></a>1. 标准工具</h1><p>这些工具都是JDK提供的，通常都是长期支持的工具，JDK承诺这些工具比较好用。不同系统、不同版本之间可能会有差异，但是不会突然就有一个工具消失。</p><h2 id="1-1-基础-appletviewer-extcheck-jar-java-javac-javadoc-javah-javap-jdb-jdeps"><a href="#1-1-基础-appletviewer-extcheck-jar-java-javac-javadoc-javah-javap-jdb-jdeps" class="headerlink" title="1.1 基础 (appletviewer, extcheck, jar, java, javac, javadoc, javah, javap, jdb, jdeps)"></a>1.1 基础 (appletviewer, extcheck, jar, java, javac, javadoc, javah, javap, jdb, jdeps)</h2><table><thead><tr><th>工具</th><th align="left">简述</th></tr></thead><tbody><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/appletviewer.html" target="_blank" rel="noopener">appletviewer</a></td><td align="left">在没有web浏览器的情况下运行和调试applet</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/extcheck.html" target="_blank" rel="noopener">extcheck</a></td><td align="left">检查Jar冲突的工具</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jar.html" target="_blank" rel="noopener">jar</a></td><td align="left">创建和管理Jar文件</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html" target="_blank" rel="noopener">java</a></td><td align="left">Java运行工具，用于运行.class字节码文件或.jar文件</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/javac.html" target="_blank" rel="noopener">javac</a></td><td align="left">用于Java编程语言的编译器</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/javadoc.html" target="_blank" rel="noopener">javadoc</a></td><td align="left">API文档生成器</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/javah.html" target="_blank" rel="noopener">javah</a></td><td align="left">C头文件和stub函数生成器，用于编写native方法</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/javap.html" target="_blank" rel="noopener">javap</a></td><td align="left">类文件反汇编器，主要用于根据Java字节码文件反汇编为Java源代码文件</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jdb.html" target="_blank" rel="noopener">jdb</a></td><td align="left">Java调试器(Java Debugger)</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jdeps.html" target="_blank" rel="noopener">jdeps</a></td><td align="left">Java类依赖性分析器</td></tr></tbody></table><h2 id="1-2-安全-keytool-jarsigner-policytool"><a href="#1-2-安全-keytool-jarsigner-policytool" class="headerlink" title="1.2 安全 (keytool, jarsigner, policytool)"></a>1.2 安全 (keytool, jarsigner, policytool)</h2><table><thead><tr><th>工具</th><th align="left">简述</th></tr></thead><tbody><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/keytool.html" target="_blank" rel="noopener">keytool</a></td><td align="left">管理密钥库和证书。主要用于获取或缓存Kerberos协议的票据授权票据。允许用户查看本地凭据缓存和密钥表中的条目(用于Kerberos协议)。Kerberos密钥表管理工具，允许用户管理存储于本地密钥表中的主要名称和服务密钥。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jarsigner.html" target="_blank" rel="noopener">jarsigner</a></td><td align="left">生成并验证JAR签名</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/policytool.html" target="_blank" rel="noopener">policytool</a></td><td align="left">管理策略文件的GUI工具，用于管理用户策略文件(.java.policy)</td></tr></tbody></table><h2 id="1-3-国际化-i18n-native2ascii"><a href="#1-3-国际化-i18n-native2ascii" class="headerlink" title="1.3 国际化/i18n (native2ascii)"></a>1.3 国际化/i18n (native2ascii)</h2><table><thead><tr><th>工具</th><th align="left">简述</th></tr></thead><tbody><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/native2ascii.html" target="_blank" rel="noopener">native2ascii</a></td><td align="left">本地编码到ASCII编码的转换器(Native-to-ASCII Converter)，用于“任意受支持的字符编码”和与之对应的“ASCII编码和(或)Unicode转义”之间的相互转换。</td></tr></tbody></table><h2 id="1-4-远程方法调用-RMI-rmic-rmiregistry-rmid-serialver"><a href="#1-4-远程方法调用-RMI-rmic-rmiregistry-rmid-serialver" class="headerlink" title="1.4 远程方法调用/RMI (rmic, rmiregistry, rmid, serialver)"></a>1.4 远程方法调用/RMI (rmic, rmiregistry, rmid, serialver)</h2><table><thead><tr><th>工具</th><th align="left">简述</th></tr></thead><tbody><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/rmic.html" target="_blank" rel="noopener">rmic</a></td><td align="left">Java RMI 编译器，为使用JRMP或IIOP协议的远程对象生成stub、skeleton、和tie类，也用于生成OMG IDL。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/rmiregistry.html" target="_blank" rel="noopener">rmiregistry</a></td><td align="left">远程对象注册表服务，用于在当前主机的指定端口上创建并启动一个远程对象注册表。</td></tr><tr><td></td><td align="left"></td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/rmid.html" target="_blank" rel="noopener">rmid</a></td><td align="left">启动激活系统守护进程，允许在虚拟机中注册或激活对象。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/serialver.html" target="_blank" rel="noopener">serialver</a></td><td align="left">生成并返回指定类的序列化版本ID</td></tr></tbody></table><h2 id="1-5-Java-IDL-与-RMI-IIOP-tnameserv-idlj-orbd-servertool"><a href="#1-5-Java-IDL-与-RMI-IIOP-tnameserv-idlj-orbd-servertool" class="headerlink" title="1.5 Java IDL 与 RMI-IIOP (tnameserv, idlj, orbd, servertool)"></a>1.5 Java IDL 与 RMI-IIOP (tnameserv, idlj, orbd, servertool)</h2><table><thead><tr><th>工具</th><th align="left">简述</th></tr></thead><tbody><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/tnameserv.html" target="_blank" rel="noopener">tnameserv</a></td><td align="left">提供对命名服务的访问</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/idlj.html" target="_blank" rel="noopener">idlj</a></td><td align="left">IDL转Java编译器(IDL-to-Java Compiler)，生成映射OMG IDL接口的.java文件，并启用以Java编程语言编写的使用CORBA功能的应用程序的.java文件。IDL意即接口定义语言(Interface Definition Language)。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/orbd.html" target="_blank" rel="noopener">orbd</a></td><td align="left">对象请求代理守护进程(Object Request Broker Daemon)，提供从客户端查找和调用CORBA环境服务端上的持久化对象的功能。使用ORBD代替瞬态命名服务tnameserv。ORBD包括瞬态命名服务和持久命名服务。ORBD工具集成了服务器管理器，互操作命名服务和引导名称服务器的功能。当客户端想进行服务器时定位，注册和激活功能时，可以与servertool一起使用。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/servertool.html" target="_blank" rel="noopener">servertool</a></td><td align="left">为应用程序注册，注销，启动和关闭服务器提供易用的接口</td></tr></tbody></table><h2 id="1-6-Java-发布工具-javapackager-pack200-unpack200"><a href="#1-6-Java-发布工具-javapackager-pack200-unpack200" class="headerlink" title="1.6 Java 发布工具 (javapackager, pack200, unpack200)"></a>1.6 Java 发布工具 (javapackager, pack200, unpack200)</h2><table><thead><tr><th>工具</th><th align="left">简述</th></tr></thead><tbody><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/javapackager.html" target="_blank" rel="noopener">javapackager</a></td><td align="left">打包、签名Java和JavaFX应用程序</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/pack200.html" target="_blank" rel="noopener">pack200</a></td><td align="left">使用Java gzip压缩器将JAR文件转换为压缩的pack200文件。压缩的压缩文件是高度压缩的JAR，可以直接部署，节省带宽并减少下载时间。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/unpack200.html" target="_blank" rel="noopener">unpack200</a></td><td align="left">将pack200生成的打包文件解压提取为JAR文件</td></tr></tbody></table><h2 id="1-7-Java-Web-启动工具-javaws"><a href="#1-7-Java-Web-启动工具-javaws" class="headerlink" title="1.7 Java Web 启动工具 (javaws)"></a>1.7 Java Web 启动工具 (javaws)</h2><table><thead><tr><th>工具</th><th align="left">简述</th></tr></thead><tbody><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/javaws.html" target="_blank" rel="noopener">javaws</a></td><td align="left">启动Java Web Start并设置各种选项的工具</td></tr></tbody></table><h2 id="1-8-故障排查，分析，监控和管理-jcmd-jconsole-jmc-jvisualvm"><a href="#1-8-故障排查，分析，监控和管理-jcmd-jconsole-jmc-jvisualvm" class="headerlink" title="1.8 故障排查，分析，监控和管理 (jcmd, jconsole, jmc, jvisualvm)"></a>1.8 故障排查，分析，监控和管理 (jcmd, jconsole, jmc, jvisualvm)</h2><table><thead><tr><th>工具</th><th align="left">简述</th></tr></thead><tbody><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jcmd.html" target="_blank" rel="noopener">jcmd</a></td><td align="left">JVM诊断命令工具，将诊断命令请求发送到正在运行的Java虚拟机。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jconsole.html" target="_blank" rel="noopener">jconsole</a></td><td align="left">用于监控Java虚拟机的使用JMX规范的图形工具。它可以监控本地和远程JVM。它还可以监控和管理应用程序。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jmc.html" target="_blank" rel="noopener">jmc</a></td><td align="left">Java任务控制客户端（JMC，Java Mission Control），包含用于监控和管理Java应用程序的工具，而不会引入与这些工具相关联的性能开销。开发者可以使用jmc命令来<a href="https://docs.oracle.com/javacomponents/jmc-5-5/jmc-user-guide/toc.htm" target="_blank" rel="noopener">创建JMC工具</a>。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jconsole.html" target="_blank" rel="noopener">jvisualvm</a></td><td align="left">一种图形化工具，可在Java虚拟机中运行时提供有关基于Java技术的应用程序（Java应用程序）的详细信息。 Java VisualVM提供内存和CPU分析，堆转储分析，内存泄漏检测，MBean访问和垃圾收集。</td></tr></tbody></table><h2 id="1-9-WebService工具-schemagen-wsgen-wsimport-xjc"><a href="#1-9-WebService工具-schemagen-wsgen-wsimport-xjc" class="headerlink" title="1.9 WebService工具 (schemagen, wsgen, wsimport, xjc)"></a>1.9 WebService工具 (schemagen, wsgen, wsimport, xjc)</h2><table><thead><tr><th>工具</th><th align="left">简述</th></tr></thead><tbody><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/schemagen.html" target="_blank" rel="noopener">schemagen</a></td><td align="left">用于XML绑定的Schema生成器，用于生成XML schema文件。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/wsgen.html" target="_blank" rel="noopener">wsgen</a></td><td align="left">XML Web Service 2.0的Java API，生成用于JAX-WS Web Service的JAX-WS便携式产物。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/wsimport.html" target="_blank" rel="noopener">wsimport</a></td><td align="left">XML Web Service 2.0的Java API，主要用于根据服务端发布的wsdl文件生成客户端</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/xjc.html" target="_blank" rel="noopener">xjc</a></td><td align="left">主要用于根据XML schema文件生成对应的Java类。</td></tr></tbody></table><h1 id="2-实验性工具"><a href="#2-实验性工具" class="headerlink" title="2. 实验性工具"></a>2. 实验性工具</h1><p>所谓的实验性工具，就是HotSpot JDK提供了，但是可能在之后的某个版本中突然就不可用了。通过<code>man</code>命令查看这些工具的详细介绍的时候，都会在第一行有<code>This command is experimental and unsupported.</code>这么一句。<strong>HotSpot JDK再三强调要谨慎使用，但是由于这些工具在Java性能调优方面作用太大，所以我们就谨慎的研究然后谨慎的使用吧。</strong></p><h2 id="2-1-监控-jps-jstat-jstatd"><a href="#2-1-监控-jps-jstat-jstatd" class="headerlink" title="2.1 监控 (jps, jstat, jstatd)"></a>2.1 监控 (jps, jstat, jstatd)</h2><table><thead><tr><th>工具</th><th align="left">简述</th></tr></thead><tbody><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jps.html" target="_blank" rel="noopener">jps</a></td><td align="left">JVM进程状态工具(JVM Process Status Tool)，在目标系统上列出HotSpot Java虚拟机进程的描述信息</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstat.html" target="_blank" rel="noopener">jstat</a></td><td align="left">JVM统计监控工具(JVM Statistics Monitoring Tool)，根据参数指定的方式收集和记录指定的jvm进程的性能统计信息。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstatd.html" target="_blank" rel="noopener">jstatd</a></td><td align="left">JVM jstat守护程序，启动一个RMI服务器应用程序，用于监视测试的HotSpot Java虚拟机的创建和终止，并提供一个界面，允许远程监控工具附加到在本地系统上运行的Java虚拟机。</td></tr></tbody></table><h2 id="2-2-故障排查-jinfo-jhat-jmap-jsadebugd-jstack"><a href="#2-2-故障排查-jinfo-jhat-jmap-jsadebugd-jstack" class="headerlink" title="2.2 故障排查 (jinfo, jhat, jmap, jsadebugd, jstack)"></a>2.2 故障排查 (jinfo, jhat, jmap, jsadebugd, jstack)</h2><table><thead><tr><th>工具</th><th align="left">简述</th></tr></thead><tbody><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jinfo.html" target="_blank" rel="noopener">jinfo</a></td><td align="left">Java的配置信息工具(Java Configuration Information)，用于打印指定Java进程、核心文件或远程调试服务器的配置信息。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jhat.html" target="_blank" rel="noopener">jhat</a></td><td align="left">Java堆分析工具(Java Heap Analysis Tool)，用于分析Java堆内存中的对象信息。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jmap.html" target="_blank" rel="noopener">jmap</a></td><td align="left">Java内存映射工具(Java Memory Map)，主要用于打印指定Java进程、核心文件或远程调试服务器的共享对象内存映射或堆内存细节。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jsadebugd.html" target="_blank" rel="noopener">jsadebugd</a></td><td align="left">适用于Java的可维护性代理调试守护程序(Java Serviceability Agent Debug Daemon)，主要用于附加到指定的Java进程、核心文件，或充当一个调试服务器。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstack.html" target="_blank" rel="noopener">jstack</a></td><td align="left">Java的堆栈跟踪工具，主要用于打印指定Java进程、核心文件或远程调试服务器的Java线程的堆栈跟踪信息。</td></tr></tbody></table><h2 id="2-3-脚本工具-jjs-jrunscript"><a href="#2-3-脚本工具-jjs-jrunscript" class="headerlink" title="2.3 脚本工具 (jjs, jrunscript)"></a>2.3 脚本工具 (jjs, jrunscript)</h2><table><thead><tr><th>工具</th><th align="left">简述</th></tr></thead><tbody><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jjs.html" target="_blank" rel="noopener">jjs</a></td><td align="left">对Nashorn引擎的调用。Nashorn是基于Java实现一个轻量级高性能的JavaScript运行环境。</td></tr><tr><td><a href="http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jrunscript.html" target="_blank" rel="noopener">jrunscript</a></td><td align="left">Java命令行脚本外壳工具(command line script shell)，主要用于解释执行javascript、groovy、ruby等脚本语言。</td></tr></tbody></table><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a></p><p>个人博文: <a href="http://www.howardliu.cn/java/jdk-tools-and-utilities">JDK 工具一览</a></p><p>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a></p><p>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/70805900" target="_blank" rel="noopener">JDK 工具一览</a></p>]]></content>
    
    <summary type="html">
    
      jdk工具：appletviewer, extcheck, jar, java, javac, javadoc, javah, javap, jdb, jdeps, keytool, jarsigner, policytool, kinit, klist, ktab, native2ascii, rmic, rmiregistry, rmid, serialver, tnameserv, idlj, orbd, servertool, javapackager, pack200, unpack200, javaws, jcmd, jconsole, jmc, jvisualvm, schemagen, wsgen, wsimport, xjc, jps, jstat, jstatd, jinfo, jhat, jmap, jsadebugd, jstack, jjs, jrunscript
    
    </summary>
    
    
      <category term="java" scheme="http://www.howardliu.cn/categories/java/"/>
    
      <category term="jdk" scheme="http://www.howardliu.cn/categories/java/jdk/"/>
    
    
      <category term="java" scheme="http://www.howardliu.cn/tags/java/"/>
    
      <category term="jdk" scheme="http://www.howardliu.cn/tags/jdk/"/>
    
      <category term="工具" scheme="http://www.howardliu.cn/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>storm笔记：Trident应用</title>
    <link href="http://www.howardliu.cn/storm/trident-tutorial/"/>
    <id>http://www.howardliu.cn/storm/trident-tutorial/</id>
    <published>2017-03-31T09:10:33.000Z</published>
    <updated>2019-09-11T14:08:23.883Z</updated>
    
    <content type="html"><![CDATA[<p>本文内容部分来自<a href="http://storm.apache.org/releases/current/Trident-tutorial.html" target="_blank" rel="noopener">Trident Tutorial</a>。</p><p>Trident是基于Storm的实时计算模型的高级抽象。它可以实现高吞吐（每秒数百万条消息）的有状态流处理和低延迟分布式查询。如果以前使用过高级批处理工具（比如Pig或Cascading），则对Trident的概念会非常熟悉，比如连接、聚合、分组、功能处理和过滤等。除此之外，Trident还增加了用于在数据库或持久化存储上进行有状态的增量处理的原语。Trident具有一致性、一次性语义，所以很容易就能够推导出Trident拓扑结构。</p><p>Trident的出现算是程序猿非常懒的又一个铁证。Strom是一个实时流处理工具，有很高的吞吐。在实际应用场景中，很多场景是借助这种实时处理能力，对实时数据进行统计，然后将统计结果实时推送到大屏或者其他可以实时浏览的地方，这样领导或者活动运营就可以实时查看销售或活动情况，比如，双十一时候的大屏，就可以使用Storm来做（我们现在就是这样做的，把全渠道的销售情况进行实时统计，然后显示在大屏上，据说领导会看）。然后，程序猿们就发现，很多统计功能非常类似，所以进行抽象，使用更加高级的功能代替一个一个的Spout、Bolt（当然，Trident拓扑结构运行的时候也是解析成Spout和Bolt运行）。</p><p>然后又有人发现，Trident这种方式也是比较麻烦，即使程序猿们通过高级抽先的Trident省去了很多麻烦，但是还是架不住运维、运营、产品等不断改变的需求，所以就有很多SQL方式解析为Trident或普通Topology的工具产生。既然运维、运营、产品等不断修改需求，那就简单的通过SQL查询（不同的SQL解析为不同的拓扑结构，在Storm中运行，可以得出不同的结果）。比如：<a href="https://github.com/epfldata/squall" target="_blank" rel="noopener">squall</a>。</p><p>这些都是题外话，下面继续说Trident。</p><a id="more"></a><h1 id="1-一个例子"><a href="#1-一个例子" class="headerlink" title="1 一个例子"></a>1 一个例子</h1><p>接下来看一个Trident的例子：</p><ol><li>统计输入句子中单词数量</li><li>实现单词统计结果的查询</li></ol><p>首先实现一个不断发送句子的Spout：</p><pre><code class="java">FixedBatchSpout spout = new FixedBatchSpout(new Fields(&quot;sentence&quot;), 3,               new Values(&quot;the cow jumped over the moon&quot;),               new Values(&quot;the man went to the store and bought some candy&quot;),               new Values(&quot;four score and seven years ago&quot;),               new Values(&quot;how many apples can you eat&quot;));spout.setCycle(true);</code></pre><p>上面的Spout循环发送句子流，下面是计算流中单词数量的代码，也就是Trident的本体：</p><pre><code class="java">TridentTopology topology = new TridentTopology(); // 1TridentState wordCounts =     topology.newStream(&quot;spout1&quot;, spout) // 2       .each(new Fields(&quot;sentence&quot;), new Split(), new Fields(&quot;word&quot;)) // 3       .groupBy(new Fields(&quot;word&quot;)) // 4       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields(&quot;count&quot;)) // 5       .parallelismHint(6);</code></pre><p>一步步说下代码：</p><p>行1，创建<code>TridentTopology</code>对象topology，这个对象就是Trident计算的入口。</p><p>行2，<code>TridentTopology</code>的<code>newStream</code>方法是定义Trident的输入源，这里使用的是一开始定义的<code>FixedBatchSpout</code>对象。当然，输入源也可以是Kafka之类的队列Broker（这个实在不知道应该翻译成什么，不过好在是说Broker，大家就都明白是干啥的）。这个地方，Trident会在Zookeeper中跟踪每个输入源的状态元数据。也就是Trident中比较重要的状态的概念，这个后面再说。Trident会将输入的流分成更小的批量数据（这里比较绕口，可以理解为Trident的入口进来一个大的批量数据，然后Trident把这个大的批量数据进行分割，变成一堆小的批量数据，比如进来的是1000条，分割成10个100条）进行处理，比如，把传入的流分成下面的样子：</p><p><img src="http://www.howardliu.cn/images/storm/batched-stream.png" alt="batched stream"></p><p>通常，小批量数据的数量会是数千或数百万个，这取决于吞吐量。</p><p>Trident提供了一整套完整的批量处理API来处理这些小批量数据。类似于Hadoop的高级抽象中处理Pig或Cascading的内容：分组、连接、聚合、功能操作、过滤等。当然，分别处理每个小的批量数据并不容易（使用Hadoop处理大的矩阵乘法的就会深有体会），所以Trident提供了跨批次进行聚合处理的功能，并可以将这些聚合结果持久化在内存中、Memcached、Cassandra或其他存储中。Trident还具有一流的实时状态查询功能，这个状态可以有Trident更新，或者其他独立的状态来源。</p><p>行3，Spout发出包含名为sentence的field的流。通过使用<code>Split</code>函数，对每个tuple进行处理，把名为sentence的字段分割为一个个单词。将分割的单词命名为word，继续向下分发。下面是<code>Split</code>的定义：</p><pre><code class="java">public class Split extends BaseFunction {   public void execute(TridentTuple tuple, TridentCollector collector) {       String sentence = tuple.getString(0);       for(String word: sentence.split(&quot; &quot;)) {           collector.emit(new Values(word));                       }   }}</code></pre><p>行4，Trident进行了单词计数和结果的持续存储。先对word字段进行分组，然后用<code>Count</code>聚合器持续聚合。</p><p>Trident的一个很强的特性是能够完全容错和一次性处理语义。Trident能够保持状态，如果发生故障需要重试，则不会对同一源数据多次更新数据。</p><p>行5，<code>persistentAggregate</code>函数实现了存储和更新聚合结果的功能，不需要操心。例子中，计数结果保存在内存中，当然也可以使用Memcached、Cassandra或其他持久化存储。这里先不做讨论。<code>persistentAggregate</code>方法将<code>Stream</code>转换为<code>TridentState</code>对象。在这里，<code>TridentState</code>对象表示所有单词计数，然后使用<code>TridentState</code>对象来实现分布式查询。</p><p>接下来实现对单词计数实现低延迟分布式查询，以空格分割的单词列表作为输入，返回这些单词的计数总和。这个查询会在后台做并行化处理，其他的与普通的RPC调用一样。比如像下面这样调用：</p><pre><code class="java">DRPCClient client = new DRPCClient(&quot;drpc.server.location&quot;, 3772);System.out.println(client.execute(&quot;words&quot;, &quot;cat dog the man&quot;);// prints the JSON-encoded result, e.g.: &quot;[[5078]]&quot;</code></pre><p>如上面的代码，除了它是在Storm集群中并行执行外，与普通的RPC调用没什么区别。通常简单的RPC查询，延迟在10ms左右，复杂的DRPC查询可能需要更长的时间，具体的时间取决于计算被分配的资源多少。</p><p>拓扑中分布式查询部分的实现如下所示：</p><pre><code class="java">topology.newDRPCStream(&quot;words&quot;)       .each(new Fields(&quot;args&quot;), new Split(), new Fields(&quot;word&quot;)) // 6       .groupBy(new Fields(&quot;word&quot;)) // 7       .stateQuery(wordCounts, new Fields(&quot;word&quot;), new MapGet(), new Fields(&quot;count&quot;)) // 8       .each(new Fields(&quot;count&quot;), new FilterNull()) // 9       .aggregate(new Fields(&quot;count&quot;), new Sum(), new Fields(&quot;sum&quot;)); // 10</code></pre><p>使用同一个<code>TridentTopology</code>对象来创建DRPC流，命名该函数为words，函数名与<code>DRPCClient</code>执行时的第一个参数名称相同。每个DRPC请求都是一个小的批量作业，将请求的单个tuple作为输入，tuple中包含名为args的字段，args中包含了客户端的请求参数。在这个例子中，请求参数就是“cat dog the man”。</p><p>行6，通过<code>Split</code>函数将请求参数分解为一个个单词，组成“word”流。</p><p>行7，将上一步分解的“word”流分组。</p><p>行8，<code>stateQuery</code>方法用于查询第一部分生成的<code>TridentState</code>对象。<code>MapGet</code>将被执行，根据输入的单词，查询单词的数量。因为DRPC流的分组方式与<code>TridentState</code>分组方式相同（都是通过word字段），所以每个单词查询会被自动路由到该单词的<code>TridentState</code>对象的分区。</p><p>行9，通过<code>FilterNull</code>函数过滤掉没有计数结果的单词。</p><p>行10，使用<code>Sum</code>函数对存在计数结果的进行加和，然后通过<code>Trident</code>自动将结果返回客户端。</p><p><code>Trident</code>在如何以最大限度的提高性能来执行拓扑方面是非常智能的。比如，它会进行下面两个提高性能的自动化操作：</p><ol><li>对状态的读取或写入操作（如<code>persistentAggregate</code>和<code>stateQuery</code>），会自动的进行批处理。比如，如果需要在当前批处理操作中执行20次更新操作，<code>Trident</code>会自动批量的读取或写入，仅执行一次读请求或写请求，而不是20次。</li><li><code>Trident</code>聚合操作进行了大量优化。<code>Trident</code>会将同一个组中的所有tuple发送到同一台机器，进行部分聚合操作，然后再通过网络发送tuple。比如，<code>Count</code>聚合操作会先计算每个分区的数量，然后将这些统计结果通过网络传输到一起，再根据这些初始统计结果计算最后的结果。</li></ol><h1 id="2-再来个例子"><a href="#2-再来个例子" class="headerlink" title="2 再来个例子"></a>2 再来个例子</h1><p>下面的例子是一个纯DRPC拓扑，用于计算URL的覆盖范围，就是在<a href="https://twitter.com/" target="_blank" rel="noopener">Twitter</a>上发布的URL影响的范围。要计算这个数据，需要先获取所有推送URL的人，然后获取所有这些人的粉丝，去重，计算总数。这种计算需要消耗非常多的资源，可能需要数千次数据库查询操作和数千万的tuple（当然，这是针对Twitter这种应用体量来说的。如果用户只有几个，一个SQL估计就出结果了）。所以就需要Storm和Trident这种可以并行化跨集群的计算。</p><p>下面这个拓扑会从两个状态读取数据：一个是将URL与分享过该URL的人做的映射，另一个是将一个人与这个人的粉丝做的映射。查询拓扑如下：</p><pre><code class="java">TridentState urlToTweeters = topology.newStaticState(getUrlToTweetersState());TridentState tweetersToFollowers = topology.newStaticState(getTweeterToFollowersState());topology.newDRPCStream(&quot;reach&quot;)       .stateQuery(urlToTweeters, new Fields(&quot;args&quot;), new MapGet(), new Fields(&quot;tweeters&quot;)) // 1       .each(new Fields(&quot;tweeters&quot;), new ExpandList(), new Fields(&quot;tweeter&quot;)) // 2       .shuffle()       .stateQuery(tweetersToFollowers, new Fields(&quot;tweeter&quot;), new MapGet(), new Fields(&quot;followers&quot;)) // 3       .parallelismHint(200)       .each(new Fields(&quot;followers&quot;), new ExpandList(), new Fields(&quot;follower&quot;)) // 4       .groupBy(new Fields(&quot;follower&quot;)) // 5       .aggregate(new One(), new Fields(&quot;one&quot;)) // 6       .parallelismHint(20)       .aggregate(new Count(), new Fields(&quot;reach&quot;));</code></pre><p>该拓扑使用<code>newStaticState</code>方法创建从外部数据库获取的<code>TridentState</code>对象，然后在拓扑中进行查询。与其他类似，这些数据库查询可以自动批量化，以求效率最大化。下面分解来说：</p><p>行1，将前面从urlToTweeters数据库读取的数据作为输入，通过<code>MapGet</code>函数进行处理。</p><p>行2，通过<code>ExpandList</code>函数将每个人分解为不同的tuple数据。</p><p>行3，将前面从tweeterToFollowers数据库读取的数据作为输入，与行2中的数据，通过<code>MapGet</code>函数进行处理，查询每个人的粉丝列表。这里最重要的就是并行话，所以需要<code>shuffle</code>来将所有人均匀分布到所有的<code>workder</code>上。可以看到在下面的<code>parallelismHint</code>中，并行数是200，说明该操作并行程度非常高，占据了计算的大部分资源。</p><p>行4、行5、行6，这里是对粉丝进行单独统计和计数。首先是粉丝列表数据通过<code>ExpandList</code>函数进行分解为不同tuple数据，然后将粉丝数据进行分组，在对粉丝数据进行<code>One</code>聚合。这里也是通过<code>parallelismHint</code>进行并行计算。</p><p><code>One</code>聚合器定义如下：</p><pre><code class="java">public class One implements CombinerAggregator&lt;Integer&gt; {   public Integer init(TridentTuple tuple) {       return 1;   }   public Integer combine(Integer val1, Integer val2) {       return 1;   }   public Integer zero() {       return 1;   }        }</code></pre><p>这是一个<code>combiner</code>聚合器，它能够先进行部分聚合，然后通过网络传输tuple进行最后的聚合，以最大限度地提高效率。</p><p>接下来说说Trident的数据结构。</p><h1 id="3-字段和tuple数据"><a href="#3-字段和tuple数据" class="headerlink" title="3 字段和tuple数据"></a>3 字段和tuple数据</h1><p>Trident中用来传输的数据模型名为<code>TridentTuple</code>，是一个命名的值列表。在一个拓扑中，tuple通过一系列的操作逐步创建。这些操作就是将输入数据进行处理，然后返回输出数据。</p><p>比如，有一个名为“stream”的流，它包含字段“x”，“y”和“z”。 要运行一个以“y”为输入的过滤器MyFilter，可以这样做：</p><pre><code class="java">stream.each(new Fields(&quot;y&quot;), new MyFilter())</code></pre><p><code>MyFilter</code>定义如下：</p><pre><code class="java">public class MyFilter extends BaseFilter {   public boolean isKeep(TridentTuple tuple) {       return tuple.getInteger(0) &lt; 10;   }}</code></pre><p>这里保留了“y”字段小于10的所有tuple数据。<code>MyFilter</code>输入的<code>TridentTuple</code>只有“y”字段。Trident可以高效的选择一组tuple作为输入，这种选择是0消耗的。</p><p>接下来看看<code>function fields</code>是如何工作的，不如下面这个函数：</p><pre><code class="java">public class AddAndMultiply extends BaseFunction {   public void execute(TridentTuple tuple, TridentCollector collector) {       int i1 = tuple.getInteger(0);       int i2 = tuple.getInteger(1);       collector.emit(new Values(i1 + i2, i1 * i2));   }}</code></pre><p>这个操作是将两个数字作为数据，然后发出两个新值：两数字的和与积。比如有“x”，“y”和“z”三个字段的流，可以如下操作：</p><pre><code class="java">stream.each(new Fields(&quot;x&quot;, &quot;y&quot;), new AddAndMultiply(), new Fields(&quot;added&quot;, &quot;multiplied&quot;));</code></pre><p><code>function</code>函数操作的输出结果是追加在输入tuple中的，因此经过<code>AddAndMultiply</code>操作之后输出tuple包含五个字段：“x”，“y”，“z”，“added”和“multipl”。“added”对应的是<code>AddAndMultiply</code>发出的第一个值，”multiplied”对应的是第二个值。</p><p><code>aggregator</code>聚合操作是替换tuple数据，比如，有一个包含“val1”和“val2”的流：</p><pre><code class="java">stream.aggregate(new Fields(&quot;val2&quot;), new Sum(), new Fields(&quot;sum&quot;))</code></pre><p>输入结果将只包含“sum”一个字段的tuple，表示“val2”的和。</p><p><code>groupBy</code>分组操作，输出结果将包含分组字段和聚合器发出的字段，比如：</p><pre><code class="java">stream.groupBy(new Fields(&quot;val1&quot;))     .aggregate(new Fields(&quot;val2&quot;), new Sum(), new Fields(&quot;sum&quot;))</code></pre><p>例子中，输出结果包含“val2”和“sum”两个字段。</p><h1 id="4-状态"><a href="#4-状态" class="headerlink" title="4 状态"></a>4 状态</h1><p>实时计算解决问题的一个关键是如何管理状态，以便在失败或重试时能够实现幂等性。消除实时计算过程中的故障是不可能的，所以，当一个节点死亡或其他问题出现时，就需要重试。问题是，如何更新状态（通过外部存储或拓扑内部状态），以便每个消息只处理一次。</p><p>这是一个棘手的问题，有一下几种情况。假设，如果要做一个计数的聚合操作，然后将技术存储在数据库中。如果只是将结果存储，当需要更新结果的时候，无法知道这个结果是否是新结果还是重试结果。该结果可能是之前尝试过更新过的，已经成功更新了数据库，但是后续步骤中失败了。也可能是尝试更新数据库的时候，更新数据库失败了。</p><p>Trident通过下面的方式解决这个问题：</p><ol><li>每个批次的数据被赋予唯一的ID，称为“transaction id”。如果这批数据重新计算，会携带相同的ID。</li><li>批次将状态隔离。在批次2状态更新之前，不会更新批次3的状态。</li></ol><p>更加这两个原子操作，可以通过状态更新完成一次语义操作。需要将事务ID与计数结果一起存储，作为原子值。然后，当更新操作时，通过数据库中的事务ID与当前批次的事务ID进行比较。如果相同，说明是同一批次的数据，就跳过更新。如果不同，就增加计数。</p><p>当然，不需要在拓扑中手动执行这个逻辑，这个逻辑包装在State中并自动完成。而且，State 状态对象也不是实现事务ID所必需的，如果不想在数据库中存储事务ID，也可以不存储。这样的话，State 在失败的情况下也会至少执行一次（可能会更好）。可以在<a href="/storm/trident-state/">这里</a>或<a href="http://storm.apache.org/releases/current/Trident-state.html" target="_blank" rel="noopener">这里</a>学习更多的关于State状态的内容。</p><p>State 状态可以存储在任何地方，外部存储，内部状态等。如果想使用一个内存状态实现，保留几个小时的数据可用，然后就将其丢弃，可以看<a href="https://github.com/nathanmarz/trident-memcached/blob/master/src/jvm/trident/memcached/MemcachedState.java" target="_blank" rel="noopener">这里</a>。</p><h1 id="5-执行拓扑"><a href="#5-执行拓扑" class="headerlink" title="5 执行拓扑"></a>5 执行拓扑</h1><p>Trident拓扑结果最后会编译成高效的Strom拓扑。当需要重新分区数据时，比如<code>groupBy</code>或<code>shuffle</code>，tuple就只能通过网络传输。所以如果Trident拓扑结构如下：</p><p><img src="http://www.howardliu.cn/images/storm/trident-to-storm1.png" alt="trident to storm"></p><p>最后会被编译成Strom spout/bolt的结构，如下：</p><p><img src="http://www.howardliu.cn/images/storm/trident-to-storm2.png" alt="trident to storm"></p><h1 id="6-结论"><a href="#6-结论" class="headerlink" title="6 结论"></a>6 结论</h1><p>Trident 使实时计算更加优雅。可以通过Trident的API实现高吞吐的流处理、状态处理、低延迟查询等功能。而且Trident中做了很多优化，可以获取最大性能。</p><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a></p><p>个人博文: <a href="http://www.howardliu.cn/storm/trident-tutorial">storm笔记：Trident应用</a></p><p>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a></p><p>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/68944280" target="_blank" rel="noopener">storm笔记：Trident应用</a></p>]]></content>
    
    <summary type="html">
    
      Trident是基于Storm的实时计算模型的高级抽象。它可以实现高吞吐（每秒数百万条消息）的有状态流处理和低延迟分布式查询。如果以前使用过高级批处理工具（比如Pig或Cascading），则对Trident的概念会非常熟悉，比如连接、聚合、分组、功能处理和过滤等。除此之外，Trident还增加了用于在数据库或持久化存储上进行有状态的增量处理的原语。Trident具有一致性、一次性语义，所以很容易就能够推导出Trident拓扑结构。
    
    </summary>
    
    
      <category term="storm" scheme="http://www.howardliu.cn/categories/storm/"/>
    
    
      <category term="大数据" scheme="http://www.howardliu.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="storm" scheme="http://www.howardliu.cn/tags/storm/"/>
    
      <category term="实时计算" scheme="http://www.howardliu.cn/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Trident" scheme="http://www.howardliu.cn/tags/Trident/"/>
    
  </entry>
  
  <entry>
    <title>常用消息队列对比</title>
    <link href="http://www.howardliu.cn/middleware/the-different-between-some-distributed-message-queue/"/>
    <id>http://www.howardliu.cn/middleware/the-different-between-some-distributed-message-queue/</id>
    <published>2017-03-08T08:03:32.000Z</published>
    <updated>2019-10-12T06:51:00.524Z</updated>
    
    <content type="html"><![CDATA[<p>作为中间件，消息队列是分布式应用间交换信息的重要组件。消息队列可驻留在内存或磁盘上, 队列可以存储消息直到它们被应用程序读走。通过消息队列，应用程序可以在不知道彼此位置的情况下独立处理消息，或者在处理消息前不需要等待接收此消息。所以消息队列可以解决应用解耦、异步消息、流量削锋等问题，是实现高性能、高可用、可伸缩和最终一致性架构中不可以或缺的一环。下面对消息队列就直接使用MQ表示。</p><p><img src="/images/middleware/message-queue.png" alt="消息队列"></p><a id="more"></a><p>现在比较常见的MQ产品主要是ActiveMQ、RabbitMQ、ZeroMQ、Kafka、MetaMQ、RocketMQ，当然还有很多其他的产品。因为个人水平有限，就简单的比较下这几种MQ的优缺点，作为自己选型的参考。</p><h1 id="1-ZeroMQ"><a href="#1-ZeroMQ" class="headerlink" title="1 ZeroMQ"></a>1 ZeroMQ</h1><p><a href="http://zeromq.org/" target="_blank" rel="noopener"><img src="http://zeromq.wdfiles.com/local--files/admin:css/logo.gif" alt="http://zeromq.org/"></a></p><blockquote><p>ZeroMQ (also known as ØMQ, 0MQ, or zmq) looks like an embeddable networking library but acts like a concurrency framework. It gives you sockets that carry atomic messages across various transports like in-process, inter-process, TCP, and multicast. You can connect sockets N-to-N with patterns like fan-out, pub-sub, task distribution, and request-reply. It’s fast enough to be the fabric for clustered products. Its asynchronous I/O model gives you scalable multicore applications, built as asynchronous message-processing tasks. It has a score of language APIs and runs on most operating systems. ZeroMQ is from iMatix and is LGPLv3 open source.</p></blockquote><p>ZeroMQ号称是“史上最快的消息队列”，基于c语言开发的，可以在任何平台通过任何代码连接，通过inproc、IPC、TCP、TIPC、多播传送消息，支持发布-订阅、推-拉、共享队列等模式，高速异步I/O引擎。</p><p>根据官方的说法，ZeroMQ是一个简单好用的传输层，像框架一样的可嵌入的socket类库，使Socket编程更加简单、简洁、性能更高，是专门为高吞吐量/低延迟的场景开发的。ZeroMQ与其他MQ有着本质的区别，它根本不是消息队列服务器，更类似与一个底层网络通讯库，对原有Socket API进行封装，在使用的使用引入对应的jar包即可，可谓是相当灵活。</p><p>同时，因为它的简单灵活，如果我们想作为消息队列使用的话，需要开发大量代码。而且，ZeroMQ不支持消息持久化，其定位并不是安全可靠的消息传输，所以还需要自己编码保证可靠性。简而言之一句话，ZeroMQ很强大，但是想用好需要自己实现。</p><h1 id="2-RabbitMQ"><a href="#2-RabbitMQ" class="headerlink" title="2 RabbitMQ"></a>2 RabbitMQ</h1><p><a href="http://www.rabbitmq.com/" target="_blank" rel="noopener"><img src="http://www.rabbitmq.com/img/rabbitmq_logo_strap.png" alt="http://www.rabbitmq.com/"></a></p><blockquote><p>官方定义：</p></blockquote><ul><li>Robust messaging for applications</li><li>Easy to use</li><li>Runs on all major operating systems</li><li>Supports a huge number of developer platforms</li><li>Open source and commercially supported</li></ul><p>RabbitMQ是基于Erlang语言编写的开源消息队列，通过Erlang的Actor模型实现了数据的稳定可靠传输。本身是实现AMQP的消息队列，因此官方推荐，如果仅仅是使用RabbitMQ的话，建议使用AMQP 0-9-1的协议。不过，因为其可扩展性，可以通过插件的形式使用STOMP、XMPP、AMQP 1.0，还可以通过插件使用HTTP这种非消息的传输协议。所以，RabbitMQ可以说是适应性非常强的一个消息队列中间件了。</p><p>当然，不仅是协议支持的多，还因为它实现了代理(Broker)架构，意味着消息在发送到客户端之前可以在中央节点上排队。此特性使得RabbitMQ易于使用和部署，适宜于很多场景如路由、负载均衡或消息持久化等，用消息队列只需几行代码即可搞定。但是，这使得它的可扩展性差，速度较慢，因为中央节点增加了延迟，消息封装后也比较大，如需配置RabbitMQ则需要在目标机器上安装Erlang环境。</p><p>总的来说，RabbitMQ在数据一致性、稳定性和可靠性方面比较优秀，而且直接或间接的支持多种协议，对多种语言支持良好。但是其性能和吞吐量差强人意，由于Erlang语言本身的限制，二次开发成本较高。</p><h1 id="3-ActiveMQ"><a href="#3-ActiveMQ" class="headerlink" title="3 ActiveMQ"></a>3 ActiveMQ</h1><p><a href="http://activemq.apache.org/" target="_blank" rel="noopener"><img src="http://activemq.apache.org/images/activemq-logo.png" alt="http://activemq.apache.org/"></a></p><blockquote><p>Apache ActiveMQ ™ is the most popular and powerful open source messaging and Integration Patterns server.<br>Apache ActiveMQ is fast, supports many Cross Language Clients and Protocols, comes with easy to use Enterprise Integration Patterns and many advanced features while fully supporting JMS 1.1 and J2EE 1.4. Apache ActiveMQ is released under the Apache 2.0 License.</p></blockquote><p>ActiveMQ介于ZeroMQ和RabbitMQ之间。类似于ZeroMQ，它可以部署于代理模式和P2P模式。类似于RabbitMQ，它易于实现高级场景，而且只需付出低消耗。被誉为消息中间件的“瑞士军刀”。</p><p>支持OpenWire、Stomp、AMQP v1.0、MQTT v3.1、REST、Ajax、Webservice等多种协议；完全支持JMS1.1和J2EE 1.4规范（事务、持久化、XA消息）；支持持久化到数据库。但是ActiveMQ不够轻巧，而且对于队列较多的情况支持不好，据说还有丢消息的情况。</p><p>目前已经有了其下一代消息产品Apollo。</p><h1 id="4-Apollo"><a href="#4-Apollo" class="headerlink" title="4 Apollo"></a>4 Apollo</h1><p><a href="https://activemq.apache.org/apollo/" target="_blank" rel="noopener"><img src="https://activemq.apache.org/apollo/images/project-logo.png" alt="https://activemq.apache.org/apollo/"></a></p><blockquote><p>ActiveMQ Apollo is a faster, more reliable, easier to maintain messaging broker built from the foundations of the original ActiveMQ. It accomplishes this using a radically different threading and message dispatching architecture. Like ActiveMQ, Apollo is a multi-protocol broker and supports STOMP, AMQP, MQTT, Openwire, SSL, and WebSockets.</p></blockquote><p>Apache称Apollo为最快、最强健的STOMP服务器。支持STOMP、AMQP、MQTT、OpenWire协议，支持Topic、Queue、持久订阅等消费形式，支持对消息的多种处理，支持安全性处理，支持REST管理API。。。功能列表很长，最大的弊病就是目前市场接收度不够，所以使用的并不广泛。</p><h1 id="5-Kafka"><a href="#5-Kafka" class="headerlink" title="5 Kafka"></a>5 Kafka</h1><p><a href="http://kafka.apache.org/" target="_blank" rel="noopener"><img src="http://kafka.apache.org/images/logo.png" alt="http://kafka.apache.org/"></a></p><blockquote><p>Kafka™ is used for building real-time data pipelines and streaming apps. It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies.</p></blockquote><p>Kafka是LinkedIn于2010年12月开发并开源的一个分布式流平台，现在是Apache的顶级项目，是一个高性能跨语言分布式Publish/Subscribe消息队列系统，以Pull的形式消费消息。具有以下特性：快速持久化，可以在O(1)的系统开销下进行消息持久化；高吞吐，在一台普通的服务器上既可以达到10W/s的吞吐速率；完全的分布式系统，Broker、Producer、Consumer都原生自动支持分布式，自动实现复杂均衡。因为设计之初是作为日志流平台和运营消息管道平台，所以实现了消息顺序和海量堆积。</p><p>Kafka自身服务与消息的生产和消费都依赖与Zookeeper，使用Scala语言开发。因为其消息的消费使用客户端Pull方式，消息可以被多个客户端消费，理论上消息会重复，但是不会丢失（除非消息过期）。因此比较常用的场景是作为日志传输的消息平台。</p><h1 id="6-RocketMQ"><a href="#6-RocketMQ" class="headerlink" title="6 RocketMQ"></a>6 RocketMQ</h1><p><a href="https://rocketmq.incubator.apache.org/" target="_blank" rel="noopener"><img src="https://rocketmq.incubator.apache.org/assets/images/rmq-logo.png" alt="https://rocketmq.incubator.apache.org/"></a></p><blockquote><p>Apache RocketMQ™ is an open source distributed messaging and streaming data platform.</p></blockquote><p>RocketMQ是阿里开源的消息中间件，目前在Apache孵化，使用纯Java开发，具有高吞吐量、高可用性、适合大规模分布式系统应用的特点。RocketMQ思路起源于Kafka，但并不是简单的复制，它对消息的可靠传输及事务性做了优化，目前在阿里集团被广泛应用于交易、充值、流计算、消息推送、日志流式处理、binglog分发等场景，支撑了阿里多次双十一活动。</p><p>因为是阿里内部从实践到产品的产物，因此里面很多接口、api并不是很普遍适用。其可靠性毋庸置疑，而且与Kafka一脉相承（甚至更优），性能强劲，支持海量堆积。不过据说，没有在mq核心上实现JMS，但是也无伤大雅。</p><h1 id="7-写在最后"><a href="#7-写在最后" class="headerlink" title="7 写在最后"></a>7 写在最后</h1><p>其实对于这些消息队列的产品，每一种都在某一领域占有一席，虽然ActiveMQ目前在社区已经不是很活跃，但是其下一代产品Apollo已经问世。ZeroMQ小而美，RabbitMQ大而稳，Kakfa和RocketMQ快而强劲。RocketMQ虽然目前还很多不完善，但是一旦在Apache孵化成为顶级项目，全球程序猿开始贡献，前途也是不可限量的。</p><hr><p>个人主页: <a href="http://www.howardliu.cn">http://www.howardliu.cn</a></p><p>个人博文: <a href="http://www.howardliu.cn/middleware/the-different-between-some-distributed-message-queue/">常用消息队列对比</a></p><p>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a></p><p>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/60875715" target="_blank" rel="noopener">常用消息队列对比</a></p>]]></content>
    
    <summary type="html">
    
      作为中间件，消息队列是分布式应用间交换信息的重要组件。消息队列可驻留在内存或磁盘上, 队列可以存储消息直到它们被应用程序读走。通过消息队列，应用程序可以在不知道彼此位置的情况下独立处理消息，或者在处理消息前不需要等待接收此消息。所以消息队列可以解决应用解耦、异步消息、流量削锋等问题，是实现高性能、高可用、可伸缩和最终一致性架构中不可以或缺的一环。下面对消息队列就直接使用MQ表示。
    
    </summary>
    
    
      <category term="middleware" scheme="http://www.howardliu.cn/categories/middleware/"/>
    
      <category term="mq" scheme="http://www.howardliu.cn/categories/middleware/mq/"/>
    
    
      <category term="中间件" scheme="http://www.howardliu.cn/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
      <category term="消息队列" scheme="http://www.howardliu.cn/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="选型" scheme="http://www.howardliu.cn/tags/%E9%80%89%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>HTTP长连接和短连接</title>
    <link href="http://www.howardliu.cn/http/http-connection/"/>
    <id>http://www.howardliu.cn/http/http-connection/</id>
    <published>2017-03-01T06:45:12.000Z</published>
    <updated>2019-10-22T13:30:43.112Z</updated>
    
    <content type="html"><![CDATA[<p>一直听别人说HTTP长连接，只知道长连接比短连接更节省资源、更快捷，但是并不真的知道原因。知其然不知其所以然，对于技术来说，这种状态是比较危险的。所以，还是要挖一下原理，即使挖的比较浅，也要迈出这一步。</p><p>HTTP是应用层协议，传输层使用的是TCP协议，网络层使用的是IP协议。</p><p>IP协议主要解决网络路由和寻址问题，TCP协议主要解决如何在IP层之上可靠的传递数据包，使在网络上的另一端收到发送端发出的所有包，并且顺序与发出顺序一致，HTTP协议主要基于TCP协议完成数据传递。</p><a id="more"></a><p><img src="/images/http/network-layer.jpg" alt="OSI参考模型与TCP/IP模型"></p><p>首先说下TCP连接与断开。</p><p>TCP的生命周期被戏称为三次握手和四次挥手，每次握手（挥手）都需要通信双方交互数据，所以TCP连接传输数据是比较耗费资源的，也就是通常所说的成本较高。</p><p><img src="/images/http/tcp-connect-disconnect.png" alt="TCP的三次握手和四次挥手"></p><p>然后，HTTP协议是无状态的、面向连接的，即协议本身不具备记忆能力，两次不同的HTTP请求之间没有任何联系。</p><p>在HTTP/1.1之前，一个网页加载资源的时候，每需要加载一个资源，就需要进行一次HTTP请求，建立一次连接，请求结束就断开连接，而每次连接（TCP连接）都需要耗费资源（时间资源、网络资源等）。</p><p>所以后来在HTTP/1.1中引入持久连接（persistent connection），即TCP连接默认不关闭，可以被多个请求复用，不用声明<code>Connection: keep-alive</code>，也就是常说的长连接。</p><p>这里所说的长连接，其实本身是TCP长连接。比如发起一次HTTP请求时，客户端与服务端创建TCP连接，在得到响应结果后，不进行TCP的四次挥手断开连接，而是会保持一段时间的TCP连接。此时，如果又有一次HTTP请求相同的服务端，就会继续使用这一个TCP连接。这样，节省了TCP连接的消耗。</p><p>当然，为了资源的有效利用，在一段时间（超时时间，请求头中<code>Keep-alive: timeout=10</code>进行设置）没有活动时，客户端或服务端会主动断开TCP连接。建议的做法是，在客户端最后一次请求时，请求头中发送<code>Connection: close</code>，明确要求关闭TCP连接。</p><hr><p>补充下TCP三次握手、四次挥手的知识。</p><p>三次握手建立连接：</p><ol><li>第一次握手：客户端发送SYN包(seq=x)到服务器，并进入SYN_SEND状态，等待服务器确认；</li><li>第二次握手：服务器收到SYN包，必须确认客户的SYN（ACK=x+1），同时自己也发送一个SYN包（seq=y），即SYN+ACK包，此时服务器进入SYN_RECV状态；</li><li>第三次握手：客户端收到服务器的SYN＋ACK包，向服务器发送确认包ACK(ACK=y+1)，此包发送完毕，客户端和服务器进入ESTABLISHED状态，完成三次握手。</li></ol><p>握手过程中传送的包里不包含数据，三次握手完毕后，客户端与服务器才正式开始传送数据。理想状态下，TCP连接一旦建立，在通信双方中的任何一方主动关闭连接之前，TCP 连接都将被一直保持下去。</p><p>四次挥手断开连接</p><ol><li>第一次挥手：主动关闭方发送一个FIN，用来关闭主动方到被动关闭方的数据传送，也就是主动关闭方告诉被动关闭方：我已经不会再给你发数据了（当然，在FIN包之前发送出去的数据，如果没有收到对应的ACK确认报文，主动关闭方依然会重发这些数据），但此时主动关闭方还可以接受数据。</li><li>第二次挥手：被动关闭方收到FIN包后，发送一个ACK给对方，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号）。</li><li>第三次挥手：被动关闭方发送一个FIN，用来关闭被动关闭方到主动关闭方的数据传送，也就是告诉主动关闭方，我的数据也发送完了，不会再给你发数据了。</li><li>第四次挥手：主动关闭方收到FIN后，发送一个ACK给被动关闭方，确认序号为收到序号+1，至此，完成四次挥手。</li></ol><blockquote><p>注意：中断连接端可以是Client端，也可以是Server端。</p></blockquote><p>详细的状态说明</p><ul><li>SYN_SEND：客户端尝试链接服务端，通过open方法。也就是TCP三次握手中的第1步之后,注意是客户端状态</li><li>SYN_RECEIVED： 服务接受创建请求的SYN后，也就是TCP三次握手中的第2步，发送ACK数据包之前。注意是服务端状态,一般15个左右正常，如果很大，怀疑遭受SYN_FLOOD攻击</li><li>ESTABLISHED：客户端接受到服务端的ACK包后的状态，服务端在发出ACK在一定时间后即为ESTABLISHED</li><li>FIN_WAIT1：主动关闭的一方，在发出FIN请求之后，也就是在TCP四次挥手的第1步</li><li>CLOSE_WAIT：被动关闭的一方，在接受到客户端的FIN后，也就是在TCP四次挥手的第2步</li><li>FIN_WAIT2：主动关闭的一方，在接受到被动关闭一方的ACK后，也就是TCP四次挥手的第2步。可以设定被动关闭方返回FIN后的超时时间，有效回收链接，避免syn-flood.</li><li>LASK_ACK：被动关闭的一方，在发送ACK后一段时间后(确保客户端已收到)，再发起一个FIN请求。也就是TCP四次挥手的第3步</li><li>TIME_WAIT：主动关闭的一方，在收到被动关闭的FIN包后，发送ACK。也就是TCP四次挥手的第4步</li></ul>]]></content>
    
    <summary type="html">
    
      HTTP是应用层协议，传输层使用的是TCP协议，网络层使用的是IP协议。IP协议主要解决网络路由和寻址问题，TCP协议主要解决如何在IP层之上可靠的传递数据包，使在网络上的另一端收到发送端发出的所有包，并且顺序与发出顺序一致，HTTP协议主要基于TCP协议完成数据传递。
    
    </summary>
    
    
      <category term="http" scheme="http://www.howardliu.cn/categories/http/"/>
    
    
      <category term="概念" scheme="http://www.howardliu.cn/tags/%E6%A6%82%E5%BF%B5/"/>
    
      <category term="网络" scheme="http://www.howardliu.cn/tags/%E7%BD%91%E7%BB%9C/"/>
    
      <category term="http" scheme="http://www.howardliu.cn/tags/http/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper客户端错误：Packet len8854970 is out of range!</title>
    <link href="http://www.howardliu.cn/zookeeper/zookeeper-error-packet-lenX-is-out-of-range/"/>
    <id>http://www.howardliu.cn/zookeeper/zookeeper-error-packet-lenX-is-out-of-range/</id>
    <published>2017-02-20T09:06:45.000Z</published>
    <updated>2019-09-11T14:08:23.875Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个生产环境使用zookeeper异常的情况，错误是<code>java.io.IOException: Packet len8854970 is out of range!</code>。然后就换了一个namespace，就没有在出错，以为是偶然发生，所以没有重视。但是年后居然又出现问题，才意识到严重性。分析之后发现，每隔一段时间，某一个znode节点下超过客户端所设置的大小，客户端连接会失败，zkCli.sh操作该节点也会失败。如果对于简单依赖zookeeper的系统，这种错误可以容忍（但是必须解决）；如果是强依赖zookeeper的系统，这种错误可以说是灾难。</p><a id="more"></a><h1 id="1-发现问题"><a href="#1-发现问题" class="headerlink" title="1 发现问题"></a>1 发现问题</h1><p>问题的发现比较曲折，首先是发现服务器磁盘写满了（吐槽下运维居然没有对磁盘添加监控），致使项目中的报警功能失效。然后就把无用日志删除，习（tou）惯（lan）的把异常的应用重启了下。幸好有个比较好的习惯就是，项目启动成功后，都会打看日志跟踪下，确定无误才会关掉终端。结果就被错误日志刷屏了：</p><pre><code class="java">ERROR 2017-02-20 10:45:44,729 [Curator-Framework-0] o.a.c.f.i.CuratorFrameworkImpl.logError() (CuratorFrameworkImpl.java:557) - Background retry gave uporg.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss        at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:838) [curator-framework-2.10.0.jar:na]        at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:na]        at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:na]        at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:na]        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_111]        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_111]        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [na:1.8.0_111]        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111]        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111]        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111]</code></pre><p>因为zookeeper的客户端使用的是<a href="http://curator.apache.org/" target="_blank" rel="noopener">Apache Curator</a>，与zookeeper的连接断开会重新创建连接，所以会出现大量的连接失败异常。</p><h1 id="2-分析问题"><a href="#2-分析问题" class="headerlink" title="2 分析问题"></a>2 分析问题</h1><p>在这个项目中，对于zookeepr的使用有下面几种场景：</p><ol><li>服务状态数据：将服务状态写入znode节点</li><li>服务注册：将服务地址写入znode节点</li><li>服务发现：获取znode节点中的可用服务地址</li><li>监听节点：监听某个znode节点或该节点的字节点状态</li></ol><p>这几种情况都需要与zookeeper的znode节点创建连接，然后执行操作。根据错误提示，<code>java.io.IOException: Packet len8854970 is out of range!</code>，out of range就是超过了某个限制，只能查看代码了。</p><pre><code class="java">protected final ByteBuffer lenBuffer = ByteBuffer.allocateDirect(4);protected ByteBuffer incomingBuffer = lenBuffer;protected void readLength() throws IOException {    int len = incomingBuffer.getInt();    if (len &lt; 0 || len &gt;= ClientCnxn.packetLen) {        throw new IOException(&quot;Packet len&quot; + len + &quot; is out of range!&quot;);    }    incomingBuffer = ByteBuffer.allocate(len);}public static final int packetLen = Integer.getInteger(&quot;jute.maxbuffer&quot;, 4096 * 1024);</code></pre><p>从代码就能够很容易的看出，这个错误是因为<code>len</code>小于0或大于<code>packetLen</code>，根据代码逻辑，<code>len</code>不小于0，那就是大于<code>packetLen</code>。而<code>packetLen</code>的值是<code>jute.maxbuffer</code>系统变量定义或默认的4096 * 1024（4M）。</p><p>继续深扒代码，因为代码比较长，这里就不写了。大体逻辑就是，创建与zookeeper连接之后，要对某个节点进行读写操作，为了提高吞吐量，先判断下该节点数据量大小是否超过设置的<code>jute.maxbuffer</code>，如果是，就抛出异常。在zookeeper客户端中，这一部分异常的处理比较粗糙，因为注释上也写着“this is ugly, you have a better way speak up”。</p><h1 id="3-解决问题"><a href="#3-解决问题" class="headerlink" title="3 解决问题"></a>3 解决问题</h1><p>根据上面的纠错，答案就很明显了。只有两种方案：</p><ul><li>把待操作节点的大小减下来，小于默认的4M</li><li>把默认的<code>jute.maxbuffer</code>大小提高</li></ul><p>对于第一种方式，需要根据自身具体情况具体操作。这里没有什么有效建议。</p><p>对于第二种方式，就比较简单了。只要在创建Zookeeper对象之前，设置<code>System.setProperty(&quot;jute.maxbuffer&quot;, 4096 * 1024 * 10 + &quot;&quot;);</code>，这里的大小根据自己的系统设置，我这里只是一个测试值（如果设置太大，这个节点真的比较大的话，会影响吞吐）。</p><blockquote><p>因为我这里使用的是<a href="http://curator.apache.org/" target="_blank" rel="noopener">Apache Curator</a>，不需要自己创建Zookeeper对象，所以需要在创建CuratorFramework对象之前添加这个变量。</p></blockquote><p>解决问题要彻底，不能留下祸患。（此处应该伴随阴笑和冷风。。。）</p><p>java客户端的问题解决了，但是通过zkCli.sh连接时，还是会出现这个问题。报错如下：</p><pre><code>2017-02-20 12:08:03,999 [myid:] - WARN  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1102] - Session 0x1591b713cefd2b3 for server localhost/127.0.0.1:2181, unexpected error, closing socket connection and attempting reconnectjava.io.IOException: Packet len8854970 is out of range!    at org.apache.zookeeper.ClientCnxnSocket.readLength(ClientCnxnSocket.java:112)    at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:79)    at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:366)    at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)WATCHER::WatchedEvent state:Disconnected type:None path:nullException in thread &quot;main&quot; org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /it-monitor/com/wfj/monitor/sales-overview-info    at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)    at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)    at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1472)    at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1500)    at org.apache.zookeeper.ZKUtil.listSubTreeBFS(ZKUtil.java:114)    at org.apache.zookeeper.ZKUtil.deleteRecursive(ZKUtil.java:49)    at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:703)    at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:588)    at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:360)    at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:323)    at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:282)</code></pre><p>找到zkCli.sh，最下面的java命令中添加对<code>jute.maxbuffer</code>的定义（使用D参数）：</p><pre><code class="sh">&quot;$JAVA&quot; &quot;-Dzookeeper.log.dir=${ZOO_LOG_DIR}&quot; &quot;-Dzookeeper.root.logger=${ZOO_LOG4J_PROP}&quot; \     &quot;-Djute.maxbuffer=41943040&quot; \     -cp &quot;$CLASSPATH&quot; $CLIENT_JVMFLAGS $JVMFLAGS \     org.apache.zookeeper.ZooKeeperMain &quot;$@&quot;</code></pre><p>当然，为了运维方便，可以把<code>jute.maxbuffer</code>的值设置成变量，通过修改配置来设置值。避免因为修改sh脚本出现其他问题。</p>]]></content>
    
    <summary type="html">
    
      这是一个生产环境使用zookeeper异常的情况，错误是java.io.IOException:Packet len8854970 is out of range!。因为是偶然发生，所以没有重视。但是年后又发现问题，才意识到问题的严重性。
    
    </summary>
    
    
      <category term="open-source" scheme="http://www.howardliu.cn/categories/open-source/"/>
    
      <category term="big-data" scheme="http://www.howardliu.cn/categories/open-source/big-data/"/>
    
      <category term="zookeeper" scheme="http://www.howardliu.cn/categories/open-source/big-data/zookeeper/"/>
    
      <category term="error" scheme="http://www.howardliu.cn/categories/open-source/big-data/zookeeper/error/"/>
    
    
      <category term="大数据" scheme="http://www.howardliu.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="开源" scheme="http://www.howardliu.cn/tags/%E5%BC%80%E6%BA%90/"/>
    
      <category term="zookeeper" scheme="http://www.howardliu.cn/tags/zookeeper/"/>
    
      <category term="异常处理" scheme="http://www.howardliu.cn/tags/%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
</feed>
