<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>沉潜飞动</title>
  
  <subtitle>君子藏器于身，待时而动。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.howardliu.cn/"/>
  <updated>2020-07-24T13:30:11.000Z</updated>
  <id>https://www.howardliu.cn/</id>
  
  <author>
    <name>Howard Liu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>如何在足够规模团队中高效使用 Git 管理代码？</title>
    <link href="https://www.howardliu.cn/how-to-use-branch-efficiently-in-git/"/>
    <id>https://www.howardliu.cn/how-to-use-branch-efficiently-in-git/</id>
    <published>2020-07-22T13:38:11.000Z</published>
    <updated>2020-07-24T13:30:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>用了 Git 多年，优势和挑战都是深有体会。</p><p>话不多说，直接上问题：如何在足够规模团队中高效使用 Git 管理代码？</p><p>继续不多话，直接上答案：分支管理。</p><a id="more"></a><p>Git 的分支管理有很多实践，有些是从 SVN 类的集中式版本管理工具继承的，有些是根据 Git 自己的特性总结的，目前市面上比较有名的三种 Git 分支管理模型是：</p><ul><li>TrunkBased：主干在手，天下我有。所有代码都往主干上招呼，发版也只用主干。</li><li>GitFlow：严谨、规范、难用，主要是记不住该往哪个分支合并了。</li><li>AoneFlow：前两种都不行，那就借鉴各自的优点，达到阴阳平衡，中庸也。</li></ul><h2 id="TrunkBased"><a href="#TrunkBased" class="headerlink" title="TrunkBased"></a>TrunkBased</h2><p>TrunkBased，又叫主干开发，有一个网站专门介绍这种开发方式：<a href="https://trunkbaseddevelopment.com/" target="_blank" rel="noopener">Trunk Based Development</a>。</p><p>TrunkBased 是持续集成思想所崇尚的工作方式，它由单个主干分支和许多发布分支组成，每个发布分支在特定版本的提交点上从主干创建出来，用来进行上线部署和 Hotfix。在 TrunkBased 模式中，没有显性的特性分支。当然实际上 Git 的分布式特征天生允许每个人有本地分支，TrunkBased 也并非排斥短期的特性分支存在，只不过在说这种模式的时候，大家通常都不会明确强调它罢了。</p><p><img src="https://www.howardliu.cn/images/git/trunk-based.jpg" alt="TrunkBased"></p><p>使用主干开发后，我们的代码库原则上就只能有一个 Trunk 分支即 master 分支了，所有新功能的提交也都提交到 master 分支上，保证每次提交后 master 分支都是可随时发布的状态。没有了分支的代码隔离，测试和解决冲突都变得简单，持续集成也变得稳定了许多。</p><p>但是这种方案缺点也比较明显，如果大家都在主干进行开发，当代码提交合并时，将会异常痛苦，一不小心就会出现冲突。而且，这种因为这种方式没有明显的特性分支，想要移除已经上线的特性会变得非常困难。（如果你说把不要的功能注释，重新发版，那就当我什么也没说。）还有一种方案是引入特性开关，通过开关控制特性是否启用和关闭，但是增加开关就引入了复杂性，引入复杂性就引入了出 bug 的风险，毕竟多增加的每行代码都有可能是一个新的 bug。</p><h2 id="GitFlow"><a href="#GitFlow" class="headerlink" title="GitFlow"></a>GitFlow</h2><p>GitFlow 来源于 Vincent Driessen 提出的 <a href="https://nvie.com/posts/a-successful-git-branching-model/" target="_blank" rel="noopener">A successful Git branching model</a>，整体来说，是一套完善的版本管理流程。缺点就是太过严格，不太适合喜欢自由而且懒的程序猿。当然，在程序猿这种物种中，没有完美的解决方案，总有那么一小撮人会觉得不好。参考 Ant、Maven 和 Gradle。</p><p>先上图：</p><p><img src="https://www.howardliu.cn/images/git/gitflow.png" alt="GitFlow：A successful Git branching model"></p><p>GitFlow 常用分支：</p><ul><li>主干分支（<code>master</code>）：最近发布到生产环境代码的分支，这个分支只能从其他分支合并，不能再 Master 分支直接修改。</li><li>主开发分支（<code>develop</code>）：包含所有要发布到下一个 Release 版本的代码。可以在 Develop 直接开发，也可以将 Feature 的特性代码合并到 Develop 中。<br><img src="https://www.howardliu.cn/images/git/gitflow-develop.png" alt="主开发分支"></li><li>特性分支（<code>feature/*</code>）：功能项开发分支，以<code>feature/</code>开头命名分支，当功能项开发完成，将被合并到主开发分支进入下一个 Release，合并完分支后一般会删点这个特性分支。<br><img src="https://www.howardliu.cn/images/git/gitflow-feature.png" alt="特性分支"></li><li>发布分支（<code>release/*</code>）：基于主开发分支创建的一个发布分支，以<code>release/</code>开头命名分支，用于测试、bug 修复及上线。完成后，合并到主干分支和主开发分支，同时在主干分支上打个 Tag 记住 Release 版本号，然后可以删除发布分支。<br><img src="https://www.howardliu.cn/images/git/gitflow-release.png" alt="发布分支"></li><li>热修复分支（<code>hotfix/*</code>）：用于解决线上 Release 版本出现的 bug，以<code>hotfix/</code>开头命名分支，修复线上问题，完成后，合并到主干分支和主开发分支，同时在主干分支上打个 tag。<br><img src="https://www.howardliu.cn/images/git/gitflow-hotfix.png" alt="热修复分支"></li></ul><p>根据上面描述，GitFlow 是一套完整的从开发到生产的管理方式，但是各种分支来回切换及合并，很容易把人搞晕，所以用的人也是越来越少。</p><h2 id="AoneFlow"><a href="#AoneFlow" class="headerlink" title="AoneFlow"></a>AoneFlow</h2><p>AoneFlow 是阿里内部的一套版本管理模型，兼顾了 TrunkBased 易于持续集成和 GitFlow 易于管理需求的特点，又规避了 GitFlow 分支繁琐的缺点，也就是中庸。</p><p>AoneFlow 使用三个分支：主干分支（<code>master</code>）、特性分支（<code>feature/*</code>）、发布分支（<code>release/*</code>），以及三条原则：</p><p><strong>规则一，开始工作前，从主干分支创建特性分支。</strong></p><p>这条规则借鉴了 GitFlow，每当开始一件新的工作项（比如新的功能或是待解决的问题，可以是一个人完成，或是多个人协作完成）时，从代表最新已发布版本的主干分支上创建一个通常以<code>feature/</code>前缀命名的特性分支，然后在这个分支上提交代码修改。也就是说，每个工作项对应一个特性分支，所有的修改都不允许直接提交到主干。</p><p><img src="https://www.howardliu.cn/images/git/aoneflow-feature.jpg" alt="从主干创建特性分支"></p><p>特性分支不止承担了新功能，也是待解决问题的分支。对于我们团队，为了避免歧义，会将新功能以<code>feature/</code>为前缀，待解决问题以<code>hotfix/</code>为前缀，除了名称外，其他都按照规则一执行。</p><p><strong>规则二，通过合并特性分支，形成发布分支。</strong></p><p>GitFlow 先将已经完成的特性分支合并回主干分支和主开发分支，然后在主干分支上打 Tag 记录发布信息。TrunkBased 是等所有需要的特性都在主干分支上开发完成，然后从主干分支的特定位置拉出发布分支。而 AoneFlow 的思路是，从主干上拉出一条新分支，将所有本次要集成或发布的特性分支依次合并过去，从而得到发布分支，发布分支通常以<code>release/</code>前缀命名。</p><p><img src="https://www.howardliu.cn/images/git/aoneflow-release.jpg" alt="合并特性分支，形成发布分支"></p><p>我们可以将每条发布分支与具体的环境相对应，<code>release/test</code>对应部署测试环境，<code>release/prod</code>对应线上正式环境等，并与流水线工具相结合，串联各个环境上的代码质量扫描和自动化测试关卡，将产出的部署包直接发布到相应环境上。</p><p>另外，发布分支的特性组成是动态的，调整起来特别容易。在一些市场瞬息万变的互联网企业，以及采用“敏捷运作”的乙方企业经常会遇到这种情况，已经完成就等待上线的需求，随时可能由于市场策略调整或者甲方的一个临时决定，其中某个功能忽然要求延迟发布或者干脆不要了。再或者是某个特性在上线前发现存在严重的开发问题，需要排除。按往常的做法，这时候就要来手工“剔代码”了，将已经合并到开发分支或者主干分支的相关提交一个个剔除出去，做过的同学都知道很麻烦。在 AoneFlow 模式下，重建发布分支，只需要将原本的发布分支删掉，从主干拉出新的同名发布分支，再把需要保留的各特性分支合并过来就搞定，而且代码是干净的，没有包含不必要的特性。</p><p>此外，发布分支之间是松耦合的，这样就可以有多个集成环境分别进行不同的特性组合的集成测试，也能方便的管理各个特性进入到不同环境上部署的时机。松耦合并不代表没有相关性，由于测试环境、集成环境、预发布环境、灰度环境和线上正式环境等发布流程通常是顺序进行的，在流程上可以要求只有通过前一环境验证的特性，才能传递到下一个环境做部署，形成漏斗形的特性发布流。当然，这种玩法比较适合有完整开发集成平台的公司，小团队玩不转，比如我们团队就玩不动这种高级玩法。</p><p><strong>规则三，发布到线上正式环境后，合并相应的发布分支到主干，在主干添加 Tag，同时删除该发布分支关联的特性分支。</strong></p><p>当一条发布分支上的流水线完成了一次线上正式环境的部署，就意味着相应的功能真正的发布了，此时应该将这条发布分支合并到主干。为了避免在代码仓库里堆积大量历史上的特性分支，还应该清理掉已经上线部分特性分支。与 GitFlow 相似，主干分支上的最新版本始终与线上版本一致，如果要回溯历史版本，只需在主干分支上找到相应的版本标签即可。</p><p><img src="https://www.howardliu.cn/images/git/aoneflow-master.jpg" alt="合并相应的发布分支到主干，在主干添加 Tag，同时删除该发布分支关联的特性分支"></p><p>除了基本规则，还有一些实际操作中不成文的技巧。比如上线后的热修复，正常的处理方法应该是，创建一条新的发布分支，对应线上环境（相当于 Hotfix 分支），同时为这个分支创建临时流水线，以保障必要的发布前检查和冒烟测试能够自动执行。</p><h2 id="再啰嗦几句废话"><a href="#再啰嗦几句废话" class="headerlink" title="再啰嗦几句废话"></a>再啰嗦几句废话</h2><p>不管哪种方式，既然存在，都会有一定合理性。所以，不管最终翻了哪个牌子，不是因为这个好看，而是因为这个更适合自己。</p><hr><p>个人主页：<a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文：<a href="https://www.howardliu.cn/how-to-use-branch-efficiently-in-git/">如何在足够规模团队中高效使用 Git 管理代码？</a><br>CSDN 主页：<a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN 博文：<a href="https://blog.csdn.net/liuxinghao/article/details/103794938" target="_blank" rel="noopener">如何在足够规模团队中高效使用 Git 管理代码？</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;用了 Git 多年，优势和挑战都是深有体会。&lt;/p&gt;
&lt;p&gt;话不多说，直接上问题：如何在足够规模团队中高效使用 Git 管理代码？&lt;/p&gt;
&lt;p&gt;继续不多话，直接上答案：分支管理。&lt;/p&gt;
    
    </summary>
    
    
      <category term="microservice" scheme="https://www.howardliu.cn/categories/microservice/"/>
    
    
      <category term="微服务" scheme="https://www.howardliu.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="Git" scheme="https://www.howardliu.cn/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>IPv4 与 int 之间互相转换</title>
    <link href="https://www.howardliu.cn/convert-ipv4-to-int/"/>
    <id>https://www.howardliu.cn/convert-ipv4-to-int/</id>
    <published>2020-07-20T13:11:13.000Z</published>
    <updated>2020-07-20T13:11:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>目前数据存储设备越来越便宜，已经不存在为了节省空间而引入复杂计算的场景，但是有时候，将 IPv4 这种长字符串数据转换为数字，更加便于比较和查询。</p><a id="more"></a><h2 id="1-直接转换"><a href="#1-直接转换" class="headerlink" title="1. 直接转换"></a>1. 直接转换</h2><p>直接能够想到的方式是把网段之间的“.”去掉，组成一个长数字就行。但是在“255.255.255.255”地址时，去掉“.”的数字是255255255255，大于int能够表示的最大数字，所以，真正使用的时候，只能使用长整型Long型。但是这种方式有两个严重的问题：</p><ol><li>单向转换：只适合那种需要从 IPv4 到数字单向转换的场景，如果想要转回来，就不容易了，比如：“192.168.10.10”和“192.168.101.0”转换成数字都是1921681010，想要从数字转换会IP地址，就会有歧义，除非搭配某些约定规则才能实现</li><li>转换之后存在歧义：也是第一条中说的，两个IP地址转换得到同一个数字，如果在需要IP比较的场景，就容易出现问题了。</li></ol><h2 id="2-移位转换"><a href="#2-移位转换" class="headerlink" title="2. 移位转换"></a>2. 移位转换</h2><p>既然直接去掉“.”有歧义的问题，那就将4段数字分开存储，IPv4的某段数字取值范围是0~255，也就是2^8个数字，恰巧int类型占32个字节，那通过简单的移位和或运算，就能得到最后的结果，而且支持从int到IPv4的转换。</p><p>比如：192.168.30.68，计算结果就是：</p><pre><code class="java">192 &lt;&lt; 24 | 168 &lt;&lt; 16 | 30 &lt;&lt; 8 | 68 = 0b11000000101010000001111001000100 = -1062724028</code></pre><p>逆向转换就是：</p><pre><code class="java">((-1062724028 &gt;&gt; 24) &amp; 0xFF) + &quot;.&quot; + ((-1062724028 &gt;&gt; 16) &amp; 0xFF) + &quot;.&quot; + ((-1062724028 &gt;&gt; 8) &amp; 0xFF) + &quot;.&quot; + (-1062724028 &amp; 0xFF) = &quot;192.168.30.68&quot;</code></pre><p>所以第二种方式除了计算麻烦一些，可以完美解决第一种方式的两个问题。</p><hr><p>个人主页：<a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文：<a href="https://www.howardliu.cn/convert-ipv4-to-int">IPv4 与 int 之间互相转换</a><br>CSDN 主页：<a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN 博文：<a href="https://blog.csdn.net/liuxinghao/article/details/107475967" target="_blank" rel="noopener">IPv4 与 int 之间互相转换</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目前数据存储设备越来越便宜，已经不存在为了节省空间而引入复杂计算的场景，但是有时候，将 IPv4 这种长字符串数据转换为数字，更加便于比较和查询。&lt;/p&gt;
    
    </summary>
    
    
      <category term="java" scheme="https://www.howardliu.cn/categories/java/"/>
    
    
      <category term="java" scheme="https://www.howardliu.cn/tags/java/"/>
    
      <category term="ipv4" scheme="https://www.howardliu.cn/tags/ipv4/"/>
    
      <category term="int" scheme="https://www.howardliu.cn/tags/int/"/>
    
  </entry>
  
  <entry>
    <title>从单体架构到微服务架构</title>
    <link href="https://www.howardliu.cn/from-monolith-to-microservice/"/>
    <id>https://www.howardliu.cn/from-monolith-to-microservice/</id>
    <published>2020-03-08T06:54:20.000Z</published>
    <updated>2020-03-08T06:54:20.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://www.howardliu.cn/images/microservice/beautiful-demoiselle-4825548_1920.jpg" alt="从单体架构到微服务架构"></p><p>微服务的优势众多，在现在如果有谁没有听过微服务架构，可以从<a href="https://www.howardliu.cn/what-is-microservice/">这里</a>了解一下。本文主要聊一聊是否值得花时间将单体架构重构为微服务架构？</p><a id="more"></a><p>微服务架构是一种架构风格，专注于软件研发效能，主要包括单位时间内实现更多功能，或者软件从想法到上线的整个持续交付的过程。在当前的互联网环境中，业务变化迅速，也促使了微服务架构的普及。这种架构迫使团队迅速反应，快速实施，在方案没有过期之前已经上线运行，经受市场考察和考验。</p><p>目前国内大多数公司正在运行的系统都是单体架构系统，不可否认，这些系统在公司发展过程中，发挥了不可替代的作用，保障了公司正常运行，创造了很多价值。但是，随着系统的日渐膨胀，集成的功能越来越多，开发效率变得越来越低，一个功能从想法到实现，需要花费越来越长的时间。更严重的是，由于代码模块纠结在一起，很多已经老化的架构或者废弃的功能，已经成为新功能的阻碍。</p><p>众所周知，单体架构随着功能增多，不可避免的是研发效能的降低：研发周期变长、研发资源占用增多。从而引发的情况是：新员工培训时间增多、员工加班时间变长、员工要求涨薪或者跳槽。到了这种情况就说明，单体架构已经不能够满足企业发展需要，这个时候，需要升级架构来提升研发效能，比如微服务架构。</p><p>想要说明微服务架构的好处，可以来一个比喻。我们建了一个空间站，为此，我们需要将人、货物和设备运输到空间站中，这个时候，运载火箭是比较好的选择，尽管运载火箭造价也比较高，但是几个月发射一次，也能够满足需求。随着空间站的扩大，火箭发射的间隔变短，运输成本高的离谱，而且越来越没法满足空间站运转需求。这个时候，可以尝试另外一种方式，比如，太空电梯。当然太空电梯的造价成本高于一次飞行的费用，但是只要建成，以后的成本就降低了很多。</p><p>这个比喻也是说明了微服务带来的美好期望，同时也说明一个问题，实施微服务架构会带来巨大的投资。所以，我们在建造太空电梯之前需要想好，我们真的需要这种投入，否则只能是一种浪费。</p><h1 id="to-be-or-not-to-be"><a href="#to-be-or-not-to-be" class="headerlink" title="to be or not to be"></a>to be or not to be</h1><p>决定从单体架构升级为微服务架构时，先问问自己下面几个问题：</p><ul><li>产品或系统是否经过市场考验</li><li>是否需要超过一个团队来保证产品发布</li><li>系统是否对可靠性、可伸缩性有较高要求</li></ul><h1 id="微服务架构"><a href="#微服务架构" class="headerlink" title="微服务架构"></a>微服务架构</h1><p>什么是微服务架构呢？Sam Newman认为是：“一组围绕业务领域建模的、小而自治的、彼此协同工作的服务。”</p><p>微服务架构中的服务，是根据业务能力抽取的业务模块，独立开发和部署，但是需要彼此配合完成整个业务功能。服务不是单纯的数据存储组件，那是数据库。也不是单纯的逻辑函单元，那是函数。只有同时包括数据+逻辑，才是真正意义上的服务。</p><h1 id="服务边界"><a href="#服务边界" class="headerlink" title="服务边界"></a>服务边界</h1><p>服务拆解过程中，DDD（领域驱动设计）可以作为微服务架构的指导方针。因为微服务是围绕业务功能定义服务，根据服务定义团队，这与DDD将业务域拆解为业务子域、定义限定上下文的方法论如出一辙，于是DDD作为微服务的指导方针，快速定义各个服务组件，完成从单体架构到微服务架构的迁移。</p><p>Alberto Brandolini提出识别服务上下文的方式叫做“Event Storming”。第一步是识别业务域中发生的事件，也就是说，我们的关注点是行为，不是数据结构。这样做的好处是，系统中不同服务之间是松散耦合关系，而且单个服务能够自治。</p><p>定义好了服务边界，还需要定义事务边界。过去，我们的服务在一个进程中，后面挂着一个数据库，事务可以选择强一致性事务，也就是ACID。当服务增多，彼此配合，这个时候可以使用最终一致性事务，也就是BASE。不同于ACID，BASE更加灵活，只要数据在最终能够保持一致就可以了。这个最终的时间范围，根据不同的业务场景不同，可能是分钟、小时、天，甚至是周或者月。</p><h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><p>微服务架构愿景美好，属于重型武器，优点众多，缺点也很明显。服务增多，运维难度增大，错误调试难度增大。所以需要自动化构建、配置、测试和部署，需要日志收集、指标监控、调用链监控等工具，也就是需要DevOps实践。<a href="https://www.howardliu.cn/the-three-ways-principles-underpinning-devops/">实现DevOps的三步工作法</a>中说明了实现DevOps文化的三个步骤。</p><p>除了上面提到的基础，还需要在早期确定服务之间如何集成和彼此调用方式，还需要确定数据体系，包括事务一致性和数据可靠性方法。随着服务增多，还需要配置管理、服务发现等众多组件。具体需要的基础组件可以参考<a href="https://www.howardliu.cn/the-base-of-microservice/">微服务的基建工作</a>。</p><p>这些基础的服务和设计，最好在早期定义，否则，后期需要花费更多的资源才能够完善架构。如果前期缺失，后期也没有补足，造成的后果就是微服务架构迁移失败，最后的系统也只是披着微服务外衣的单体架构。</p><h1 id="进化还是革命？"><a href="#进化还是革命？" class="headerlink" title="进化还是革命？"></a>进化还是革命？</h1><p>定义好服务边界之后，还有一个问题需要解决：是逐步进化更新系统、还是破釜沉舟重构整个系统。</p><p>第二种方式很诱人，比较符合大多数程序猿的思维，系统不行，推倒重来，名为重构。但是在大多数情况下，这种方式不能被允许，因为市场变化迅速、竞争激烈，大多数公司不会停止业务，去等待重构一个能够运行、只是有些缺点的系统。所以，逐步提取更新系统才是王道，大多数公司也能接受。这种方式又被称为绞杀模式。</p><h1 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h1><p>该如何逐步过渡到微服务架构？下面一步步进行展示：</p><p><img src="https://www.howardliu.cn/images/microservice/from_monolith_to_microservice_0.png" alt="单体架构"></p><p>第一步，将用户视图层与服务层部分逻辑进行分离。业务逻辑委托给服务层，支持页面展示的查询定向到数据库。这个阶段，我们不修改数据库本身。</p><p><img src="https://www.howardliu.cn/images/microservice/from_monolith_to_microservice_1.png" alt="部分拆分视图和业务逻辑"></p><p>第二步，用户视图层与数据库完全分离，依赖于服务层操作数据库。</p><p><img src="https://www.howardliu.cn/images/microservice/from_monolith_to_microservice_2.png" alt="完全拆分视图和业务逻辑"></p><p>第三步，将用户视图层与服务层拆分为不同服务，并在服务层创建一个API层，用户视图层与服务层之间通信。</p><p><img src="https://www.howardliu.cn/images/microservice/from_monolith_to_microservice_3.png" alt="物理拆分视图和业务逻辑"></p><p>第四步，拆分数据库，将不同业务数据拆分到不同的数据库中，同时对应业务服务层拆分到不同的服务。用户视图层通过API网关与不同业务服务层的API组件通信。这个时候需要注意，如果团队没有微服务开发经验，可以首先抽取简单业务域服务，因为业务简单，实现简单，可以练手，积累经验。</p><p><img src="https://www.howardliu.cn/images/microservice/from_monolith_to_microservice_4.png" alt="业务服务层拆分、垂直拆分数据库"></p><p>最后一步，拆分用户视图层。</p><p><img src="https://www.howardliu.cn/images/microservice/from_monolith_to_microservice_5.png" alt="拆分用户视图层"></p><p>绞杀模式的优势就在于，我们可以随着业务变化随时调整方案，不会造成整个业务进化过程的停摆。</p><h1 id="成功标准"><a href="#成功标准" class="headerlink" title="成功标准"></a>成功标准</h1><p>当我们完成了整个升级过程，就需要检查一下我们是否达到了预期的结果。引入微服务的目的首先是改善开发流程，我们可以通过简单的指标来衡量：</p><ul><li>开发周期：从概念到上线持续的时间</li><li>开发效能：单位时间内团队或个人完成的功能或用户故事</li><li>系统可伸缩性</li><li>平均维修时间：查找和排除故障所需时间</li></ul><p>通过对比老架构和新架构的这些特性值，可以评估升级过程取得的效果。当然，升级过程中也要有这些指标的监控。</p><h1 id="最重要的事"><a href="#最重要的事" class="headerlink" title="最重要的事"></a>最重要的事</h1><p>作为攻城狮，我们为能够解决或改善周围世界而自豪，着迷于提供解决方案。同时，我们也要意识到，我们付出的每一份努力，都要有回报。如果不能带来任何回报的重构升级，都是浪费时间。</p><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/from-monolith-to-microservice/">从单体架构到微服务架构</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="https://blog.csdn.net/liuxinghao/article/details/105038453" target="_blank" rel="noopener">从单体架构到微服务架构</a></p>]]></content>
    
    <summary type="html">
    
      微服务架构是一种架构风格，专注于软件研发效能，主要包括单位时间内实现更多功能，或者软件从想法到上线的整个持续交付的过程。在当前的互联网环境中，业务变化迅速，也促使了微服务架构的普及。这种架构迫使团队迅速反应，快速实施，在方案没有过期之前已经上线运行，经受市场考察和考验。目前国内大多数公司正在运行的系统都是单体架构系统，不可否认，这些系统在公司发展过程中，发挥了不可替代的作用，保障了公司正常运行，创造了很多价值。但是，随着系统的日渐膨胀，集成的功能越来越多，开发效率变得越来越低，一个功能从想法到实现，需要花费越来越长的时间。更严重的是，由于代码模块纠结在一起，很多已经老化的架构或者废弃的功能，已经成为新功能的阻碍。
    
    </summary>
    
    
      <category term="microservice" scheme="https://www.howardliu.cn/categories/microservice/"/>
    
    
      <category term="微服务" scheme="https://www.howardliu.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="单体架构" scheme="https://www.howardliu.cn/tags/%E5%8D%95%E4%BD%93%E6%9E%B6%E6%9E%84/"/>
    
      <category term="重构" scheme="https://www.howardliu.cn/tags/%E9%87%8D%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>别让非理性思维毁了你的人生</title>
    <link href="https://www.howardliu.cn/dont-let-irrational-thinking-ruin-your-lift/"/>
    <id>https://www.howardliu.cn/dont-let-irrational-thinking-ruin-your-lift/</id>
    <published>2020-02-23T08:07:46.000Z</published>
    <updated>2020-02-23T08:07:46.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://www.howardliu.cn/images/quantum-physics.jpg" alt="别让非理性思维毁了你的人生" title="别让非理性思维毁了你的人生"></p><blockquote><p>本文是盖瑞·马库斯的《怪诞脑科学:战胜焦虑、混乱、拖延的自控术》一书读后感，借用作者给出的13条建议，结合自己的理解分享给大家。</p></blockquote><p>我们的身体的精密程度远超机器可以比拟，大脑神经元复杂程度远远超过世界上任何已存在的机器，但是我们却没有最简单的机器那样精准计算和准确无误的存储。</p><p>我们的记忆会向我们所想要的那个方向发生偏差，比如很多嫌疑人都认为自己犯的错有情可原。我们会偶尔断片或者忘记刚放下的一个小东西，所以我们会忘带钥匙、找不到手机，或者忘了锁门。</p><p>机器就不会发生这些，只要是存储的资料，没有修改绝对不会改变，只要是设定好的程序，一定会按照预期产生相同且一致的结果。非理性思维，是产生这种情况的“罪魁祸首”，同时也是我们有别于机器的重要原因。</p><p>德国化学家厄恩斯特·费希尔（Ernst Fischer）曾陷入这样的沉思：“随着机器变得越来越完美高效，于是这就变得很清楚了——人类的伟大之处就在于他们身上存在不完美的地方。”一个由工程师设计出来的生物可能永远都不懂什么是爱，永远都学不会欣赏艺术或理解诗歌。以动物的理性来看，把时间花在创造和欣赏艺术上，还不如把其用来收集坚果，为过冬做好准备呢。而从人类的角度，艺术是生活乐趣的一部分。</p><p>尽管我们有着机器或者动物所没有的优势，但是我们大脑中那些非理性的部分，让我们固执、偏执，存在认知偏差等一系列不好的因素，阻碍了我们个人乃至我们整个人类种族的发展，所以无论如何，我们都应该做的更好。盖瑞·马库斯给出的13条建议，每一条都是建立在谨慎的实证研究基础之上的，同时，也是我们可以习得的一些好的习惯，有可能改变我们生活中各种的不如意。</p><a id="more"></a><h2 id="1-尽可能考虑有无其他可行的选项"><a href="#1-尽可能考虑有无其他可行的选项" class="headerlink" title="1. 尽可能考虑有无其他可行的选项"></a>1. 尽可能考虑有无其他可行的选项</h2><p>“一条道走到黑”、“不撞南墙不回头，不到黄河心不死”，有些时候这些行为不能证明我们多么的坚定，只能说明我们比较冲动。即使我们义无反顾的走到最后，也不一定能够得到我们所期望的结果，甚至我们会在“黑道”里面迷失，在“南墙”上撞得头破血流，在“黄河”的漩涡里沉底。</p><p>我们专注于一件事情是好事，不好的是我们只关注这件事情，这就是我们常说的钻牛角尖。</p><p>我们需要对自己最关注的事物之外的其他想法和可能性，投入的更多关注。在做出正确抉择之前，我们往往需要把不会采取的途径和最终选用的途径都加以考虑才行。</p><p>有可能，最终我们还是要走这条路，但是因为我们考虑过其他的情况，可能就从其他情况中找到一只手电筒，那这条路就更加清晰了。</p><h2 id="2-重新界定问题"><a href="#2-重新界定问题" class="headerlink" title="2. 重新界定问题"></a>2. 重新界定问题</h2><p>我们生活中常常会被别人误导。</p><p>一句话，不同的说法，可能会产生完全不一样的效果，比如，“我们还有10%的问题没有解决”和“我们已经解决了90%的问题”，或者“每满100减20”和“打8折”。</p><p>一件事，从不同的角度，会有不一样的想法，比如，“死刑是惩罚凡有重大过错的罪犯”和”死刑是为了警告那些可能犯重大过错的人“。</p><p>所以，当有人告诉你一个问题的时候，看能不能换个角度，重新界定问题。不一样的角度，世界可能就不同了。</p><h2 id="3-始终牢记：相关关系不等于因果关系"><a href="#3-始终牢记：相关关系不等于因果关系" class="headerlink" title="3. 始终牢记：相关关系不等于因果关系"></a>3. 始终牢记：相关关系不等于因果关系</h2><p>一件事情发生，另外一件事情紧跟着也发生，但是两件事可能没有因果关系。</p><p>《傻鹅皮杜妮》是一本绘本，皮杜妮一开始认为“有书有爱书的人就有智慧”，于是天天带着捡来的书，越来越骄傲，给农场的动物们造成了各种麻烦，直到最后酿成大错，才领悟到，虽然她每天带着书，爱惜书，但是没有读过书，不可能会有智慧。</p><p>跳水运动员身材都是那种很完美的三角形身材，跳水运动员与三角形身材有很高的相关性，但不会因为有三角形身材就能够称为跳水运动员，那需要经过日复一日的辛苦训练习得的。</p><p>那些成功人士都有一项比较擅长的体育运动，成功人士与体育运动有很高的相关性，但不会因为一个人擅长某项体育运动，就能够称为成功人士。</p><p>我们很容易将相关关系和因果关系混为一谈，忽略一件事情的本质，也就会影响到我们最终做的决定。</p><h2 id="4-用于别忘了控制样本的大小"><a href="#4-用于别忘了控制样本的大小" class="headerlink" title="4. 用于别忘了控制样本的大小"></a>4. 用于别忘了控制样本的大小</h2><p>任何单一事件都可能是随机的，但同一模式的反复出现就不大可能是一个偶发事件了。从数学上考虑，样本数量越大，统计结果就越准确。</p><p>一种新型药物从研发到上市，需要经过漫长的临床试验期，尽可能多的在动物和人身上测试使用效果。没有任何一种药物，在一个人身上其效果，就认为这种药物对所有人都有效。</p><p>所以我们做的任何决定，都不要被少数人的结论而左右。你身边有10个人玩股票赚钱了，如果你就认为股票一定赚钱，那极有可能会倾家荡产。</p><p>如果你的同学同事都是大学生，你就认为咱们国家的大学生比率已经很高了，那就大错特错。我国大学生比例9.5%，即使将范围限制在21岁这个范围，大学生比例也不过才28.2%（如果不含大专是13%）。</p><p>所以，不以大量样本为基础得出的结论，都是耍流氓。</p><h2 id="5-预知自己的冲动并事先约束"><a href="#5-预知自己的冲动并事先约束" class="headerlink" title="5. 预知自己的冲动并事先约束"></a>5. 预知自己的冲动并事先约束</h2><p>相信大家都有过冲动消费，比如双十一，因为有满减、红包、返现等各种优惠，就买了一堆没用甚至只用一次的东西。我们饥饿的时候，通常会购买高热量的食物。我们减肥过程中，刚完成一次高运动量的健身，很有可能会吃的比平时多。这些都是冲动。</p><p>我们可以在看到优惠之前，想好自己要购买的商品；感到饥饿，计划好自己晚餐要吃什么，然后严格按照计划执行。</p><p>离我们最近的还有抖音或快手这些小视频APP，每个小视频1分钟以内，如果不加以控制，几个小时就一晃没了。我们可以设置一个青少年锁，到了一个小时，会弹出密码输入解密，这个时候关掉抖音，该干嘛就去干嘛，别等着时间过去之后，再唉声叹气的后悔什么都没做。</p><h2 id="6-别只是设定目标，要制定应变方案"><a href="#6-别只是设定目标，要制定应变方案" class="headerlink" title="6. 别只是设定目标，要制定应变方案"></a>6. 别只是设定目标，要制定应变方案</h2><p>仅有目标不能保证目标的效果，得在理性的那一刻制定理性的方案，才有可能更理性的执行。</p><p>在很多时候，人们都几乎不可能完成一个表达模糊的目标，比如“我要减肥”。喊着这句话减肥的人，都是在喊完之后，该吃吃，该喝喝。设置是“我要减1斤”，这种目标更加明确的口号，也不能保证一定能够实现，我们需要制定更加详细严格的计划。</p><p>心理学家彼得·葛尔韦泽（Peter Gollwitzer）的研究表明：要是给希望实现的目标制订细致的应变方案，采取“如果X，就Y”的形式，就能极大地提高成功的概率。还是减肥这件事，我们的计划是，如果看到薯条，就走开；如果主食超过半碗米饭，就要多跑1000米；如果聚餐时候喝啤酒了，就要多运动HIIT半小时。。。</p><blockquote><p>根据这条建议，也是找到自己一直减肥失败的原因了。</p></blockquote><h2 id="7-在任何时候，如果你已经疲惫或心里还在考虑其他事情，就尽可能不去做重要决定"><a href="#7-在任何时候，如果你已经疲惫或心里还在考虑其他事情，就尽可能不去做重要决定" class="headerlink" title="7. 在任何时候，如果你已经疲惫或心里还在考虑其他事情，就尽可能不去做重要决定"></a>7. 在任何时候，如果你已经疲惫或心里还在考虑其他事情，就尽可能不去做重要决定</h2><p>身心疲惫（或精神涣散）之时进行思考，这和醉酒之后开车没有多大的差别。因为疲惫之后（或精神涣散时），我们更多是依赖我们的反射系统，而非慎思系统。</p><p>几乎所有大额的交易都是在最疲劳的时候，大型商务谈判都是采用疲劳战，长时间谈判，谈到最后总会有人崩溃，谁崩溃谁就会让步。能够进行大型商务谈判的，都是有强健体魄和坚定意志的。</p><h2 id="8-随时在收益和代价之间进行权衡比较"><a href="#8-随时在收益和代价之间进行权衡比较" class="headerlink" title="8. 随时在收益和代价之间进行权衡比较"></a>8. 随时在收益和代价之间进行权衡比较</h2><p>这一条看似是连小孩都懂的道理，实际上却需要我们认真思考。</p><p>举个例子，点外卖的时候，经常会碰到满减优惠，满减和商品定价，都是商家经过计算设置的，选择了合适的量之后，发现再加8块，就能多减7块，还能多一份沙拉。理性的讲，没有这份沙拉，你也可以吃饱，而且不用额外多花1块钱。但是，很多人会因为多的这份沙拉，多花一块钱。这个也算是前面讲到的冲动，放在这一条里面，就是没有权衡收益和代价。</p><p>做出合理判断的前提显然是权衡利弊，然而除非我们提高警惕，否则我们的性格和情绪往往会成为理性判断的绊脚石。</p><h2 id="9-设想你的决定可能会被抽查"><a href="#9-设想你的决定可能会被抽查" class="headerlink" title="9. 设想你的决定可能会被抽查"></a>9. 设想你的决定可能会被抽查</h2><p>老话说：“举头三尺有神明”。有时候，我们做的决定受非理性思维影响，是因为我们不需要为这个决定负责任。假设，我们经常设想，如果有人看着我们，我们做的决定还是这个吗？</p><p>根据研究，如果我们需要跟别人解释我们的理由，我们就会更加公正客观的做决定。也就是，当我们需要对决定负责任时，我们就会投入更多的精力，进行更周到的研究，做更精细的调查，然后做出更加负责的决策。</p><h2 id="10-和自己保持距离"><a href="#10-和自己保持距离" class="headerlink" title="10. 和自己保持距离"></a>10. 和自己保持距离</h2><blockquote><p>写到这一条，突然发现，这里很多的建议都有益于减少冲动消费。</p></blockquote><p>我们购物的时候，看到一件东西在打折，而且折扣很诱人，我们就会设想，在将来，我们会用到这件东西，到时候再买，就得多花xxx，于是就买了，然后，这件东西的归宿要么是吃灰，要么是丢弃。</p><p>我们的大脑对于远近的考虑机制几乎是以完全不同的方式建立起来的：（对大脑而言）近，是明确具体的概念；而远，则是抽象的概念。不是任何时候以抽象的概念来考虑问题都会得到更好的结果，因为我们会对未来设定很多设想。</p><p>在任何时候，我们都应当问问自己，未来的我会对现在的这个决定产生什么感受？认识到我们的处理方式在此时此刻和将来会有所不同，并尽量平衡和利用好即时和远期两种思维模式。这样一来，我们就不会因为把选择完全建立在即时所想的基础上而深受其害。</p><p>研究表明，非理性行为会随着时间流逝而淡化，越是复杂的决定，越需要有足够时间斟酌。当意识到我们冲动了，只要一个方法：明天再说。</p><h2 id="11-要当心生动化、个性化和逸闻趣事"><a href="#11-要当心生动化、个性化和逸闻趣事" class="headerlink" title="11. 要当心生动化、个性化和逸闻趣事"></a>11. 要当心生动化、个性化和逸闻趣事</h2><p>我们大脑会被生动个性的东西所吸引，这是源于我们的祖先，对鲜艳的东西敏感，可以更快的找到成熟的果实。于是这个基因一直延续到现在，我们对于鲜艳的、生动的、个性化的等一系列不那么死气沉沉的东西更加敏感，更加信任。</p><p>蒂莫西·威尔逊做的关于在校大学生和避孕套品牌的研究，得出了一个经典结论，即“照我说的做，而不是照我做的做”。在实验中，受试者得到两个消息来源：一个来自《消费者报告》上数据充分的结论，它推荐的是避孕套A；另一个则来自一则逸闻趣事（据说是另一个学生写的），它推荐的是避孕套B，理由是有人使用避孕套A在“嘿咻”过程中不慎破裂，从而让当事人极为焦虑，担心自己可能怀孕。几乎所有大学生原则上都认为《消费者报告》上的结论更可靠，并希望自己的朋友在挑选避孕套时不要受那个逸闻趣事的影响。但当问及他们自己的选择时，差不多有1/3（31%）的大学生仍然相信了那则生动的逸闻趣事，从而选择了避孕套B。</p><p>换句话说，越是吸引你的，越应该小心，比如广告。所谓的蛊惑人心，不外如是。</p><p>我们需要做的就是，通过对客观且科学的事物加以特别关注，来克服我们易受生动性影响的弱点。</p><h2 id="12-挑选重点"><a href="#12-挑选重点" class="headerlink" title="12. 挑选重点"></a>12. 挑选重点</h2><p>我们大脑做复杂决定的时候，需要消耗大量能量，这也是为什么我们做重大决策之后，会感觉的疲惫。所以，为了减少这种消耗，把能量留给更需要的事情。</p><p>《布里丹之驴》（Buridan’s Ass）的故事：驴子虽然面临着两堆同样远近、同样诱人的干草，最后却活活饿死了。显然，吃哪堆干草，根本不是需要做复杂决定的事情，随便选一堆，吃完再吃另外一堆。</p><p>小事随性，大事随心。</p><h2 id="13-尽量理性"><a href="#13-尽量理性" class="headerlink" title="13. 尽量理性"></a>13. 尽量理性</h2><p>这条建议基本上起的是告诫作用，属于咒语系。告诫自己要理性就能产生作用的重要原因之一在于：你这样做的时候，就能够自动启发自己在生活实践当中使用前面曾经介绍过的种种技巧。仅仅告诉自己要理性可能还不够，但这样做的同时，再配合使用其他技巧，就能看到这方面的效果了。</p><blockquote><p>想要完整的《怪诞脑科学:战胜焦虑、混乱、拖延的自控术》，掌控自己的大脑，可以搜索微信公众号 kanshanshuo，回复“怪诞”，获取电子书。</p></blockquote><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/dont-let-irrational-thinking-ruin-your-lift/">别让非理性思维毁了你的人生</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="https://blog.csdn.net/liuxinghao/article/details/104469326" target="_blank" rel="noopener">别让非理性思维毁了你的人生</a></p>]]></content>
    
    <summary type="html">
    
      尽可能考虑有无其他可行的选项；重新界定问题；始终牢记：相关关系不等于因果关系；用于别忘了控制样本的大小；预知自己的冲动并事先约束；别只是设定目标，要制定应变方案；在任何时候，如果你已经疲惫或心里还在考虑其他事情，就尽可能不去做重要决定；随时在收益和代价之间进行权衡比较；设想你的决定可能会被抽查；和自己保持距离；要当心生动化、个性化和逸闻趣事；挑选重点；尽量理性。
    
    </summary>
    
    
      <category term="闲聊" scheme="https://www.howardliu.cn/categories/%E9%97%B2%E8%81%8A/"/>
    
    
      <category term="闲聊" scheme="https://www.howardliu.cn/tags/%E9%97%B2%E8%81%8A/"/>
    
      <category term="非理性" scheme="https://www.howardliu.cn/tags/%E9%9D%9E%E7%90%86%E6%80%A7/"/>
    
      <category term="认知偏差" scheme="https://www.howardliu.cn/tags/%E8%AE%A4%E7%9F%A5%E5%81%8F%E5%B7%AE/"/>
    
      <category term="自控术" scheme="https://www.howardliu.cn/tags/%E8%87%AA%E6%8E%A7%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title>实现DevOps的三步工作法</title>
    <link href="https://www.howardliu.cn/the-three-ways-principles-underpinning-devops/"/>
    <id>https://www.howardliu.cn/the-three-ways-principles-underpinning-devops/</id>
    <published>2020-02-04T02:29:33.000Z</published>
    <updated>2020-02-04T02:29:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>《凤凰项目-一个IT运维的传奇故事》是一本比较神奇的书，用讲故事的方式，展现了IT团队（开发、测试、运维）在开发效能低、系统交付慢的情况下，通过实践三步工作法，在团队中实现加快系统交付、提升开发效能，使团队走上DevOps之路。而且本书有一个值得称道的地方是，通过类比制造业的工作流程，可以直观发现技术团队工作过程中隐藏的问题。</p><p>这里需要提醒一下开发人员，看书的时候一定要佛系，因为这个故事是以运维角度展开的，有一些大骂开发的情节。如果是想找具体的DevOps工具的，建议不要看了，里面没有具体的工具介绍，是以最朴素的方式，讲述DevOps的优势和实践。</p><p>先说一下概念：</p><ul><li>价值流：一个组织基于客户的需求所执行的一系列有序的交付活动。或者，为了给客户设计、生产和提供产品或服务所需从事的一系列活动，它包含了信息流和物料流的双重价值。</li><li>技术价值流：把业务构想转化为向客户交付价值的、有技术驱动的服务所需要的流程。流程的输入是需求，由开发部门完成开发，进行整体测试，部署到生产环境正常运行，并为客户提供服务，以产生价值。</li><li>前置时间：从需求确认（开发接收需求）开始计时，到工作完成时结束</li><li>处理时间：从实际开始处理工作开始计时，到工作完成结束</li><li>等待时间：从需求确认（开发接收需求）开始计时，到实际开始处理工作时结束</li><li>在制品/半成品：价值流里没有彻底完成的工作、处于队列中的工作。部分完成的工作会逐渐过期，随着时间推移到最终失去价值。</li><li>约束点：价值流中的瓶颈，即整个价值流流速的上限点。</li></ul><a id="more"></a><h1 id="第一步：流动原则"><a href="#第一步：流动原则" class="headerlink" title="第一步：流动原则"></a>第一步：流动原则</h1><p><img src="https://www.howardliu.cn/images/devops/3-ways-first-way.png" alt="first way"></p><p>第一步流动原则是为了打通技术价值流通道，实现开发到运维的工作快速从左到右流动。通过加速技术价值流的流速，缩短满足客户需求的前置时间，提高工作质量和产量，并使企业具有更强的竞争力。相关实践包括：持续构建、集成、测试和部署，按需搭建环境，限制在制品数量，构建能够安全实施变更的系统和组织。</p><p>通过持续加强工作内容的可视化，减小批次大小和等待间隔，内建质量以防止缺陷向下游传递，</p><h2 id="使工作可视"><a href="#使工作可视" class="headerlink" title="使工作可视"></a>使工作可视</h2><p>在制造业，原料或半成品的堆积、订单积压都是直观可见，产生阻塞的地方，就是约束点。但是在技术价值流中，很多问题是隐藏的，没有办法很明显的看到阻塞和约束点。同时，因为信息的不可见或者彼此信息不全，可能将问题传递到下一环节，甚至上线时才出现问题，或者根本没法交付。这就需要尽可能的将工作可视化，用来识别工作在哪里流动、排队、停滞。</p><p>一般，可以使用Kanban管理（来自日语，就是看板）或Sprint计划板作为工具。</p><p><img src="https://www.howardliu.cn/images/devops/kanban.png" alt="Kanban管理"></p><p>可视化管理中有一个需要注意的地方，要着眼全局目标，而不是局部目标。全局目标是增加系统质量，提升开发效能，局部目标是开发的完成率、测试的缺陷数、系统的可用性等。不是说局部目标不重要，这些局部目标需要其他方式来优化，我们现在需要提升整体的效率，一旦我们陷入细节中，就是一叶障目不见泰山，没有办法把握全局了。也就是吉恩•金所说的“不允许局部优化造成整体性能下降”。</p><h2 id="限制在制品数"><a href="#限制在制品数" class="headerlink" title="限制在制品数"></a>限制在制品数</h2><p>工作可视化之后，就可以开始有素质的找茬了。</p><p>第一步，限制并行任务。为什么？因为如果有并行任务，我们就需要花时间切换任务。有一种说法，如果同时进行两个任务，会有20%的时间用于切换任务，比如理清思路、进入状态、恢复工作环境等。如果是三个，那就会有40%的时间用于切换任务。并行任务越多，用于切换任务所花费的时间越多，造成的人力浪费越多。并行任务减少之后，浪费的时间减少，花费在工作上的时间就增加了，整体的交付效率也相应的提升。如果是用的是看板管理，可以通过限制每一列或每个工作中心在制品（并行任务）数量，并把卡片数量的上限标记在每一列上。</p><h2 id="减小批量大小"><a href="#减小批量大小" class="headerlink" title="减小批量大小"></a>减小批量大小</h2><p>这个就是敏捷中提倡的小步快跑，先交付，先尝试，就可以先试错，先改错，有问题可以及早暴露，不至于最后集成一个大疙瘩，无力回天。</p><p>这里不得不提持续部署，相信很多团队在使用持续部署工具，比如Jenkins，代码提交之后，触发Jenkins工作流，开始进行编译、测试、部署和发布。我们要做的就是小批次的提交代码，这部分代码被编译、测试，如果有问题，能够尽早发现，如果没有问题，经过测试发布到正式环境，就能够及早的呈现给客户。</p><p><img src="https://www.howardliu.cn/images/devops/large-batches-single-piece-flow.png" alt="减小批量大小"></p><h2 id="减少交接次数"><a href="#减少交接次数" class="headerlink" title="减少交接次数"></a>减少交接次数</h2><p>在传统的IT团队中，代码从开发完成到部署上线需要经过N多个部门，每个部门有自己的KPI，有自己的任务排期，不同部门沟通需要成本，工单审批需要时间，这样就造成了交付时间的延长。另外，不同部门对于一个功能的认知有自己的认知陷阱，开发认为理所当然的时期，运维可能根本不了解。信息的隔离，可能造成一些已知的缺陷没有及时传递到下游，出现各种返工的情况。</p><p>为了减少这种交接，可以引入自动化方式完成大部分的操作，或者调整架构，让团队尽量少的依赖于其他人。</p><p><img src="https://www.howardliu.cn/images/devops/devops-self-service-operations.png" alt="减少交接次数"></p><h2 id="持续识别和改善约束点"><a href="#持续识别和改善约束点" class="headerlink" title="持续识别和改善约束点"></a>持续识别和改善约束点</h2><p>约束点就是瓶颈，如果整个团队中存在约束点，交付工作流就会有瓶颈。随着工作的优化，约束点之前的工作会堆积到约束点，而约束点之后的角色因为任务还没有到达，可能出现等待的情况。想要提升整体的效能，必须找到约束点，并进行拓宽，才能增加任务的吞吐量，任何不针对约束点进行的优化都是假象。</p><p>一般按照下面的步骤拓宽约束：</p><ol><li>识别约束点，任务队列最长的角色，就是约束点；</li><li>根据找到的约束点，找到拓宽约束点的方式；</li><li>根据2中的决定，考虑全局工作；</li><li>改善系统的约束点；</li><li>如果约束已经拓宽，整个工作流中会出现新的约束点，重复上面的步骤。</li></ol><h2 id="消除价值流中的困境和浪费"><a href="#消除价值流中的困境和浪费" class="headerlink" title="消除价值流中的困境和浪费"></a>消除价值流中的困境和浪费</h2><p>想要提升交付效率，除了开源，还需要节流。减少任何超出客户需求和他们愿意支付范围的任何材料和资源：</p><ul><li>半成品：价值流里没有彻底完成的工作、处于队列中的工作。部分完成的工作会逐渐过期，随着时间推移到最终失去价值。比如没有确认的需求、等待评审的变更。</li><li>额外工序：在交付过程中执行的、并未给客户增值的额外工作。比如对下游没有使用过的文档，或者对输出结果没有增值的评审。</li><li>额外功能：在交付过程中构建那些组织和客户完全不需要的功能，还没有到镀金阶段，却要浪费时间在镀金上，镀金的功能会增加功能测试和管理的复杂度和工作量。</li><li>任务切换：将人员分配到多个项目或价值流中，因为会出现任务切换，会在价值流中耗费额外的工作流和时间。</li><li>等待：由于资源竞争产生的等待，这将增加周期时间，延迟向客户交付。比如等待其他部门配合。</li><li>移动：信息或数据在工作中心之间移动的工作量。比如对于那种需要频繁沟通的人员不在一地办公，人员移动就产生浪费了。或者工作交接也会产生移动浪费，需要额外沟通。</li><li>缺陷：由于信息、材料或产品的错误、残缺或模糊，需要一定的工作量来确认。缺陷的产生和被检测出来的时间间隔越长，解决问题就越困难。</li><li>非标准或手工操作：需要依赖其他人的非标准或手动的工作，比如手动部署系统</li><li>填坑侠：为了实现组织目标，不得不把有些人和团队置于不太合理的处境。</li></ul><p>只有解决了上面的八种浪费，系统的改进，减轻或消除这些负担，实现快速流动的目标。</p><h1 id="第二步：反馈原则"><a href="#第二步：反馈原则" class="headerlink" title="第二步：反馈原则"></a>第二步：反馈原则</h1><p><img src="https://www.howardliu.cn/images/devops/3-ways-second-way.png" alt="second way"></p><p>第一步工作法是为了使工作能够在价值流能够从左向右流动，第二步工作法是创建从右到左的每个阶段能够快速、持续获得工作反馈的机制。该方法通过放大反馈环防止问题复发，并能够缩短问题检测周期，实现快速修复故障。我们的目标是从源头控制质量，并在流程中嵌入相关知识；创造更安全的工作系统，在故障或事故发生前检测到并解决它；最终建立安全和可靠的工作系统。</p><p>一般来说，发现和纠正问题最好的时机是发生故障时，只有发现问题，才能够解决问题。通过在整个工作流和组织中建立高质量的反馈机制，就可以在规模比较小、成本比较低的情况下修复系统。在灾难发生前消除问题，并创造出组织性学习氛围。</p><h2 id="在复杂系统中安全地工作"><a href="#在复杂系统中安全地工作" class="headerlink" title="在复杂系统中安全地工作"></a>在复杂系统中安全地工作</h2><p>复杂系统的一个重要特征是无法将系统视为一个整体，系统中的各个组件之间通常是紧耦合且紧密关联的，不能仅仅依据组件的行为解释系统的行为。复杂系统中故障存在且不可避免，所以我们需要设计一个安全的工作系统，可以让工程师们在系统中无所畏惧的开展工作，也就是各种折腾，这样才能在灾难发生前，快速检测出错误。可以采取下面4项措施让负载系统更加安全：</p><ul><li>管理复杂的工作，从中识别设计和操作的问题</li><li>群策群力解决问题，从而快速构建新知识</li><li>在整个组织中，将区域性的新知识应用到全局范围</li><li>领导者要持续培养有以上才能的人</li></ul><h2 id="及时发现问题"><a href="#及时发现问题" class="headerlink" title="及时发现问题"></a>及时发现问题</h2><p>想要及时发现问题，一般有两种做法：被动等待、主动试错。</p><p>通常，我们会搭建监控系统、设置多维度指标，对系统进行监控，当系统发生故障时，相关人员会收到报警信息，针对报警信息开始定位解决问题。这种方式属于被动等待的做法，因为要等待故障发生，故障发生的时机不可控，可能发生在上班的时候，更有可能发生在晚上睡觉时、周末休息时、休假旅游时，还有的会在结婚交拜的时，但是这种方式又不能没有，被动等待所搭建的监控系统是主动试错的基础。</p><p>主动试错就是在安全的工作系统中，不断对设计和假设进行验证，这种方式两个关键词是主动、安全。如果我们验证过程中把生产系统弄瘫了，那就笑话了。这样做的目标是更早、更快、以最低的成本、从尽可能多的维度增加系统的信息流，并尽可能清晰的确定问题的前因后果。能排除的假设越多，定位和解决问题的速度就越快。同时，这个过程也是练兵的过程，能很好的学习和创新。</p><h2 id="群策群力，战胜问题获取新知"><a href="#群策群力，战胜问题获取新知" class="headerlink" title="群策群力，战胜问题获取新知"></a>群策群力，战胜问题获取新知</h2><p>这个是承接“及时发现问题”的，因为发现问题之后，需要解决问题，我们需要发动所有相关人员，群策群力，解决问题。出现问题的时候，最忌讳的是绕开问题或者用“时间不够”这类理由搪塞。我们要做的是不惜全面停产，也要把问题解决。</p><p>至于为什么要相关人员都参与到解决问题中，理由如下：</p><ul><li>相关人员参与定位和处理问题，能够让大家更深入的理解系统，把无法规避、早期无知阶段变成学习的过程。</li><li>能够防止把问题带入下游环节，一旦进入下游环节，修复成本和工作量将呈指数增加，还会欠下技术债</li><li>阻止启动新的工作，问题不解决，就开始新的功能，就会引入新的问题</li><li>不解决问题，故障就会再次发生，修复成本更高</li></ul><p><img src="https://www.howardliu.cn/images/devops/pdca.jpg" alt="PDCA环"></p><h2 id="在源头保障质量"><a href="#在源头保障质量" class="headerlink" title="在源头保障质量"></a>在源头保障质量</h2><p>这点主要是针对QA和开发两个部门，有点类似国家政策：“谁污染谁治理”。在日常工作中，我们需要价值流中的每个人在他们的控制领域内发现并解决问题，通过这种方式，可以把质量控制、安全责任和决策制定都置于开展工作的场景里，而不是依赖于外围高层管理者的审批。</p><p>比如开发人员开发过程中，可以使用自动化测试，不依赖于测试团队，这样，开发人员就能够在任何需要的时候快速测试自己的代码，经过完善的自动化测试，就可以把代码部署到正式环境中。这样，自己对自己负责，同时也是对他人负责。</p><h2 id="为下游工作进行优化"><a href="#为下游工作进行优化" class="headerlink" title="为下游工作进行优化"></a>为下游工作进行优化</h2><p>这点负责的是精益原则：我们最重要的客户是我们的下游，为下游优化我们的工作，需要我们对他们有同理心，更好的识别可能阻碍快速平滑流动的设计问题。比如开发需要为运维优化自己的工作，比如架构、性能、稳定性、可测试性、可配置性、安全性等一系列特性，这些优化工作，和给客户提供功能同样重要。</p><h1 id="第三步：持续学习与实验原则"><a href="#第三步：持续学习与实验原则" class="headerlink" title="第三步：持续学习与实验原则"></a>第三步：持续学习与实验原则</h1><p><img src="https://www.howardliu.cn/images/devops/3-ways-third-way.png" alt="third way"></p><p>第一步建立从左到右的工作流，第二步建立从右到左的反馈机制，第三步就是要建立持续学习与实验的文化，通过提升个人技能，进而转化为团队和组织的财富。</p><p>这一步的核心是建立高度信任的文化，这种文化强调每个人都是持续学习者，在日常工作中，主动承担风险；通过安全的方法改进工作和开发产品，从成功或失败中积累经验，从而识别有价值的想法，摒弃无价值的想法。个人的努力带动整体的进化，帮助整个团队尝试和实践新技术、新方法。</p><p>必要的做法包括营造一种勇于创新、敢于冒险（相对于畏惧或盲目服从命令）以及高信任度（相对于低信任度和命令控制）的文化，把至少20%的开发和IT运维周期划拨给非功能性需求，并且不断鼓励进行改进</p><h2 id="建立学习型组织和安全文化"><a href="#建立学习型组织和安全文化" class="headerlink" title="建立学习型组织和安全文化"></a>建立学习型组织和安全文化</h2><p>在复杂系统中，精确预测出结果是不现实的。也就是说，无论我们怎么小心，故障总是会发生。</p><p>Westrum模型提出组织文化的三种类型：</p><ul><li>病态型组织的特点是大量的恐惧和威胁，倾向于隐藏失败。</li><li>官僚型组织的特点是严格的规则和流程，每个部门各扫门前雪，在这种组织中，通过评判系统处理事故，采用恩威并施的手段。</li><li>生机型组织是积极探索和分享信息，在这种组织中，整个团队所有员工共同承担责任，对事故积极反思并找到根本原因。</li></ul><p>第三步推崇的就是生机型组织，在故障发生时，团队关注的是如何设计安全的系统，防止事故复发，而不是追究人的问题。正如Etsy的工程师拜塞尼•马克里说的：“不指责，就没有恐惧；没有恐惧，就能够坦诚；坦诚能够有效的预防事故。”</p><h2 id="将日常工作的改进制度化"><a href="#将日常工作的改进制度化" class="headerlink" title="将日常工作的改进制度化"></a>将日常工作的改进制度化</h2><p>在技术价值流中，为了防止灾难性事故的发生，团队会陷入实施各种临时解决方案的工作中，这样就没有时间去完成那些有价值的工作。所以，用临时方案解决问题的模式，会导致问题和技术债务的累积。所以，我们需要在日常工作中留出时间来改善日常工作，比如偿还技术债务、修复bug、重构和优化代码等，这就要求我们的团队在开发间歇中预留一段时间，可以让团队成员解决问题。一件事情的果总会是另一件事情的因。我们在把日常问题解决了，有助于发现和解决潜在风险，或者有更多的精力去做更多有意义的事情。</p><h2 id="把局部发现转化为全局优化"><a href="#把局部发现转化为全局优化" class="headerlink" title="把局部发现转化为全局优化"></a>把局部发现转化为全局优化</h2><p>局部发现转化为全局优化就是说要在团队中做到先富带动后富。单个团队或个人获得了某种独有的知识或经验，应该把这种隐性知识（难以通过文档或沟通方式传递的知识）转换为显性知识，建立全局知识库，形成集体智慧。当其他人做类似的工作时，只需要在知识库中搜索，就能够很快找到前辈的经验。</p><h2 id="在日常工作中注入弹性模式"><a href="#在日常工作中注入弹性模式" class="headerlink" title="在日常工作中注入弹性模式"></a>在日常工作中注入弹性模式</h2><p>这样做的目标是为了给团队或系统增加弹力，提升抗脆弱性。想要抗脆弱，就要知道脆弱点在哪。根据前人的经验，我们可以通过缩短部署实践、提高测试覆盖率、缩短测试执行时间、系统解耦等方式，提升系统的弹力。我们可以通过故障演练，比如随机的拔网线、关电源、杀进程（比如Netflix的Chaos Monkey）等，能够验证系统的恢复能够力。我们还可以通过压测（单接口、全链路）来测试系统的瓶颈和上限。</p><p><img src="https://www.howardliu.cn/images/devops/chaos-monkey.jpg" alt="Netflix Chaos Monkey"></p><h2 id="领导层强化学习文化"><a href="#领导层强化学习文化" class="headerlink" title="领导层强化学习文化"></a>领导层强化学习文化</h2><p>这点是说给领导听的，优秀的领导力不是体现在做出所有的决定都是正确的，而是能够为团队创造条件，让团队在日常工作中感受到这种卓越。因为领导者不会亲自参与到一线工作，一线工作者也不了解大的组织环境或者不具备工作领域以外做出改变的权利，所以领导者与一线工作者之间是互补关系，必须互相尊重。</p><h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><p>DevOps三步工作法作为支撑DevOps的基础原则，也衍生出了DevOps的行为和模式。相信很多团队已经开始走DevOps之路了，下面列出来4个阶段：</p><ol><li>只有Dev没有Ops，所有的事情开发自己搞定。</li><li>有Dev也有Ops，他们相互独立，Ops承接了开发代码之外所有的工作。</li><li>Dev+Ops，Ops做了一些自动化的工具提升效率，但主要是给自己去用，开发不用。</li><li>DevOps，在上游工作的开发愿意使用下游的运维提供的系统或平台，通过API自助、自动的完成相应的工作。</li></ol><p>可以看下自己处于那种阶段，如果没有达到，可以参考上面的三步工作法，一步一步的实现DevOps。</p><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/the-three-ways-principles-underpinning-devops/">实现DevOps的三步工作法</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="https://blog.csdn.net/liuxinghao/article/details/104242451" target="_blank" rel="noopener">实现DevOps的三步工作法</a></p>]]></content>
    
    <summary type="html">
    
      实现DevOps的三步工作法：流动原则、反馈原则、持续学习与实验原则。
    
    </summary>
    
    
      <category term="devops" scheme="https://www.howardliu.cn/categories/devops/"/>
    
    
      <category term="DevOps" scheme="https://www.howardliu.cn/tags/DevOps/"/>
    
      <category term="Phoenix" scheme="https://www.howardliu.cn/tags/Phoenix/"/>
    
  </entry>
  
  <entry>
    <title>微服务中服务注册和发现的可行性方案</title>
    <link href="https://www.howardliu.cn/service-registry-and-discovery/"/>
    <id>https://www.howardliu.cn/service-registry-and-discovery/</id>
    <published>2020-01-28T08:57:09.000Z</published>
    <updated>2020-01-28T08:57:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>在 <a href="https://www.howardliu.cn/the-base-of-microservice/">微服务的基建工作</a> 中提到过，在云原生、微服务时代，如果还是手动修改服务地址，是几乎不可完成的工作，需要一种机制完成自动上报和获取服务地址的支撑组件，可以保障服务的快速上线和下线，这就是服务注册/发现组件。</p><a id="more"></a><blockquote><p>为了表述方便，从系统规模定义几个阶段：</p><ul><li>巨型应用架构时期：很多应用都是一个巨型服务，一个应用包含所有功能，部署在小型机和大型机上，或者直接部署在物理服务器上。</li><li>单体架构时期：应用体量缩小，服务增多，而且出现虚拟化技术，物理服务器被连接成虚拟化平台，应用部署在虚拟机中。</li><li>SOA架构时期：应用通用功能逐渐沉淀，业务应用借助沉淀的通用组件逐渐解耦，微服务的很多组件也是从这个时期开始成型。</li><li>微服务架构时期：这个时期承接模块化时期，甚至有一种说法是微服务只是SOA的一种特殊形式。系统进一步解耦，根据业务角色不同，应用以业务为分界，缩小为业务单元。</li><li>函数架构时期：应用进一步分割为函数，实现serverless架构，不需要具体的服务器概念，只需要执行函数的服务即可。目前来看，这个时期是比较理想的时期，因为不同人相互协作定义的函数，可能重复或者冲突，不利于架构的演进。</li></ul></blockquote><p>随着大家对在微服务或者函数架构中趟坑，很多人开始提出回归单体应用架构，这应该也是架构螺旋进步的一种方式。</p><p>在微服务中，还有一种角色是根据调用关系定义的：</p><ul><li>客户端服务（简称客户端）：调用其他服务的实例</li><li>服务端服务（简称服务端）：被其他服务实例调用的实例</li></ul><blockquote><p>微服务中客户端和服务端只对一个调用定义的，客户端在其他调用关系中，角色可能会转变为服务端。</p></blockquote><h1 id="服务注册表"><a href="#服务注册表" class="headerlink" title="服务注册表"></a>服务注册表</h1><p>说到服务发现时，必须要说一个重要组件：<strong>服务注册表</strong>，它是服务发现的核心，是一个包含了所有服务实例的网络位置和监控状态的数据库，通过服务注册组件将信息写入服务注册表，通过服务发现组件获取有效的服务实例的网络位置信息。目前常用的服务注册表有：Eureka、etcd、Consul、Zookeeper，Kibernetes等镜像调度服务没有明确的服务注册表组件，是通过内置的服务注册功能实现。对于比如F5和Nginx这种代理器，其中的upstream配置也属于服务注册表。</p><h1 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h1><p>在微服务架构中，服务之间通过轻量级协议互相调用，一般是HTTP请求，为了完成一次请求，服务需要知道目标服务实例的网络位置（IP和端口）。</p><p>在巨型应用架构时期，配置一个符合要求的服务器环境需要花费大量的时间，也就意味着服务地址发生变动的概率和频率都非常低，而且很多应用部署在一台小型机或者大型机上。到了单体架构时期，应用体量大数量少，发生地址变动所需要修改的地方就比较少，所以对于服务发现也就没有那么强的需求。换句话说，在单体架构之前，服务实例的相对位置固定，变动频率低，可以通过硬编码到代码中。</p><p>但是到了云时代，服务器环境配置变得简单，数量逐渐增多，扩展和迁移逐渐频繁。而且，随着虚拟化和容器的应用，服务器地址都是根据规则动态分配，由于服务升级、扩展、失败回滚等情况增多，服务的网络位置甚至不可预知。这个时候必须使用服务发现机制保证客户端服务能够自动获取服务端服务的地址。</p><p>通常，服务发现有两种模式：客户端发现模式、服务端发现模式。</p><h2 id="客户端发现模式"><a href="#客户端发现模式" class="headerlink" title="客户端发现模式"></a>客户端发现模式</h2><p>客户端发现模式<strong>通过客户端组件根据负载均衡算法决定相应服务实例的网络位置</strong>，也就是说，客户端组件保存有服务端所有实例的服务注册表，调用发生时，根据负载均衡算法，从服务注册表中选择一个网络位置，向服务端发起请求，完成调用。由于网络的不可靠性，有的客户端组件还会实现访问失败重试、访问超时时间设定等功能。</p><p>这种模式的架构如图：</p><p><img src="https://www.howardliu.cn/images/microservice/service_discovery_client_mode.png" alt="客户端发现模式"></p><p>具体的过程为：</p><ol><li>服务实例向服务注册器上报网络位置，即注册</li><li>客户端服务发现组件定时拉取服务注册器中服务实例的网络位置信息及健康状态，保存在服务注册表中</li><li>客户端服务调用服务端服务时，通过客户端服务发现组件，根据负载均衡算法，选取可用一个服务实例，发起调用</li></ol><p>在Spring Cloud（或者说是Netflix开源组件）中，组件Eureka Server组件相当于服务注册器，Eureka Client组件实现了服务注册表，Ribbon实现了负载均衡算法和重试策略。</p><p>客户端发现模式优缺点兼备。优点是对已有服务友好，除了客户端组件外，其他部分无需改动。而且，客户端存有所有服务实例信息，可以有针对性的定义负载均衡算法。缺点是客户端与服务注册器绑定，需要针对每种语言实现不同的客户端组件。</p><h2 id="服务端发现模式"><a href="#服务端发现模式" class="headerlink" title="服务端发现模式"></a>服务端发现模式</h2><p>服务端发现模式是<strong>有一个单独的服务发现组件，这个实例持有服务注册表，同时也起到负载均衡器的作用</strong>，客户端调用服务端时，直接调用服务发现实例，通过服务实例代理到后端服务实例中，所以服务端发现模式也被称为<strong>代理模式</strong>。</p><p>这种模式的架构如图：</p><p><img src="https://www.howardliu.cn/images/microservice/service_discovery_server_mode.png" alt="服务端发现模式"></p><p>具体的过程为：</p><ol><li>服务实例向服务注册器上报网络位置，即注册</li><li>服务发现实例定时通过某种机制获取服务注册器中服务实例的网络位置信息及健康状态，保存在服务注册表中</li><li>客户端服务调用服务端服务时，直接调用服务发现实例，服务发现实例根据内部实现，查询服务注册表，将请求代理到后端服务实例</li></ol><p>服务端发现模式中的服务发现组件有两种实现方式：</p><ul><li>第一种，集中式代理，服务发现组件是单独的服务实例，这个实例是高可用高吞吐的系统组件，代理后端服务实例，代表性的是F5和Nginx。</li><li>第二种，主机进程代理，服务发现组件由系统环境提供，集成在主机上或者集成在操作系统中，代表性的是Istio ServiceMesh。</li></ul><p>两种实现方式的优点是语言无关，客户端不需要关心任何服务发现的细节，只需要将原有的调用实例的请求修改为向服务发现实例发送请求。集中式代理的缺点是，存在单点问题，需要单独部署一个高可用、高吞吐的系统服务，由原来的一次调用增加为两次调用，有性能开销。主机进程代理的缺点是运维复杂，需要能力强的运维团队做支持。</p><h1 id="服务注册"><a href="#服务注册" class="headerlink" title="服务注册"></a>服务注册</h1><p>服务注册是将服务实例的网络信息和健康状态写入服务注册表中，有两种方式：自注册模式、第三方注册模式。</p><h2 id="自注册模式"><a href="#自注册模式" class="headerlink" title="自注册模式"></a>自注册模式</h2><p>这种模式是服务实例主动向服务注册表上报网络位置和健康状态，有的实现中，服务实例还会通过心跳保障注册信息不会过期。</p><p><img src="https://www.howardliu.cn/images/microservice/service_registry_self_registration_pattern.png" alt="自注册模式"></p><p>Eureka就是采用的这种方式，服务实例通过Eureka Client组件主动上报自己的网络位置信息和健康状态。</p><p>这种模式实现相对简单，但是把服务实例和服务注册表耦合，优缺点明显。</p><h2 id="第三方注册模式"><a href="#第三方注册模式" class="headerlink" title="第三方注册模式"></a>第三方注册模式</h2><p>第三方注册模式是服务实例不需要直接向服务注册表注册信息，而是借助被称为注册器的组件进行注册。服务注册器是通过扫描部署环境或者订阅事件的方式，跟踪服务实例的变更。当监测到服务实例有变化，会向服务注册表上报变化信息。</p><p><img src="https://www.howardliu.cn/images/microservice/service_registry_side_registration_pattern.png" alt="第三方注册模式"></p><p>这种方式可以将服务实例与服务注册表解耦，同时也引入另外的问题。即注册器需要内置在部署环境中，增加了运维复杂性。或者注册器需要部署一个集中式的管理组件，成为系统约束点。</p><h1 id="未完待续"><a href="#未完待续" class="headerlink" title="未完待续"></a>未完待续</h1><p>在微服务中，服务实例的运行环境会动态变化，实例网络位置也是如此，因此，客户端为了访问服务必须要使用服务发现机制。</p><p>服务发现有客户端发现模式和服务端发现模式，服务注册有自注册模式和第三方注册模式。服务发现和服务注册通过服务注册表链接在一起。</p><p>后面有时间，会再补充目前比较常用的服务发现、服务注册的相关组件。</p><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/service-registry-and-discovery/">微服务中服务注册和发现的可行性方案</a><br>CSDN主页: <a href="https://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">https://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="https://blog.csdn.net/liuxinghao/article/details/104108544" target="_blank" rel="noopener">微服务中服务注册和发现的可行性方案</a></p>]]></content>
    
    <summary type="html">
    
      在[微服务的基建工作](https://www.howardliu.cn/the-base-of-microservice/)中提到过，在云原生、微服务时代，如果还是手动修改服务地址，是几乎不可完成的工作，需要一种机制完成自动上报和获取服务地址的支撑组件，可以保障服务的快速上线和下线，这就是服务注册/发现组件。
    
    </summary>
    
    
      <category term="microservice" scheme="https://www.howardliu.cn/categories/microservice/"/>
    
    
      <category term="微服务" scheme="https://www.howardliu.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>微服务的基建工作</title>
    <link href="https://www.howardliu.cn/the-base-of-microservice/"/>
    <id>https://www.howardliu.cn/the-base-of-microservice/</id>
    <published>2020-01-04T03:24:54.000Z</published>
    <updated>2020-02-10T13:59:04.709Z</updated>
    
    <content type="html"><![CDATA[<p>前文说了一下<a href="https://www.howardliu.cn/what-is-microservice">《什么是微服务》</a>，在文末提到，初创团队不建议直接使用微服务，对于初创团队，最根本的是活下去，而想要使用微服务，需要有很多基础建设。本文就来说下，微服务都需要哪些基础建设。</p><blockquote><p>需要说明的是，下面这些组件，都是基于服务太多这个前提。</p></blockquote><p>微服务的出现是为了研发效能的提升：相同的人数可以处理更多的需求、维护更多的产品，可以更快的交付产品。基于这点，微服务的基础组件，就从解放人力，减少人为失误出发。</p><p>下面给出一张微服务基础组件的图片：</p><p><img src="https://www.howardliu.cn/images/microservice/microservice-architecture.png" alt="微服务架构"></p><a id="more"></a><h1 id="1-容器"><a href="#1-容器" class="headerlink" title="1 容器"></a>1 容器</h1><h2 id="1-1-运行容器"><a href="#1-1-运行容器" class="headerlink" title="1.1 运行容器"></a>1.1 运行容器</h2><p>服务运行的容器是支持服务提供对外访问的基础，根据微服务的要求，每个服务单独运行的独立进程中，所需要的运行容器就需要小巧灵活，运行容器可以集成在运行环境中，或者能够集成在服务可执行包中。</p><p>在Java领域，各大厂商都有自己的web容器：WebLogic、JBoss、Tomcat、Jetty等。SpringBoot内嵌了Tomcat和Jetty，默认打包方式是FatJar，jar保证包含了服务运行所有的基础，可以支持微服务部署的基本要求。</p><h2 id="1-2-部署容器"><a href="#1-2-部署容器" class="headerlink" title="1.2 部署容器"></a>1.2 部署容器</h2><p>部署容器是服务运行的加成组件，容器的好处在于一套镜像可以支撑测试和生成部署。这样做可以避免测试环境没有问题，生产环境各种报错的情况。但是想要实现一套镜像到处运行，还需要集中配置的支持。</p><p>而且对于部署容器，最好有一套容器调度平台，这样能够更有效的使用资源。如果没有，可能用部署容器和普通部署方式，区别不是很大。</p><h1 id="2-服务注册-发现"><a href="#2-服务注册-发现" class="headerlink" title="2 服务注册/发现"></a>2 服务注册/发现</h1><p>服务注册/发现是两个组件，彼此没有必然关系，但一般这两个组件会成对出现，解决同一个问题：服务地址动态变化的配置问题。</p><p>在单体架构时代，企业内部只有几个大系统，想要调用其他服务，只需要提前指定IP地址就行了。但是到了云原生时代，服务实例的网络地址都是动态分配的，想要提前指定IP是比较困难的。而且，在部署过程中，还会出现动态扩展、服务迁移、服务死亡等情况，服务实例也是动态变化。如果还是靠手工，不仅浪费时间，而且容易出错。</p><p>最好的办法就是，服务自己或通过代理人上报位置，然后客户端想要调用服务时，只需要从注册器中拿到一个可用的服务地址，直接调用，这就是服务注册/发现组件。</p><p><img src="https://www.howardliu.cn/images/microservice/service_registry_discovery.png" alt="服务注册/发现"></p><h1 id="3-网关"><a href="#3-网关" class="headerlink" title="3 网关"></a>3 网关</h1><p>在单体架构中，通常只有一组冗余或负载均衡的服务。在微服务架构中，每个服务都提供了一组细粒度的服务。对于一个普通的应用，可能需要调用多个不同的微服务，来获取全部的数据。这样做有三个缺点：</p><ol><li>调用服务太多，将应用于后端服务捆绑，同时也造成第二个缺点</li><li>服务难以重构，随着时间推移，服务可能会出现合并或者拆分的情况，应用于服务捆绑在一起，就难以重构</li><li>后端服务对外暴露的协议不一致，可能对web不友好。虽然微服务要求服务之间使用轻量级通信，但是并没有强调必须使用HTTP协议。</li></ol><p>这个时候，就需要有一个统一的出入口，来解耦应用于微服务，并且屏蔽内部细节，接收所有调用者请求。</p><p>目前在网关上有很多优秀的实践，将反向路由、安全认证、限流熔断、日志监控、灰度发布等功能放在网关上，将功能前置，简化微服务功能，让微服务团队可以更加专心于业务。</p><p><img src="https://www.howardliu.cn/images/microservice/api_gateway.png" alt="API网关"></p><h1 id="4-授权认证"><a href="#4-授权认证" class="headerlink" title="4 授权认证"></a>4 授权认证</h1><p>安全第一，什么行业都是安全第一。</p><p>授权认证是两个概念：用户授权和安全认证。用户授权是指给指定用户授权访问资源，然后某个用户访问资源的时候，认证用户是否有访问资源的权限。两者配合，共同完成资源的安全保护。业界比较常用的是使用OAuth2协议实现授权认证。</p><p><img src="https://www.howardliu.cn/images/microservice/iam.jpg" alt="授权认证"></p><h1 id="5-配置管理"><a href="#5-配置管理" class="headerlink" title="5 配置管理"></a>5 配置管理</h1><p>配置管理和前面说的服务注册/发现一样，都是为了解决服务太多，人工出差这个痛点的。</p><p>通常开发人员把配置放在配置文件中，这样配置不够规范，配置项追溯都比较麻烦。比较危险的是，涉及到用户名密码等一些安全性配置的，又不符合审计要求。而且，一旦需要大规模修改配置，改动时间长，改动之后就需要重新部署，可能对整个产品造成影响。所以就需要一个集中的配置管理服务。</p><h1 id="6-日志收集"><a href="#6-日志收集" class="headerlink" title="6 日志收集"></a>6 日志收集</h1><p>日志是记录服务运行情况的主要来源，也可以在发生异常情况时还原现场。但是随着服务的增多，日志分布在许多的服务器中，如果不进行聚合，在排查问题的时候，难上加难。</p><h1 id="7-监控告警"><a href="#7-监控告警" class="headerlink" title="7 监控告警"></a>7 监控告警</h1><p>监控告警不是微服务的专利，当服务集群或服务器达到一定规模，想要做到7*24不停机、不宕机提供服务，就需要监控告警。因为微服务的服务规模比较大，会将监控的必要性放大。</p><h2 id="7-1-指标"><a href="#7-1-指标" class="headerlink" title="7.1 指标"></a>7.1 指标</h2><p>通常监控指标是会从系统、应用、业务等几个维度进行：</p><ol><li>系统监控：主要是监控物理机、虚拟机、操作系统的运行情况，主要指标包括CPU、内存、磁盘、网络等，其他的一些相关的数据包括物理机运行时间、操作系统版本、操作系统内核，这些也是排查问题的一些基本依据。这里还需要重点说一下网络，微服务都是通过网络调用或被调用，一旦网络出现问题，整个微服务集群都是不可用的，所以网络监控需要细化到流量、数据包、丢包、错报、连接数等指标。</li><li>应用监控：主要是监控应用的运行情况，包括应用运行时间、http服务端口、服务url、http服务响应码、http服务响应时间、SQL、缓存命中、TPS、QPS等。对于Java应用，还需要包括JVM运行情况：JDK版本、内存使用（堆内存、非堆内存等）、GC等Java虚拟机运行情况。</li><li>业务监控：主要是监控一些核心业务执行情况，对业务有一定的侵入性，各个服务的指标不同，各家监控方式也不同，通常是埋码。比如监控登录注册、商品信息、库存情况、下单、支付、发货等各个业务。</li></ol><h2 id="7-2-健康"><a href="#7-2-健康" class="headerlink" title="7.2 健康"></a>7.2 健康</h2><p>一般健康检查是通过心跳检测进行的，通常会分为两种：</p><ol><li>一种是建立TCP链接，执行ping/pong调用。这种方式需要服务中与监控系统建立TCP链接，需要在服务中嵌入监控组件，对服务有侵入。但是因为其执行效率高，而且针对性强，不会出现漏报的情况。</li><li>一种是监听服务端口，这种方式只需要在容器内或者虚拟机增加监控插件，对服务没什么侵入，但是由于端口可用和服务可用不是一个概念，所以会出现漏报的情况。</li></ol><h2 id="7-3-调用链"><a href="#7-3-调用链" class="headerlink" title="7.3 调用链"></a>7.3 调用链</h2><p>微服务之间彼此调用，整个的调用链路彼此交错，如果不加管理，很有可能演变成请求风暴。</p><p>调用链监控是为了分析系统依赖、请求耗时、请求瓶颈的一种方式。目前，市面上多数调用链监控组件都是基于Google Dapper开发的。下面给出调用链监控的原理示意图（因为调用链监控的内容比较多，所以会在后面单独开一章）：</p><p><img src="https://www.howardliu.cn/images/microservice/trace.png" alt="Trace"></p><h2 id="7-4-异常收集"><a href="#7-4-异常收集" class="headerlink" title="7.4 异常收集"></a>7.4 异常收集</h2><p>异常分成两种，逻辑异常和行为异常。逻辑异常是说代码中存在异常逻辑，比如常见的NPE；行为异常时用户行为不可预期而出现的异常，这两种情况对系统都有一定危害。所以需要收集这些异常情况，并且能够定位异常发生的位置。异常信息收集主要是为了定位问题，所以上报的信息一定要全面而且容易定位。所以上报信息中需要保护异常码，可以自定义一定长度的字符串，便于定位位置。然后是要上报参数，用于还原现场。还要上报异常信息，用来分析异常情况。</p><h1 id="8-最后"><a href="#8-最后" class="headerlink" title="8 最后"></a>8 最后</h1><p>上面提到的组件，都是为了更好的管理微服务，减少人肉运维，减少人为失误。寥寥数语，难以道尽上述组件的优点，每个组件都需要单独细聊，这里就当是个引子。</p><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/the-base-of-microservice/">微服务的基建工作</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="https://blog.csdn.net/liuxinghao/article/details/103845612" target="_blank" rel="noopener">微服务的基建工作</a></p>]]></content>
    
    <summary type="html">
    
      微服务的出现是为了研发效能的提升，相同的人数可以接受/处理更多的需求、维护更多的产品，可以更快的研发、交付，做到这两点，团队效能已经不容小视。
    
    </summary>
    
    
      <category term="microservice" scheme="https://www.howardliu.cn/categories/microservice/"/>
    
    
      <category term="微服务" scheme="https://www.howardliu.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="架构" scheme="https://www.howardliu.cn/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>什么是微服务？</title>
    <link href="https://www.howardliu.cn/what-is-microservice/"/>
    <id>https://www.howardliu.cn/what-is-microservice/</id>
    <published>2020-01-01T10:00:00.000Z</published>
    <updated>2020-01-04T03:57:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>我所理解的微服务，就六个字：“高内聚，低耦合”。</p><p>没错，就是这个在软件开发过程中被反复提到的六个字，各类设计模式、架构设计、从入门到放弃等各种书中总会提到，从初级到高级到骨灰级程序员、架构师挂在嘴边的也是这六个字。只不过，在微服务概念之前，这六个字被用在类、模块、组件上，微服务则是将它放在服务上。</p><blockquote><p>注：上面是精简版，下面是完整版，看官自便。</p></blockquote><a id="more"></a><h1 id="什么是微服务"><a href="#什么是微服务" class="headerlink" title="什么是微服务"></a>什么是微服务</h1><p>微服务，2014年被大神马丁·福勒提出（所以2014年被称为微服务元年），在<a href="https://martinfowler.com/articles/microservices.html" target="_blank" rel="noopener">他的博客</a>中提出他对微服务的理解。这个时候，微服务还高高在上，大家不知道如何落地实现。一直到2016年，随着各种技术和工具的落地，很多互联网公司才初步实现微服务，至此，微服务才真正造福（还是祸乱？）于开发界。</p><p>那到底什么是微服务？这里引用马丁·福勒原文：</p><blockquote><p>The microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies.</p></blockquote><p>概括大意就是：</p><p>微服务是一种架构风格：</p><ol><li>一组小服务</li><li>每个服务运行在独立进程中</li><li>服务之间使用轻量级通信</li><li>服务可独立部署</li><li>是基于业务能力实现</li><li>无集中式管理</li></ol><p>基本上，符合上面这几条建议的架构风格，就是微服务。不过业界现在也没有统一定义，所以，如果发现以后微服务定义和上面的有偏差，一定是微服务的兼容性、包容性使然。</p><p>业界对于微服务还有一句更加简短的描述：<strong>微服务是一组小而自治的服务</strong>。</p><p>这两个概念互相补充，微服务就是一组<strong>小</strong>、<strong>自治</strong>、<strong>基于业务</strong>的服务，以松散的服务方式，构建可独立部署的模块化应用。</p><h2 id="小"><a href="#小" class="headerlink" title="小"></a>小</h2><p>说到“小”，有很多种观点，有的以代码行数界定，有的以重构时间界定，我个人认为这两种方式都不好。</p><p>首先，来说说以代码行数界定的方式。大家都是业内人士，都清楚代码行数有时候和编程习惯、开发能力有直接关系。比如，IP地址校验，正则表达式的写法和自己判断编写规则判断，代码行数就完全不一样。再者，代码行数是定2万行还是10万行，还是多少？假设定2万行，那20001行是不是就不符合微服务定义。再者，对于一个项目所依赖的基础类库、应用框架、编码规范，这些定义不同，代码行数也会有偏差。最后，对于动态语言和静态语言，其编译器、运行环境、支撑类库不同，代码行数也不尽相同。所以，用代码行数评判，是完全不可行的。</p><p>第二，用重构时间来界定。重构是一个考验对业务和架构理解的行为，功夫深浅不同，重构时间不同，功夫不到，还可能进入重构地狱，重构最后还会流产；功夫到了，可能几天时间就能重构一个异常复杂、异常庞大的系统。所以，用这种看经验、看水平的方式区分服务的微还是不微，也不可行。</p><p>我认为，这里说的小，可以类比“单一职责原则”。服务小而专，服务之间耦合低，可以实现业务的高度自治。简单说，就是能够让普通开发人员理解，业务上不掺杂不相干的业务，保持业务原子性。能够做到这样，就是小。</p><p>拿电商系统中的商品服务举例，商品包括商品基本信息、价格、库存等一系列功能。这些都属于商品，但如果把这些业务都放在一个服务里，拿这个服务就违反了小这个概念。商品主要是电商中的信息展示、下单、发货场景，展示基本信息、价格、库存，下单校验价格、库存，发货需要商品库存和仓储信息。看似几个功能中商品数据都有重叠，但是又不是完全重叠。比如商品信息展示，基本信息是最重要的，价格第二等重要，库存重要性最低。如果遇到高峰或者压力过大，库存可以降级，在下单时再做校验；如果压力再升，价格也可以不用实时更新变价。所以一个商品就可以拆分为三个服务：基本信息服务、商品价格服务、商品库存服务。这样一拆，职责单一，功能内聚。</p><p>可能有人认为把商品放在一个服务多好，接口区分就好了，实现简单。其实就“小”这个概念来说，也不是不可以，但是这会和下面的几条建议有冲突。</p><h2 id="自治"><a href="#自治" class="headerlink" title="自治"></a>自治</h2><p>每个微服务都是独立的个体，部署在独立进程中，每个服务一个进程。进程彼此独立，也就互相之间没有制约，无论是升级、修改、发布、回滚等一系列操作，都可以在一定情况下忽略其他服务。而且，服务在独立进程中，就可以独立监控，可以比较清晰的知道服务的运行情况，监控服务占用物理资源、运行Metric等就更加简单。这也算是服务独立进程部署的一个附加价值。</p><p>服务部署在独立进程中，彼此之间耦合低，通过网络通信时，还需要承诺使用技术无关的轻量级通信协议，最常见的是 HTTP/REST 或者 RPC 通信。很多关于微服务通信的建议中不推荐 RPC，是因为这个很可能会引入技术限制，但是目前的很多 RPC 组件，都提供了多语言支持，而且 RPC 的效率明显高于 HTTP/REST。但是不推荐使用强语言依赖的通信协议，比如Java RMI。同时也不推荐使用使用比较重的协议，比如 WebService。这样做的好处就是服务之间不耦合，可以选择更加适合的语言、工具或平台。有一条比较好的建议是，微服务的服务通信接口不应该随意变动，如果需要变动，需要提供更加兼容的方式，这样能够在一定程度上减少服务修改造成的影响。</p><p>每个微服务部署在独立进程中，从另一个维度解释就是，每个服务可以独立部署，在持续集成方面就有个更灵活的时间。部署过程中的提交、静态代码扫描、单元测试等流水线，可以独立或极少的依赖其他服务进行写作测试，也能够应用蓝绿部署、金丝雀部署等各种优秀的部署实践。退一步讲，可以独立的部署，也就可以独立的回滚。</p><p>还有一点比较重要，自治能够帮助每个微服务个体可以根据业务需要由选择技术栈，需要关注的是业务本身适合的技术，而不是其他依赖或不兼容的情况。（当然，技术栈不是越多越好，技术栈过多，可能在人员协调和招聘方面会有一定的影响。）</p><h2 id="基于业务"><a href="#基于业务" class="headerlink" title="基于业务"></a>基于业务</h2><p>微服务团队是围绕业务组织的跨职能团队，产品、设计、研发、测试、架构、运维等各种角色齐全，按照业务能力组织团队，在不同业务深耕，不断优化迭代。</p><p>这里不得不提到康威定律：</p><blockquote><p>Organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations.(设计系统的组织，其产生的设计等同于组织之内、组织之间的沟通结构)</p><p>第一定律：Communication dictates design（组织沟通方式会通过系统设计表达出来）</p><p>第二定律：There is never enough time to do something right, but there is always enough time to do it over（时间再多一件事情也不可能做的完美，但总有时间做完一件事情）</p><p>第三定律：There is a homomorphism from the linear graph of a system to the linear graph of its design organization（线型系统和线型组织架构间有潜在的异质同态特性）</p><p>第四定律： The structures of large systems tend to disintegrate during development, qualitatively more so than with small systems（大的系统组织总是比小系统更倾向于分解）</p></blockquote><p><img src="https://www.howardliu.cn/images/microservice/conway.png" alt="康威定律"></p><p>上面这张图是康威定律在几个超大型公司的理论支持，看官可以从他们的产品和组织架构得到一定的启发。</p><p>组件基于业务的团队，还可以是团队成员关注产品本身，而不是只关注与项目。团队成员就是产品的主人，有责任和义务保证产品的快速发展和演进迭代。同时可以制定更加行之有效的奖惩机制，成员有更强的产品成就感、荣誉感。亚马逊有一条建议”you build, you run it”，业务团队对生产中产品付全部责任，</p><p>微服务自治的特性中提过，基于业务能力，可以更加业务特性选择不同的技术栈，这一点非常重要。</p><!-- updated: 2020-01-04 11:57:00 微服务的优势 --><h1 id="微服务的优势"><a href="#微服务的优势" class="headerlink" title="微服务的优势"></a>微服务的优势</h1><p>上面说了一些微服务的特点，我们使用它，一定是它有一些别的架构风格不完全具备的优势：</p><ol><li>每个服务足够内聚，足够小，代码聚焦在一个业务中，容易理解</li><li>微服务之间松耦合，彼此独立，升级或修改彼此影响少</li><li>每个服务专注一件事情，所以开发简单，开发效率高，代码交付快</li><li>更容易让开发人员理解，开发、修改、维护成本降低</li><li>团队小而精，便于管理且沟通成本低，而且更专注自己的工作成果</li><li>可自由选择技术栈</li><li>更容易实践DevOps，实现自动化测试、自动部署、持续集成等一系列优秀实践</li><li>数据分离，服务分离，更便于根据各个服务的特点进行优化、扩容、备份等</li></ol><h1 id="微服务的挑战"><a href="#微服务的挑战" class="headerlink" title="微服务的挑战"></a>微服务的挑战</h1><p>IT界有句名言：没有银弹。也就是说，没有完美的解决方案，微服务也是。</p><p>微服务是一个重型杀伤性武器，是叶轻眉留给范闲的巴雷特，但是前提是，范闲有子弹，而且会用。微服务有千百种好处，但是这些好处都是有代价的。</p><h2 id="系统复杂性"><a href="#系统复杂性" class="headerlink" title="系统复杂性"></a>系统复杂性</h2><p>由单体服务拆解为一系列的微服务，也就形成了分布式系统，系统复杂性不可同日而语。</p><p>首先是服务性能。相较于进程内的方法调用，微服务之间只能通过进程间通信协议进行沟通，其通信效率依赖于网络和带宽等物理因素。同时，从原来的进程内部方法调用，变成服务之间网络通信，也会对网络和带宽带来压力。两者之间，互为因果，彼此影响。</p><p>然后是服务的可靠性，服务增多，服务发生故障的概率不变的情况下，发生故障的次数就会增多，可靠性随之降低。同时，由于第一种情况引入的网络延迟等，服务通信失败的情况更加复杂。比如，网络超时这种异常情况：被调用方已经开始处理请求，而且最终会成功，但是调用方出现网络超时这种错误，如果单纯的认为服务调用异常，本地事务回滚，就可能造成数据的不一致性（也是第二个复杂性）。</p><p>最后就是开发、测试、问题定位也变得不可控：</p><ul><li>对于开发，在单体架构中，业务之间都是方法调用，这个再简单不过，但是在微服务中，就得改为接口调用，而且还要根据通信接口的特性捕捉异常，根据捕捉异常的类型进行处理。而且，在需要事务的场景中，单体架构本地事务就可以解决绝大部分问题，但是在微服务场景中，就需要引入分布式事务，不同的分布式事务实践又需要不同的回滚实现。</li><li>对于测试，单体架构的测试可以使用简单的测试用例，但是在微服务中，服务之间彼此依赖，一个业务可能涉及多个服务之间的数据，启动多个服务联合测试，会造成人力成本提升；使用Mock接口，就增加了开发工作量，也对测试人员有了更高的要求。</li><li>对于问题定位，一旦测试过程中或线上出现问题，需要定位问题，找到问题出现的原因，进而解决问题。但是在微服务架构中，因为服务众多，一个错误产生的原因，可能是调用方数据不准确，也可能是被调用方逻辑有问题，甚至是被调用方的下游被调用方出现的异常。</li></ul><h2 id="数据一致性"><a href="#数据一致性" class="headerlink" title="数据一致性"></a>数据一致性</h2><p>如上面提到的，因为微服务之间调用的方式由方法调用变为服务通信调用，而且数据分而治之，所以没有办法依赖于简单的数据库事务解决，所以数据一致性就是问题。所以会出现两种方式，一种是分布式事务保证数据同步一致性，一种的基于异步消息保证数据最终一致性。无论哪种方式，又会增强第一种挑战：系统复杂性。</p><h2 id="运维复杂性"><a href="#运维复杂性" class="headerlink" title="运维复杂性"></a>运维复杂性</h2><p>运维的复杂性主要体现在配置、部署、监控等方面。</p><p>随着服务的增多，服务参数、数据库、缓存等一些列启动运行和依赖配置随之增多，而且变得复杂，而且不同的环境，还需要不同的配置。</p><p>服务增多，部署成本随之增高。在单体架构时代，只有一个应用包。但是在微服务时代，可能需要上百个服务。如果没有流水线这样的持续发布工具，单靠人力，部署将会是运维的噩梦。</p><p>在微服务时代，监控是必不可少的。监控包括服务监控、系统监控、指标告警、日志采集等，不单单是需要监控的终端变多，监控的数据量、异常分析都会增多，这也都提高了运维复杂性。</p><h2 id="测试复杂性"><a href="#测试复杂性" class="headerlink" title="测试复杂性"></a>测试复杂性</h2><p>在系统复杂性中提到过测试变得更加复杂。这个复杂不单单是测试的方式，还有测试的范围。在微服务中，为了覆盖所有的场景，测试可以分单元测试、服务测试、端到端测试。为了简便，服务测试又分了Mock和Stub等不同的流派。</p><h1 id="微服务是什么"><a href="#微服务是什么" class="headerlink" title="微服务是什么"></a>微服务是什么</h1><p>微服务就是将“高内聚、低耦合”应用到服务中的一种软件研发建议，是一种更适合当下业务快速多变的架构风格，是一组<strong>小</strong>、<strong>自治</strong>、<strong>基于业务</strong>的模块化松散服务。</p><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/what-is-microservice/">什么是微服务？</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="https://blog.csdn.net/liuxinghao/article/details/103794938" target="_blank" rel="noopener">什么是微服务？</a></p>]]></content>
    
    <summary type="html">
    
      什么是微服务？我所理解的微服务，就六个字：“高内聚，低耦合”。没错，就是这个在软件开发过程中被反复提到的六个字，各类设计模式、架构设计、从入门到放弃等各种书中总会提到，从初级到高级到骨灰级程序员、架构师挂在嘴边的也是这六个字。只不过，在微服务概念之前，这六个字被用在类、模块、组件上，微服务则是将它放在服务上。
    
    </summary>
    
    
      <category term="microservice" scheme="https://www.howardliu.cn/categories/microservice/"/>
    
    
      <category term="微服务" scheme="https://www.howardliu.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="架构" scheme="https://www.howardliu.cn/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>微服务编程范式</title>
    <link href="https://www.howardliu.cn/the-paradigm-of-microservice/"/>
    <id>https://www.howardliu.cn/the-paradigm-of-microservice/</id>
    <published>2019-11-09T09:12:21.000Z</published>
    <updated>2020-01-27T10:03:28.075Z</updated>
    
    <content type="html"><![CDATA[<p>目前很多互联网公司都采用微服务架构，微服务的优点和缺点被反复说到，这里不在重复赘述，只结合工作中的一些实践，说说要用微服务要注意的点，厚颜写做编程范式，其实就是一些具体实践而已。</p><a id="more"></a><h1 id="原则（道）"><a href="#原则（道）" class="headerlink" title="原则（道）"></a>原则（道）</h1><p>原则是比较抽象的一个概念，简单说是一些指导意见，在不同的组之间可以共享，是为了实现一个共同的目的，必须遵守的一些规则，或者叫做规矩。</p><p>这些规则（或规矩）对我们开发过程中有一定的约束作用，不容置疑，必须遵守。如果发现有那个团队或者个人没有遵守，一定要及时纠正，否则，原则就没有任何存在的意义。</p><p>现在用的很广的一个原则是Heroku的 <strong><a href="http://www.12factor.net" target="_blank" rel="noopener">12-Factor</a></strong> 原则。具体内容如下：</p><blockquote><p>I. 基准代码<br>一份基准代码，多份部署<br>II. 依赖<br>显式声明依赖关系<br>III. 配置<br>在环境中存储配置<br>IV. 后端服务<br>把后端服务当作附加资源<br>V. 构建，发布，运行<br>严格分离构建和运行<br>VI. 进程<br>以一个或多个无状态进程运行应用<br>VII. 端口绑定<br>通过端口绑定提供服务<br>VIII. 并发<br>通过进程模型进行扩展<br>IX. 易处理<br>快速启动和优雅终止可最大化健壮性<br>X. 开发环境与线上环境等价<br>尽可能的保持开发，预发布，线上环境相同<br>XI. 日志<br>把日志当作事件流<br>XII. 管理进程<br>后台管理任务当作一次性进程运行</p></blockquote><p>在我们团队中有一些原则可以和大家分享：</p><ol><li>不要为了用而用，程序猿（或者叫攻城狮）是最喜欢尝鲜的人，有了新技术出来，就想用。这无疑是优点，但是在工作中这样，就有可能给系统带来灾难。所以如果想用某项技术，必须充分调研之后，经过一系列的验证，才能引入。</li><li>战略目标明确，业务部门愿景清晰。作为开发团队，可能很少关心业务团队的想法，这是很致命的。我们开发的东西，不是业务部门想要的，顾客想要一个吃饭的勺子，你给做了一个铁铲，铁铲做的再好又有什么用。</li><li>服务之间调用必须使用HTTP/RESTful方案，不能引入其他方案。不是说其他方案不好，而是最好协议一致，一致的协议能够减少系统的复杂性和沟通成本。</li></ol><h1 id="标准（术）"><a href="#标准（术）" class="headerlink" title="标准（术）"></a>标准（术）</h1><p>标准的定义会比原则更加具体，有点类似于道和术、战略和战术的关系，不同的技术栈、不同的团队可能会制定不同的标准。</p><ol><li>我们团队都是使用的是Java技术栈，所以标准大体上采用的是<a href="http://dd.ma/m5R1zqRj" target="_blank" rel="noopener">《阿里巴巴Java开发手册》</a>，有一部分内容作了修改，这里对孤尽表示感谢。这个手册脱胎于《Effective Java》和《代码简洁之道》，其中加入了孤尽在阿里的一些实践，所以对于Java栈的同学是比较适用的。</li><li>我们使用的是Spring Cloud，服务之间的调用，必须使用Feign Client形式，指定这个标准是为了以后使用K8s时改动最小。</li><li>页面与服务之间的调用，HTTP返回状态码都是200，在返回的数据中，定义具体的状态码，这个状态码参照HTTP状态码，同时定义一个子级状态码，用来具体定义业务情况。比如，状态码等于500表示服务异常，子级状态码等于5001，表示操作数据库异常等。</li><li>监控系统、日志系统、任务调度等，如果需要，也要指定明确是标准。比如打印日志时，日志开头必须以6位数字开头，因为我们的日志系统与监控系统对接，6位数字能够定位到不同的系统、模块、业务，直接定位问题位置。</li><li>不一而足（每个团队有每个团队具体的标准，这里就不一一列举了）。。。</li></ol><p>这些标准，最好形成文档，放在知识库中。这样，团队的成员可以随时查看，有新人加入时，也能避免老员工口口相传，传错了指令。</p><blockquote><p>有些架构师认为，原则和标准就是一个东西。就我来看，这两个粒度不同，对于大的团队，最好分开。因为大的团队，技术栈更加多样化，标准没有办法做到一致，但是原则可以做到不会脱离大框。对于小的团队，因为技术栈比较单一，有可能就是一个技术栈（就像我现在的团队），因为标准只有一个，就是对原则的细化，所以两者就是一个东西。</p></blockquote><p>标准的另外一个作用，就是告诉团队成员怎么做是对的，怎么做是更好的方案，更加直白的表述是，按照标准进行开发，bug更少。</p><p>有了开发标准之后，就需要将标准推广到所有人，但是每个人的理解力和执行力是有偏差的，所以需要一些手段来快速推广。具体的方法有：DEMO（示例）、代码模板（脚手架）。</p><h2 id="DEMO（示例）"><a href="#DEMO（示例）" class="headerlink" title="DEMO（示例）"></a>DEMO（示例）</h2><p>软件开发是一个比较有技术壁垒的行业，一个新人（没有经验或者有经验刚加入新团队的人）想要快速了解老系统所使用的标准，是比较困难的，毕竟每家团队采用的标准都不是百分之百一致。比较简单的做法就是，提供新人DEMO示例，然后告诉他，“就照着这个做”。</p><blockquote><p>对于有经验的新人，一定是先接受，然后在了解清楚之后，才会提出自己的一些看法，而不是一上来，就搬出自己以前的经验，全盘否定团队指定的标准。</p></blockquote><h2 id="代码模板（脚手架）"><a href="#代码模板（脚手架）" class="headerlink" title="代码模板（脚手架）"></a>代码模板（脚手架）</h2><p>代码模板的作用实现一个服务的集成方案，经过有效可靠的裁剪和定制。在需要新建服务时，就使用这个方案，直接进行业务代码开发即可，所以也被称为脚手架，比如SpringBoot的Starter和AutoConfiguration。</p><p>前面说过，我们团队使用的是Java技术栈，基于SpringCloud开发，所以我们对SpringCloud进行封装，定义了几根通用的组件，比如定义了一个<code>web-misc</code>组件，引入这个组件，就能够实现，引入实现WebMvc，并且提供了统一的获取Metric信息的接口，统一的异常处理的ExceptionHandler等。</p><h1 id="及时清理技术债务"><a href="#及时清理技术债务" class="headerlink" title="及时清理技术债务"></a>及时清理技术债务</h1><p>虽然我们都是代码洁癖，但是有时候迫于时间压力、业务压力，我们不得不背负一些技术债务。</p><p>比如我们知道硬编码会破坏系统灵活性，但是明天就要上线，根本没有时间做配置型服务，所以只能硬编码到系统中，这就是技术债务。第二天系统上线，正常运行，如果这个时候把硬编码事情抛之脑后，这个债务就会产生利息，不知道哪天就会变成炸弹。</p><p>对于技术债务，我们团队的做法是做一个技术债务清单，设置超时提醒功能，限期修复。这个清单是公开的，每一项都注明了，是谁，因为什么，于什么时间，产生的技术债务，需要在什么时候修复。如果是临时特性或功能产生的债务，也需要注明特性或功能过期时间，由专人检查债务是否已经没有了。</p><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/the-paradigm-of-microservice/">微服务编程范式</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="https://blog.csdn.net/liuxinghao/article/details/102990899" target="_blank" rel="noopener">微服务编程范式</a></p>]]></content>
    
    <summary type="html">
    
      目前很多互联网公司都采用微服务架构，微服务的优点和缺点被反复说到，这里不在重复赘述，只结合工作中的一些实践，说说要用微服务要注意的点，厚颜写做编程范式，其实就是一些技巧而已。
    
    </summary>
    
    
      <category term="microservice" scheme="https://www.howardliu.cn/categories/microservice/"/>
    
    
      <category term="微服务" scheme="https://www.howardliu.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="方法论" scheme="https://www.howardliu.cn/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"/>
    
      <category term="范式" scheme="https://www.howardliu.cn/tags/%E8%8C%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>源码安装NGINX</title>
    <link href="https://www.howardliu.cn/build-nginx-from-sources/"/>
    <id>https://www.howardliu.cn/build-nginx-from-sources/</id>
    <published>2019-11-09T05:21:01.000Z</published>
    <updated>2020-01-27T09:58:58.703Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录一次从源码安装Nginx过程，参考的是<a href="http://nginx.org/en/docs/configure.html" target="_blank" rel="noopener">Nginx官网</a>。</p><a id="more"></a><p>安装过程比较简单，就是下载源码包，下载依赖包，打包编译安装就完事了。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><ol><li>安装依赖包<pre><code class="shell">yum -y install gcc automake autoconf libtool makeyum -y install openssl openssl-devel</code></pre>因为下面的安装过程会打包编译https模块，依赖于openssl，所以需要安装这个依赖，否则会出现<code>./configure: error: SSL modules require the OpenSSL library.</code>的异常。</li><li>下载源码包<br>直接从<a href="http://nginx.org/download/nginx-1.17.5.tar.gz" target="_blank" rel="noopener">官网</a>下载安装包即可</li><li>下载依赖包<br>需要的依赖<code>pcre</code>和<code>zlib</code>，从各自的官网下载即可：<a href="http://www.pcre.org/" target="_blank" rel="noopener">PCRE</a>，<a href="https://ftp.pcre.org/pub/pcre/pcre-8.41.tar.gz" target="_blank" rel="noopener">下载地址</a>，<a href="http://zlib.net/" target="_blank" rel="noopener">zlib</a>，<a href="http://zlib.net/zlib-1.2.11.tar.gz" target="_blank" rel="noopener">下载地址</a>。</li><li>配置<pre><code class="shell"> # 进入nginx源码目录中 cd /path/to/nginx-source-directory ./configure \     --sbin-path=/usr/local/nginx/sbin/nginx \     --conf-path=/usr/local/nginx/conf/nginx.conf \     --pid-path=/usr/local/nginx/nginx.pid \     --with-http_ssl_module -\     -with-pcre=../pcre-8.41 \     --with-zlib=../zlib-1.2.11</code></pre></li><li>编译<pre><code class="shell"># 进入nginx源码目录中cd /path/to/nginx-source-directorymake</code></pre></li><li>安装<pre><code class="shell"># 进入nginx源码目录中cd /path/to/nginx-source-directorymake install</code></pre></li></ol><p>到这里Nginx就安装完成了，下面给出一些nginx配置的建议。</p><h1 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h1><p>nginx安装完之后，就需要进行一些配置，下面是我的一些建议。</p><ol><li><p>目录结构</p><p>在<code>conf</code>目录中，创建<code>vhosts</code>和<code>upstreams</code>目录，两个目录分别存储<code>server</code>和<code>upstream</code>的定义。在<code>nginx.conf</code>中添加下面的代码引入配置：</p><pre><code class="conf"> include upstreams/*.conf; include vhosts/*.conf;</code></pre></li><li><p>文件命名</p><p>在<code>vhosts</code>定义文件格式为<code>*.vhost.conf</code>，如果监听服务是域名，以域名倒置格式命名，比如：<code>cn.howardliu.www.vhost.conf</code>，这样的好处是，相同一级域名，不同二级域名的配置文件，在文件列表展示的时候，会在一起，比较好区分隔离。</p><p><code>upstreams</code>定义文件格式为<code>*.upstream.conf</code>，以服务模块名进行区分，好处是能够在一个文件中定义相同服务模块的内容，进行服务或模块隔离，或者以<code>服务.模块.upstream.conf</code>的格式，但是这样的坏处是，比较散，文件比较多。</p></li><li><p>建议显示生命链接超时时间</p><p>在<code>nginx.conf</code>的<code>http</code>中定义超时时间，下面是我测试环境的一个定义，需要根据自己的情况：</p><pre><code class="conf"> fastcgi_connect_timeout 300; fastcgi_read_timeout 300; fastcgi_send_timeout 300; proxy_connect_timeout 300s; proxy_read_timeout 300s; proxy_send_timeout 300s;</code></pre></li></ol><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/build-nginx-from-sources/">源码安装NGINX</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="https://blog.csdn.net/conansix/article/details/102986884" target="_blank" rel="noopener">源码安装NGINX</a></p>]]></content>
    
    <summary type="html">
    
      本文主要记录一次从源码安装Nginx过程，参考的是[Nginx官网](http://nginx.org/en/docs/configure.html)。
    
    </summary>
    
    
      <category term="devops" scheme="https://www.howardliu.cn/categories/devops/"/>
    
    
      <category term="DevOps" scheme="https://www.howardliu.cn/tags/DevOps/"/>
    
      <category term="Nginx" scheme="https://www.howardliu.cn/tags/Nginx/"/>
    
      <category term="部署" scheme="https://www.howardliu.cn/tags/%E9%83%A8%E7%BD%B2/"/>
    
  </entry>
  
  <entry>
    <title>闲聊之更换手机的成本</title>
    <link href="https://www.howardliu.cn/the-cost-to-change-phone/"/>
    <id>https://www.howardliu.cn/the-cost-to-change-phone/</id>
    <published>2019-11-09T05:13:00.000Z</published>
    <updated>2020-02-23T08:54:51.857Z</updated>
    
    <content type="html"><![CDATA[<p>现在手机越来越便宜，换手机是比较常见的一件事，所以各大厂商为了降低更换手机的成本，也是各种手段费尽心思：一键换机、云账号。。。但是这些方式都建立在一个基础上，就是同品牌手机才能用。如果是跨品牌换机，那真是要经历九九八十一难，百转千回才能顺利使用新机。像我这种懒人，可能还要在很长一段时间继续使用旧手机。</p><a id="more"></a><p>特意查了一下，成本有很多定义：</p><blockquote><p>1、成本是生产和销售一定种类与数量产品以耗费资源用货币计量的经济价值。企业进行产品生产需要消耗生产资料和劳动力，这些消耗在成本中用货币计量，就表现为材料费用、折旧费用、工资费用等。企业的经营活动不仅包括生产，也包括销售活动，因此在销售活动中所发生的费用，也应计入成本。同时，为了管理生产所发生的费用，也应计入成本。同时，为了管理生产经营活动所发生的费用也具有形成成本的性质。</p><p>2、成本是为取得物质资源所需付出的经济价值。企业为进行生产经营活动，购置各种生产资料或采购商品，而<br>支付的价款和费用，就是购置成本或采购成本。随着生产经营活动的不断进行，这些成本就转化为生产成本和销售成本。</p><p>3、成本是为达到一定目的而付出或应付出资源的价值牺牲，它可用货币单位加以计量。</p><p>4、成本是为达到一种目的而放弃另一种目的所牺牲的经济价值。</p><p>。。。</p></blockquote><p>适用于更换手机的成本，应该属于第三种：<strong>成本是为达到一定目的而付出或应付出资源的价值牺牲，它可用货币单位加以计量。</strong></p><p>首先，我们更换手机的场景都有哪些呢？</p><p>因为我从诺基亚的砸核桃手机之后，用的一直是小米，所以对小米手机是比较了解的，下面就以小米手机来举例子。</p><ol><li>同品牌同款手机更新换代，比如从小米1到小米3</li><li>同品牌不同款手机更换，比如从红米手机到小米手机</li><li>相同操作系统不同品牌手机更换，比如从华为到小米</li><li>不同操作系统手机更换，比如从Android到IOS或从IOS到Android</li></ol><p>第一、二种情况，对于一家好的厂商，应该都能做到几乎0成本换机，只要登录账号，或者扫描二维码，连上网，手机放在边上，等待一段时间就可以使用新机了。这点小米的MIUI做的很好，不光是通讯录，包括照片、APP，甚至是桌面配置都是直接恢复。</p><p>第三种情况，就看不同厂商是否对自己有足够的自信了，目前来看，花费的成本和第四种几乎一样。</p><p>第四种情况，换机成本几乎是相当于最大。需要重新导入通讯录，重新安装APP。最痛苦的无疑是安装APP的过程，需要对照旧手机的APP列表，一个一个从应用商店找到，然后安装。当然，也可以选择用到的时候再安装，可能出现的情况就是，着急用了，才发现手机上没有这个应用，然后安装，安装好之后发现忘记账号密码了，还得再找回密码。如果是有好几个手机号的时候，可能连哪个手机号注册的都忘了。</p><p>这里不得不说一下，苹果提供了一个叫做Move To IOS的应用，功能是从Android安装之后，直接将Android机的数据拷贝到苹果机。想法很好，奈何，奈何，Android的移动市场都搜不到这个应用，从苹果官网下载后，装上了，结果发现根本没法用。</p><p>这些都是明面上的一些成本，还有隐形的成本。比如，我最近都在使用极客时间，旧手机中下载了很多视频课程，但是再更换新手机的过程中发现，新手机还得再下载一次。幸好现在宽带普及度很高，流量也是好几十G，要是以前，这些时间成本就都是需要转换成金钱成本了。</p><p>深思一下，既然第一、二种更换手机的场景，换机成本几乎为0，为什么第三、四种，成本就会无限大。如果说第四种情况，跨越操作系统了，成本大还能够接收，但是第三种情况，都是一样的操作系统，为什么不能一键换机呢？（接下来就是我的胡说八道的时间了）</p><p><strong>原因无非就是这些厂商不够自信。</strong> 他们心理活动应该是这个样子的，万一给用户提供了一键换机，让他们发现别人的手机更好用怎么办？这么轻松就能够换机，换过去不回来了，那我岂不是损失了一个客户，就让他们在我的这个圈子里玩吧。</p><p>在云应用的领域，有一个协议叫做OpenTracing，这个协议是规定了调用链数据格式，把各个调用链监控服务（比如Zipkin、TraceView、SkyWalking、DataDog、Jaeger等）接口数据做了统一定义。只要遵守协议了，就能像乐高一样，可以实现一定程度的自由组合，随意搭建自己的调用链监控服务。这样，原来各自为政的项目，就能够让用户自由选择，更换成本几乎为0。</p><p>从历史上看，所有的事物最终都会 <strong>趋于一致，又各有千秋</strong>。比如，秦始皇统一度量衡、书同文、车同轨，这是打破了各方诸侯国文化不同的屏障，减少隔阂。大家可以自由的交流。但是不是说秦老大的书同文政策一出，各方人民就都被格式化成相同的了，各自还是有自己的特色。车同轨了，但是车的外观和内饰还是各有不同。</p><p>那是不是可以这样展望，在之后的几年间，各大厂商意识到换机的高昂成本，达成一致，推出一项一键换机的服务，在接入这个服务的厂商之间更换手机，无论什么品牌、什么型号、什么系统，都能够0成本换机，包括通讯录、文件、应用、铃声等等。然后，各大厂商各自发展自己的特色服务，专心种植自己的梧桐树，提升用户体验。做好这些，剩下的就交给市场。</p><p>如果真能做到，那就是手机领域的天下大同社会了，岂不美哉。。。</p><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/the-cost-to-change-phone/">瞎说八道之更换手机的成本</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="https://blog.csdn.net/conansix/article/details/102986989" target="_blank" rel="noopener">瞎说八道之更换手机的成本</a></p>]]></content>
    
    <summary type="html">
    
      现在手机越来越便宜，换手机是比较常见的一件事，所以各大厂商为了降低更换手机的成本，也是各种手段费尽心思：一键换机、云账号。。。但是这些方式都建立在一个基础上，就是同品牌手机才能用。如果是跨品牌换机，那真是要经历九九八十一难，百转千回才能顺利使用新机。像我这种懒人，可能还要在很长一段时间继续使用旧手机。
    
    </summary>
    
    
      <category term="闲聊" scheme="https://www.howardliu.cn/categories/%E9%97%B2%E8%81%8A/"/>
    
    
      <category term="闲聊" scheme="https://www.howardliu.cn/tags/%E9%97%B2%E8%81%8A/"/>
    
      <category term="成本" scheme="https://www.howardliu.cn/tags/%E6%88%90%E6%9C%AC/"/>
    
      <category term="生活" scheme="https://www.howardliu.cn/tags/%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
  <entry>
    <title>蓝绿部署、金丝雀发布（灰度发布）、AB测试</title>
    <link href="https://www.howardliu.cn/blue-green-deployments-a-b-testing-and-canary-releases/"/>
    <id>https://www.howardliu.cn/blue-green-deployments-a-b-testing-and-canary-releases/</id>
    <published>2019-10-26T09:00:00.000Z</published>
    <updated>2020-01-27T09:58:54.834Z</updated>
    
    <content type="html"><![CDATA[<p>随着微服务架构的普及，线上服务越来越多，随之而来的就是部署越来越频繁；随着互联网行业的兴旺，产品迭代的频率也是越来越快，服务上线速度逐步提升。有上线、有部署，就有风险。有风险，就对业务有影响，然后就有了一系列减少这种风险的部署方案：蓝绿部署、金丝雀发布（灰度发布），也有适应产品迭代频率的AB测试。</p><p>本文主要是简单解释下这几个概念，帮助自己理解，如果有错误，请大佬们斧正。</p><a id="more"></a><h1 id="蓝绿部署"><a href="#蓝绿部署" class="headerlink" title="蓝绿部署"></a>蓝绿部署</h1><p>蓝绿色部署是一种通过运行两个相同的称为 BLUE 和 GREEN 的生产环境来减少停机时间和降低风险的技术。</p><p>蓝绿部署，以颜色命名，简单的理解就是，线上有两套集群环境，在架构图中，一套标记成蓝色，称为蓝色集群BLUE；一套标记为绿色，称为绿色集群GREEN。通过将流量引入两个集群，完成系统升级切换。</p><p><img src="https://www.howardliu.cn/images/op/blue_green_deployments.png" alt="blue green deployments"></p><p>步骤一：部署绿色集群，这个时候是初始状态，蓝色集群承担全部责任，接收全部流量，等待被替换。绿色集群刚刚部署，还没有投入使用，流量为0，等待验证和上线。</p><p>步骤二：蓝色集群流量不变，向绿色集群引入流量。这个过程可以分成几个阶段完成。第一个阶段，引入少量非实时流量，仅用于数据测试；第二个阶段，引入全部实时流量，用于做系统验证。</p><p>步骤三：切断向蓝色集群引入流量，将全部流量引入绿色集群。这个时候，绿色集群已经承担全部责任，接收全部流量。这个过程也可以分阶段操作。第一个阶段，平衡蓝色和绿色集群流量，也就是蓝色和绿色集群一同承担职责；第二个阶段，切断蓝色集群流量，流量全部写入绿色集群。是否采用分阶段操作，完全看升级的功能是否是破坏性的，是否可兼容。</p><p>步骤四：监控系统运行，这个过程是必要的。因为没有人能够保证测试时100%的覆盖的，所以新集群可能会出现这样那样、或大或小的问题，如果评估需要回滚，就需要将全部流量切换到蓝色集群。也完成了版本回滚。</p><h1 id="金丝雀发布（灰度发布）"><a href="#金丝雀发布（灰度发布）" class="headerlink" title="金丝雀发布（灰度发布）"></a>金丝雀发布（灰度发布）</h1><p>金丝雀发布，与蓝绿部署不同的是，它不是非黑即白的部署方式，所以又称为灰度发布。它能够缓慢的将修改推广到一小部分用户，验证没有问题后，再推广到全部用户，以降低生产环境引入新功能带来的风险。</p><p><img src="https://www.howardliu.cn/images/op/canarydeployment.png" alt="canary releases"></p><p>步骤一：将流量从待部署节点移出，更新该节点服务到待发布状态，将该节点称为金丝雀节点；</p><p>步骤二：根据不同策略，将流量引入金丝雀节点。策略可以根据情况指定，比如随机样本策略（随机引入）、狗粮策略（就是内部用户或员工先尝鲜）、分区策略（不同区域用户使用不同版本）、用户特征策略（这种比较复杂，需要根据用户个人资料和特征进行分流，类似于千人千面）；</p><p>步骤三：金丝雀节点验证通过后，选取更多的节点称为金丝雀节点，重复步骤一和步骤二，直到所有节点全部更新</p><h1 id="AB测试"><a href="#AB测试" class="headerlink" title="AB测试"></a>AB测试</h1><p>AB测试和上面两种发布方式不是一个范围的概念，它是为了进行效果验证的手段，其他两种是为了实现线上平稳发布的手段，这里把他们放在一起说，是因为这三个概念很容易弄混。</p><p>AB测试是线上同时运行多个不同版本的服务，这些服务更多的是用户侧的体验不同，比如页面布局、按钮颜色，交互方式等，通常底层业务逻辑还是一样的，也就是通常说的换汤不换药。</p><p><img src="https://www.howardliu.cn/images/op/abtesting.png" alt="A/B Testing"></p><p>这个没有具体的步骤（也可以采用金丝雀部署的步骤，只不过不是全量更新），根据策略（这个策略可以是金丝雀分布中的策略一致），将一部分流量引入A版本，另外一部分流量引入B版本，也可能出现CDEF版本。然后相关人员通过分析不同版本的实际效果，选出最优解。最优解可能是一个版本获胜，取代另一个版本，也可能是催生出更多的版本，服务于用户，还有可能是多个版本在不同区域同时提供服务。</p><h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><p>这里总结一下：</p><table><thead><tr><th>名称</th><th>特点</th><th>优势</th><th>劣势</th></tr></thead><tbody><tr><td>蓝绿部署</td><td>同时存在两个集群，两个集群中只有一个集群真正提供服务，另外一个集群测试、验证或待命</td><td>服务文档，版本回退简单，适用于各种场景的升级，大版本不兼容升级的或迭代兼容升级</td><td>浪费硬件资源，需要同时有两个集群，如果集群比较大，比如有1000个节点，这种方式几乎不可用</td></tr><tr><td>金丝雀部署</td><td>逐点部署，逐步替换线上服务</td><td>小步快跑，快速迭代</td><td>只能适用于兼容迭代的方式，如果是大版本不兼容的场景，就没办法使用这种方式了</td></tr></tbody></table><p>AB测试和上面两个不是一个范畴，不做比较。但是需要说明的一点，AB测试可以采用上面两种部署方式的手法。</p><p>参考：<br><a href="https://docs.cloudfoundry.org/devguide/deploy-apps/blue-green.html" target="_blank" rel="noopener">Using Blue-Green Deployment to Reduce Downtime and Risk</a><br><a href="https://martinfowler.com/bliki/BlueGreenDeployment.html" target="_blank" rel="noopener">Blue Green Deployment</a><br><a href="https://blog.christianposta.com/deploy/blue-green-deployments-a-b-testing-and-canary-releases/" target="_blank" rel="noopener">Blue-green Deployments, A/B Testing, and Canary Releases</a><br><a href="https://martinfowler.com/bliki/CanaryRelease.html" target="_blank" rel="noopener">Canary Release</a></p><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/blue-green-deployments-a-b-testing-and-canary-releases/">蓝绿部署、金丝雀发布（灰度发布）、AB测试</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="https://blog.csdn.net/liuxinghao/article/details/102758824" target="_blank" rel="noopener">蓝绿部署、金丝雀发布（灰度发布）、AB测试</a></p>]]></content>
    
    <summary type="html">
    
      随着微服务架构的普及，线上服务越来越多，随之而来的就是部署越来越频繁；随着互联网行业的兴旺，产品迭代的频率也是越来越快，服务上线速度逐步提升。有上线、有部署，就有风险。有风险，就对业务有影响，然后就有了一系列减少这种风险的部署方案：蓝绿部署、金丝雀发布（灰度发布），也有适应产品迭代频率的AB测试。
    
    </summary>
    
    
      <category term="devops" scheme="https://www.howardliu.cn/categories/devops/"/>
    
    
      <category term="DevOps" scheme="https://www.howardliu.cn/tags/DevOps/"/>
    
      <category term="部署" scheme="https://www.howardliu.cn/tags/%E9%83%A8%E7%BD%B2/"/>
    
      <category term="系统运维" scheme="https://www.howardliu.cn/tags/%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>HTTP的响应头Vary</title>
    <link href="https://www.howardliu.cn/http/http-response-header-vary/"/>
    <id>https://www.howardliu.cn/http/http-response-header-vary/</id>
    <published>2019-10-22T12:15:52.000Z</published>
    <updated>2020-01-27T09:49:02.167Z</updated>
    
    <content type="html"><![CDATA[<p>写在前面：Vary 是一个HTTP响应头部信息，它决定了对于未来的一个请求头，应该用一个缓存的回复(response)还是向源服务器请求一个新的回复。它被服务器用来表明在 content negotiation algorithm（内容协商算法）中选择一个资源代表的时候应该使用哪些头部信息（headers）。</p><a id="more"></a><h2 id="描述问题"><a href="#描述问题" class="headerlink" title="描述问题"></a>描述问题</h2><p>今天碰到一个问题，需要根据不同的UA跳转不同的页面，实现手机访问网站跳转到Wap页面，PC端访问跳转到PC端的页面。根据之前的了解，在Nginx做了UA区分，实现了根据UA跳转页面。</p><p>结果，解决一个问题，同时引入一个更大问题。</p><p>CND中缓存的文档出现了错位，因为请求地址是相同的，结果有时候存储的是PC页面，有时候存储的是WAP页面。</p><h2 id="分析问题"><a href="#分析问题" class="headerlink" title="分析问题"></a>分析问题</h2><p>CDN缓存了错误的数据，是因为相同地址、不同UA，Nginx返回的内容是不一样的。</p><p>在CDN缓存过期时，UA是手机，Nginx就会返回Wap页面，缓存的就是Wap页面。</p><p>在CDN缓存过期时，UA是PC，Nginx就会返回PC页面，缓存的就是PC页面。</p><h2 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h2><p>针对这种架构，有两种解决方法，一种是，在PC页面中增加UA校验，如果是手机，就跳转页面，代码如下：</p><pre><code class="js">var originHref = window.location.href;var afterSchema = originHref.indexOf(&quot;://&quot;) + 3;var domainEnd = originHref.indexOf(&quot;/&quot;, afterSchema);var domain = originHref.substring(afterSchema, domainEnd);if (domain !== &#39;m.howardliu.cn&#39;) {    if (navigator) {        var ua = navigator.userAgent.toLowerCase();        if (/mobile|android|iphone|ipad|phone/i.test(ua)) {            window.location.href = originHref.substring(0, afterSchema) + &#39;m.howardliu.cn&#39; + originHref.substring(domainEnd);        }    }}</code></pre><p>这种方式可以解决问题，但是不够优雅，因为需要浏览器先加载PC的页面，然后再进行检查，也就是会加载两个页面。如果网速比较慢时，就会在PC站驻留比较长的时间，然后才跳转到wap站，用户体验差。</p><p>那就得只能在CDN进行处理，CDN服务商提供的解决方案是，在响应头中增加<code>vary: Accept-Encoding, User-Agent</code>，具体配置是在nginx中配置：</p><pre><code class="conf">add_header Vary &quot;Accept-Encoding, User-Agent&quot;;</code></pre><p>果然解决了问题。</p><p>学非探其花，要深拔其根。http响应头中的<code>Vary</code>是什么作用呢？为什么配置了它之后，CDN就能够区分不同的响应信息了？</p><p><a href="https://tools.ietf.org/html/rfc7231" target="_blank" rel="noopener">RFC 7231</a>中的<a href="https://tools.ietf.org/html/rfc7231#section-7.1.4" target="_blank" rel="noopener">7.1.4.  Vary</a>给出了解释：</p><blockquote><p>The “Vary” header field in a response describes what parts of a request message, aside from the method, Host header field, and request target, might influence the origin server’s process for selecting and representing this response. The value consists of either a single asterisk (“*”) or a list of header field names (case-insensitive).</p><p>Vary = “*” / 1#field-name</p><p>A Vary field value of “” signals that anything about the request might play a role in selecting the response representation, possibly including elements outside the message syntax (e.g., the client’s network address). A recipient will not be able to determine whether this response is appropriate for a later request without forwarding the request to the origin server. A proxy MUST NOT generate a Vary field with a “” value.</p><p>A Vary field value consisting of a comma-separated list of names indicates that the named request header fields, known as the selecting header fields, might have a role in selecting the representation. The potential selecting header fields are not limited to those defined by this specification.</p><p>For example, a response that contains</p><p>Vary: accept-encoding, accept-language</p><p>indicates that the origin server might have used the request’s Accept-Encoding and Accept-Language fields (or lack thereof) as determining factors while choosing the content for this response.</p><p>An origin server might send Vary with a list of fields for two purposes:</p><ol><li>To inform cache recipients that they MUST NOT use this response to satisfy a later request unless the later request has the same values for the listed fields as the original request (Section 4.1 of [RFC7234]). In other words, Vary expands the cache key required to match a new request to the stored cache entry.</li><li>To inform user agent recipients that this response is subject to content negotiation (Section 5.3) and that a different representation might be sent in a subsequent request if additional parameters are provided in the listed header fields (proactive negotiation).</li></ol><p>An origin server SHOULD send a Vary header field when its algorithm for selecting a representation varies based on aspects of the request message other than the method and request target, unless the variance cannot be crossed or the origin server has been deliberately configured to prevent cache transparency. For example, there is no need to send the Authorization field name in Vary because reuse across users is constrained by the field definition (Section 4.2 of [RFC7235]). Likewise, an origin server might use Cache-Control directives (Section 5.2 of [RFC7234]) to supplant Vary if it considers the variance less significant than the performance cost of Vary’s impact on caching.</p></blockquote><p>简单来说，<code>Vary</code>是http的一种约定，当客户端与服务端之间，针对相同的URL请求，服务端存在不同内容的响应，且响应内容是根据请求头的不同，返回不同时，就需要<code>Vary</code>指定需要区分的请求头了。</p><p>比如，上面提到的需要根据UA来区分，那响应信息里面就需要包括：<code>Vary: User-Agent</code>。如果需要根据<code>Accept-Encoding</code>和<code>Accept-Language</code>进行区分，响应头就需要包含<code>Vary: Accept-Encoding, Accept-Language</code>。这样做，其实是为了客户端能够很好的对结果缓存。</p><p>总结一下：</p><ol><li><code>Vary</code>是服务端添加在响应头的信息</li><li><code>Vary</code>的内容来源于请求头</li><li>实现完整协议的客户端（包括浏览器和缓存服务器）缓存数据时，会将<code>Vary</code>一起缓存。</li></ol>]]></content>
    
    <summary type="html">
    
      Vary 是一个HTTP响应头部信息，它决定了对于未来的一个请求头，应该用一个缓存的回复(response)还是向源服务器请求一个新的回复。它被服务器用来表明在 content negotiation algorithm（内容协商算法）中选择一个资源代表的时候应该使用哪些头部信息（headers）。
    
    </summary>
    
    
      <category term="http" scheme="https://www.howardliu.cn/categories/http/"/>
    
    
      <category term="概念" scheme="https://www.howardliu.cn/tags/%E6%A6%82%E5%BF%B5/"/>
    
      <category term="网络" scheme="https://www.howardliu.cn/tags/%E7%BD%91%E7%BB%9C/"/>
    
      <category term="http" scheme="https://www.howardliu.cn/tags/http/"/>
    
  </entry>
  
  <entry>
    <title>spring-cloud-config 非对称加密 keystore 文件加载异常</title>
    <link href="https://www.howardliu.cn/spring-cloud/spring-cloud-config-encrypt-keystore-path-error/"/>
    <id>https://www.howardliu.cn/spring-cloud/spring-cloud-config-encrypt-keystore-path-error/</id>
    <published>2017-10-12T01:35:44.000Z</published>
    <updated>2020-01-27T10:04:45.778Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Spring Cloud Config是Spring Cloud一个全新的项目，依赖版本仓库（比如Git、SVN）实现分布式系统外部配置的集中管理。<br>文中Spring Cloud的版本是Dalston.SR4，可能在其他之后的版本有修改。</p></blockquote><p>最近这段时间在学习Spring Cloud，准备在项目中使用。Spring Cloud不能简单的算是一个框架，而应该认为是一个微服务的整体解决方案，它集成了Spring Boot、Netflix等等很多非常优秀的框架，很多组件开箱即用。也正是因为它集成了这么多框架，致使其版本不够稳定，即使是SR的版本，也存在这样那样的问题。甚至有的上一个版本没有问题，这个版本就出问题了。</p><a id="more"></a><p>因为配置内容可能涉及到某些敏感信息，所以可以简单的在Spring Cloud Config中使用非对称加密实现敏感信息的安全存储。但是在使用过程中，却碰到一些奇葩的问题，无论是官方文档还是一些大神的博文，都没有明确解决。</p><p>当然，这个问题的出现与我的配置有关，很多人可能并没有碰到。这里把问题描述下，万一有人和我一样配置，也出现问题，就可以帮助别人节省点时间了。</p><h1 id="1-正确姿势"><a href="#1-正确姿势" class="headerlink" title="1. 正确姿势"></a>1. 正确姿势</h1><p>Spring Cloud Config的非对称加密可以使用RSA加密方式，通过JDK自带的keytool生产秘匙对，对敏感信息进行加密解密。</p><p>生产秘匙对：</p><pre><code class="sh">keytool -genkeypair -alias mytestkey -keyalg RSA -dname &quot;CN=Web Server,OU=China,O=www.howardliu.cn,L=Beijing,S=Beijing,C=China&quot; -keypass changeme -keystore config-service.jks -storepass letmein</code></pre><p>将生成的keystore文件拷贝到<code>src/main/resources</code>目录中，然后在配置文件<code>bootstrap.yml</code>中配置信息秘匙对信息：</p><pre><code>encrypt:  key-store:    location: classpath:/config-service.jks    alias: mytestkey    password: letmein    secret: changeme</code></pre><p>在<code>pom.xml</code>中加入了如下配置：</p><pre><code class="xml">&lt;build&gt;    &lt;resources&gt;        &lt;resource&gt;            &lt;directory&gt;src/main/resources&lt;/directory&gt;            &lt;filtering&gt;true&lt;/filtering&gt;            &lt;excludes&gt;                &lt;exclude&gt;**/*.jks&lt;/exclude&gt;            &lt;/excludes&gt;        &lt;/resource&gt;        &lt;resource&gt;            &lt;directory&gt;src/main/resources&lt;/directory&gt;            &lt;filtering&gt;false&lt;/filtering&gt;            &lt;includes&gt;                &lt;include&gt;**/*.jks&lt;/include&gt;            &lt;/includes&gt;        &lt;/resource&gt;    &lt;/resources&gt;&lt;/build&gt;</code></pre><p>然后启动应用，通过访问<a href="http://localhost:8888/encrypt/status" target="_blank" rel="noopener">http://localhost:8888/encrypt/status</a> ，返回<code>{&quot;status&quot;:&quot;OK&quot;}</code>，说明配置正确。这个使用就可以通过访问<code>/encrypt</code>和<code>decrypt</code>两个endpoints对指定信息进行加密解密了。</p><p>这个项目的代码在<a href="https://github.com/howardliu-cn/spring-learning/tree/master/micro-service/config-service" target="_blank" rel="noopener">https://github.com/howardliu-cn/spring-learning/tree/master/micro-service/config-service</a>。</p><h1 id="2-错误姿势"><a href="#2-错误姿势" class="headerlink" title="2. 错误姿势"></a>2. 错误姿势</h1><p>刚开始学习的时候并不是如上面那样顺利，碰到了一些问题。</p><h2 id="2-1-encrypt-key-store-配置位置引起的加密信息未解密"><a href="#2-1-encrypt-key-store-配置位置引起的加密信息未解密" class="headerlink" title="2.1 encrypt.key-store.*配置位置引起的加密信息未解密"></a>2.1 encrypt.key-store.*配置位置引起的加密信息未解密</h2><p>最开始是将<code>encrypt.key-store.*</code>配置在<code>application.yml</code>中，这个时候通过访问<a href="http://localhost:8888/encrypt/status" target="_blank" rel="noopener">http://localhost:8888/encrypt/status</a> ，返回<code>{&quot;status&quot;:&quot;OK&quot;}</code>。原本以为配置成功，结果在客户端获取加密信息的时候，返回的结果并没有解密，只是将<code>{cipher}</code>去掉了。</p><p>这个错误算是比较容易排查，Spring Cloud的主要配置文件就<code>application.yml</code>和<code>bootstrap.yml</code>两个，因为客户端中需要把Spring Cloud Config的配置信息放在<code>bootstrap.yml</code>中，就把<code>encrypt.key-store.*</code>配置<code>bootstrap.yml</code>试试，结果果然成功。</p><blockquote><p>这个错误出现的时候，<code>encrypt.key-store.location</code>使用的是绝对路径<code>file://${user.home}/work/projects/spring-learning/micro-service/config-service/src/main/resources/config-service.jks</code>。</p></blockquote><h2 id="2-2-maven-filter对keystore文件的污染"><a href="#2-2-maven-filter对keystore文件的污染" class="headerlink" title="2.2 maven filter对keystore文件的污染"></a>2.2 maven filter对keystore文件的污染</h2><p>因为对Spring Cloud Config服务端定义了<code>/info</code>这个endpoints这个信息，需要用到maven的构建内容，所以在pom.xml中加入了filter信息：</p><pre><code class="xml">&lt;build&gt;    &lt;resources&gt;        &lt;resource&gt;            &lt;directory&gt;src/main/resources&lt;/directory&gt;            &lt;filtering&gt;true&lt;/filtering&gt;        &lt;/resource&gt;    &lt;/resources&gt;&lt;/build&gt;</code></pre><p>结果在访问 <a href="http://localhost:8888/encrypt/status" target="_blank" rel="noopener">http://localhost:8888/encrypt/status</a> 的时候，返回错误信息：</p><pre><code>Thu Oct 12 13:55:51 CST 2017There was an unexpected error (type=Internal Server Error, status=500).Cannot load keys from store: class path resource [config-service.jks]</code></pre><p>服务端报错如下：</p><pre><code>2017-10-12 13:55:50.991 ERROR 14733 --- [nio-8888-exec-1] o.a.c.c.C.[.[.[/].[dispatcherServlet]    : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.IllegalStateException: Cannot load keys from store: class path resource [config-service.jks]] with root causejava.io.IOException: Invalid keystore format    at sun.security.provider.JavaKeyStore.engineLoad(JavaKeyStore.java:658) ~[na:1.8.0_144]    at sun.security.provider.JavaKeyStore$JKS.engineLoad(JavaKeyStore.java:56) ~[na:1.8.0_144]    at sun.security.provider.KeyStoreDelegator.engineLoad(KeyStoreDelegator.java:224) ~[na:1.8.0_144]    at sun.security.provider.JavaKeyStore$DualFormatJKS.engineLoad(JavaKeyStore.java:70) ~[na:1.8.0_144]    at java.security.KeyStore.load(KeyStore.java:1445) ~[na:1.8.0_144]    at org.springframework.security.rsa.crypto.KeyStoreKeyFactory.getKeyPair(KeyStoreKeyFactory.java:53) ~[spring-security-rsa-1.0.3.RELEASE.jar:na]    at org.springframework.cloud.config.server.encryption.KeyStoreTextEncryptorLocator.locate(KeyStoreTextEncryptorLocator.java:84) ~[spring-cloud-config-server-1.3.3.RELEASE.jar:1.3.3.RELEASE]    at org.springframework.cloud.config.server.encryption.EncryptionController.checkEncryptorInstalled(EncryptionController.java:167) ~[spring-cloud-config-server-1.3.3.RELEASE.jar:1.3.3.RELEASE]    at org.springframework.cloud.config.server.encryption.EncryptionController.status(EncryptionController.java:113) ~[spring-cloud-config-server-1.3.3.RELEASE.jar:1.3.3.RELEASE]    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_144]    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_144]    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144]    at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144]    at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:967) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at javax.servlet.http.HttpServlet.service(HttpServlet.java:635) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) ~[spring-webmvc-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at javax.servlet.http.HttpServlet.service(HttpServlet.java:742) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) ~[tomcat-embed-websocket-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55) ~[spring-boot-1.5.4.RELEASE.jar:1.5.4.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:110) ~[spring-boot-actuator-1.5.4.RELEASE.jar:1.5.4.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilterInternal(BasicAuthenticationFilter.java:215) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:64) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177) ~[spring-security-web-4.2.3.RELEASE.jar:4.2.3.RELEASE]    at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:262) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:105) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:81) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:106) ~[spring-boot-actuator-1.5.4.RELEASE.jar:1.5.4.RELEASE]    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.9.RELEASE.jar:4.3.9.RELEASE]    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:198) ~[tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:478) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:140) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:80) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:87) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:799) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:861) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1455) [tomcat-embed-core-8.5.15.jar:8.5.15]    at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) [tomcat-embed-core-8.5.15.jar:8.5.15]    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_144]    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_144]    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-embed-core-8.5.15.jar:8.5.15]    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]</code></pre><p>从错误堆栈信息中可以看到，是秘匙对文件格式错误。但是并没有对文件进行修改，何来错误一说。本着怀疑一切的精神，重新生成了一份keystore文件，然后执行，仍然报错。</p><p>那问题就来了，文件格式没有问题，结果却报文件格式有问题，那就只能是编译的时候文件被修改了。于是将<code>encrypt.key-store.location</code>使用绝对路径<code>file://${user.home}/work/projects/spring-learning/micro-service/config-service/src/main/resources/config-service.jks</code>进行实验，结果成功解析。然后将路径改为编译后target中的文件路径，就报上面的错误。果然是maven编译的时候报错。</p><p>就在<a href="https://stackoverflow.com/questions/30992424/tomcat-java-io-ioexception-invalid-keystore-format-when-loading-keystore-via-cl" target="_blank" rel="noopener">stackoverflow</a>上找到解决方式，是maven的filter污染了文件。就把pom.xml中的配置改为这样：</p><pre><code class="xml">&lt;build&gt;    &lt;resources&gt;        &lt;resource&gt;            &lt;directory&gt;src/main/resources&lt;/directory&gt;            &lt;filtering&gt;true&lt;/filtering&gt;            &lt;excludes&gt;                &lt;exclude&gt;**/*.jks&lt;/exclude&gt;            &lt;/excludes&gt;        &lt;/resource&gt;        &lt;resource&gt;            &lt;directory&gt;src/main/resources&lt;/directory&gt;            &lt;filtering&gt;false&lt;/filtering&gt;            &lt;includes&gt;                &lt;include&gt;**/*.jks&lt;/include&gt;            &lt;/includes&gt;        &lt;/resource&gt;    &lt;/resources&gt;&lt;/build&gt;</code></pre><p>再次测试，果然成功。不过遗憾的是，还不确定filter为什么会污染文件，后面慢慢研究，等找到原因再把结果放在下面。</p><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/spring-cloud/spring-cloud-config-encrypt-keystore-path-error/">spring-cloud-config 非对称加密 keystore 文件加载异常</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/78216201" target="_blank" rel="noopener">spring-cloud-config 非对称加密 keystore 文件加载异常</a></p>]]></content>
    
    <summary type="html">
    
      因为配置内容可能涉及到某些敏感信息，所以可以简单的在Spring Cloud Config中使用非对称加密实现敏感信息的安全存储。但是在使用过程中，却碰到一些奇葩的问题，无论是官方文档还是一些大神的博文，都没有明确解决。当然，这个问题的出现与我的配置有关，很多人可能并没有碰到。这里把问题描述下，万一有人和我一样配置，也出现问题，就可以帮助别人节省点时间了。
    
    </summary>
    
    
      <category term="spring-cloud" scheme="https://www.howardliu.cn/categories/spring-cloud/"/>
    
      <category term="spring-cloud-config" scheme="https://www.howardliu.cn/categories/spring-cloud/spring-cloud-config/"/>
    
      <category term="error" scheme="https://www.howardliu.cn/categories/spring-cloud/spring-cloud-config/error/"/>
    
    
      <category term="spring-cloud" scheme="https://www.howardliu.cn/tags/spring-cloud/"/>
    
      <category term="spring-cloud-config" scheme="https://www.howardliu.cn/tags/spring-cloud-config/"/>
    
      <category term="error" scheme="https://www.howardliu.cn/tags/error/"/>
    
      <category term="micro-service" scheme="https://www.howardliu.cn/tags/micro-service/"/>
    
  </entry>
  
  <entry>
    <title>代码质量管理：SonarQube + Jenkins Pipeline配置</title>
    <link href="https://www.howardliu.cn/sonarqube-jenkins-pipeline/"/>
    <id>https://www.howardliu.cn/sonarqube-jenkins-pipeline/</id>
    <published>2017-09-13T07:03:21.000Z</published>
    <updated>2020-01-27T09:58:24.128Z</updated>
    
    <content type="html"><![CDATA[<p>前段时间对自己的项目进行代码质量扫描，曾经以为自己的代码质量算是不错的，结果发现一堆的bug或者smell code，灵魂受到1w点伤害。</p><p>可以想到，在时间紧、任务中的情况下，代码质量绝对是不能够保证的，虽然功能算是完整，但是可能就在某个隐藏的角落，就有无数的bug在潜伏着，所以有时间的话都对自己的代码进行代码质量检查吧。虽然不能保证有完美的代码，但是可以把bug数降低，也可以根据扫描的结果养成良好的编程习惯。</p><p>身为程序员就得严谨。</p><p>闲言碎语不再讲。</p><p>本文主要是介绍通过Jenkins Pipeline与SonarQube集成，对代码进行扫描，这里使用的是Jenkins2.19.1，SonarQube6.4。</p><a id="more"></a><h1 id="1-基础工作"><a href="#1-基础工作" class="headerlink" title="1. 基础工作"></a>1. 基础工作</h1><h2 id="1-1-安装插件"><a href="#1-1-安装插件" class="headerlink" title="1.1 安装插件"></a>1.1 安装插件</h2><p>在Jenkins管理界面中的 系统管理-&gt;插件管理 安装最新的 <em>SonarQube plugin</em> 插件，并重启Jenkins：</p><p><img src="https://www.howardliu.cn/images/code_quality/jenkins_available-plugins_sonarqube.png" alt="SonarQube plugin"></p><h2 id="1-2-配置-安装SonarQube-Scanner"><a href="#1-2-配置-安装SonarQube-Scanner" class="headerlink" title="1.2 配置/安装SonarQube Scanner"></a>1.2 配置/安装SonarQube Scanner</h2><p>等待重启之后，在 系统管理-&gt;Global Tool Configuration 中配置/安装最新的SonarQube Scanner：</p><p><img src="https://www.howardliu.cn/images/code_quality/jenkins_global_tool_sonarqube_scanner_1.png" alt="SonarQube plugin"></p><p>可以选择自动安装，这样会在需要用到的时候自动从默认地址安装到默认路径，不需要手动其下载安装，非常方便。当然，也可以自己去网上下载，如果是自己下载的话，需要把<em>自动安装</em>的勾去掉，然后填上自己下载的SonarQube Scanner运行包路径：</p><p><img src="https://www.howardliu.cn/images/code_quality/jenkins_global_tool_sonarqube_scanner_2.png" alt="SonarQube plugin"></p><h2 id="1-3-配置SonarQube服务"><a href="#1-3-配置SonarQube服务" class="headerlink" title="1.3 配置SonarQube服务"></a>1.3 配置SonarQube服务</h2><p>因为SonarQube Scanner工具需要把扫描的代码及结果发送到SonarQube服务器上，所以需要配置SonarQube服务地址。</p><p>在 系统管理-&gt;系统设置 中增加 <em>SonarQube servers</em> 相关配置：</p><p><img src="https://www.howardliu.cn/images/code_quality/jenkins_system_setting_sonarqube_server.png" alt="SonarQube plugin"></p><p>至此，基础配置工作就结束了，下面可以开始配置扫描任务了。</p><h1 id="2-配置待扫描项目"><a href="#2-配置待扫描项目" class="headerlink" title="2. 配置待扫描项目"></a>2. 配置待扫描项目</h1><p>首先创建一个Jenkins Pipeline项目:</p><p><img src="https://www.howardliu.cn/images/code_quality/jenkins_new_project_pipeline.png" alt="Pipeline Project"></p><p>然后修改Pipeline脚本，比如：</p><pre><code class="Groovy">node {    stage(&#39;SCM&#39;) {        git([url: &#39;https://github.com/howardliu-cn/cynomys.git&#39;])    }    stage(&#39;SonarQube analysis&#39;) {        def sonarqubeScannerHome = tool name: &#39;SonarQube Scanner&#39;        withSonarQubeEnv(&#39;SonarQube&#39;) {            sh &quot;${sonarqubeScannerHome}/bin/sonar-scanner&quot;        }    }}</code></pre><p>如果需要指定从某个分支复制代码，可以增加<code>branch</code>参数；如果使用ssh方式复制代码，需要通过<code>credentialsId</code>参数配置Jenkins中配置好的秘钥ID。比如：</p><pre><code class="Groovy">node {    stage(&#39;SCM&#39;) {        git([url: &#39;git@10.6.3.213:RD/messenger.git&#39;, branch: &#39;develop&#39;, credentialsId: &#39;fae8b1b9-8818-48e9-a28a-24b928015a6c&#39;])    }    stage(&#39;SonarQube analysis&#39;) {        def sonarqubeScannerHome = tool name: &#39;SonarQube Scanner&#39;        withSonarQubeEnv(&#39;SonarQube&#39;) {            sh &quot;${sonarqubeScannerHome}/bin/sonar-scanner&quot;        }    }}</code></pre><p>这两种方式都需要在项目的根路径下面有一个<code>sonar-project.properties</code>文件，其内容如下：</p><pre><code># must be unique in a given SonarQube instancesonar.projectKey=cynomys:0.0.1# this is the name and version displayed in the SonarQube UI. Was mandatory prior to SonarQube 6.1.sonar.projectName=cynomyssonar.projectVersion=0.0.1# Path is relative to the sonar-project.properties file. Replace &quot;\&quot; by &quot;/&quot; on Windows.# This property is optional if sonar.modules is set.sonar.sources=.sonar.exclusions=**/test/**,**/target/**sonar.java.source=1.8sonar.java.target=1.8# Encoding of the source code. Default is default system encodingsonar.sourceEncoding=UTF-8</code></pre><p>除了通过Pipeline的方式之外，还可以创建普通项目进行扫描，可以参考<a href="https://support.cloudbees.com/hc/en-us/articles/204947684-How-can-I-Run-A-Sonar-Analysis-Without-Maven-" target="_blank" rel="noopener">这里</a>。</p><p>修改完成，保存，就可以开始扫描了。</p><h1 id="3-开始扫描"><a href="#3-开始扫描" class="headerlink" title="3. 开始扫描"></a>3. 开始扫描</h1><p>直接在项目页面点击立即构建，就会开始扫描。然后登录SonarQube服务，就能够看到代码质量检查的结果了。</p><p><img src="https://www.howardliu.cn/images/code_quality/sonarqube_dashboard_project.png" alt="SonarQube 项目 Dashboard页面"></p><h1 id="4-个人建议"><a href="#4-个人建议" class="headerlink" title="4. 个人建议"></a>4. 个人建议</h1><ol><li>代码质量检查是非常必要的，可以在代码运行之前就可以找到很多bug</li><li>smell code虽然不影响运行，但是在某些情况下对代码的重构、复用、修改造成不必要的影响</li><li>工具只是工具，可以减少低效劳动，但是绝对不会是万能的</li></ol><p>希望之后不会被吐槽是在写bug。。。</p><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/sonarqube-jenkins-pipeline/">代码质量管理：SonarQube + Jenkins Pipeline配置</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/77967158" target="_blank" rel="noopener">代码质量管理：SonarQube + Jenkins Pipeline配置</a></p>]]></content>
    
    <summary type="html">
    
      本文主要是介绍通过Jenkins Pipeline与SonarQube集成，对代码进行扫描，这里使用的是Jenkins2.19.1，SonarQube6.4。
    
    </summary>
    
    
      <category term="cd" scheme="https://www.howardliu.cn/categories/cd/"/>
    
    
      <category term="DevOps" scheme="https://www.howardliu.cn/tags/DevOps/"/>
    
      <category term="CD" scheme="https://www.howardliu.cn/tags/CD/"/>
    
      <category term="code quality" scheme="https://www.howardliu.cn/tags/code-quality/"/>
    
      <category term="SonarQube" scheme="https://www.howardliu.cn/tags/SonarQube/"/>
    
      <category term="Jenkins Pipeline" scheme="https://www.howardliu.cn/tags/Jenkins-Pipeline/"/>
    
  </entry>
  
  <entry>
    <title>java.lang.OutOfMemoryError:GC overhead limit exceeded</title>
    <link href="https://www.howardliu.cn/java/gc-overhead-limit-exceeded/"/>
    <id>https://www.howardliu.cn/java/gc-overhead-limit-exceeded/</id>
    <published>2017-09-09T08:24:59.000Z</published>
    <updated>2020-02-07T06:22:57.139Z</updated>
    
    <content type="html"><![CDATA[<p><code>java.lang.OutOfMemoryError</code>这个错误是比较经典的错误了，经过JDK不断的迭代，服务器硬件的不断升级。。。总之，社会在发展，时代在进步。很多错误已经消失在时代的浪潮中。我也是很久没有见过这个错误了，以至于都以为在Java的世界中不会再碰到这个错误。结果，就在最疏忽的时候碰到了TA，真是，心中一万只神兽奔袭而过，狠狠的践踏了我这颗上了年纪的心脏啊。</p><p>吐槽完毕，言归正传。</p><p>Java刚刚出现的年代，有一个相比于其他语言的优势就是，内存回收机制。不需要明确的调用释放内存的API，java就自动完成，这个过程就是Garbage Collection，简称GC。这对以懒著称的程序猿们来说，绝对是重大利好。但是，凡事有利必有弊，可以肯定的是，Java语言是人创造的，GC也是人编写的代码，绝对不是机器自动完成的。也就是说，GC的过程是另外一群程序猿根据可能出现的情况，预设了GC条件，把符合回收条件的内存空间释放出来。一旦被占用的内存空间不符合释放的条件，GC没办法清理，那就会适时出现<code>java.lang.OutOfMemoryError</code>。这个错误就是提醒我们这群程序猿，写GC程序的程序猿不知道这种情况怎么处理，为了安全也不便处理，谁使用Java就自己看着解决吧。</p><p>说起来，<code>java.lang.OutOfMemoryError</code>有几种分类的，这次碰到的是<code>java.lang.OutOfMemoryError: GC overhead limit exceeded</code>，下面就来说说这种类型的内存溢出。</p><a id="more"></a><p>简单来说，<code>java.lang.OutOfMemoryError: GC overhead limit exceeded</code>发生的原因是，当前已经没有可用内存，经过多次GC之后仍然没能有效释放内存。</p><h1 id="1-原因"><a href="#1-原因" class="headerlink" title="1. 原因"></a>1. 原因</h1><p>众所周知，JVM的GC过程会因为STW，只不过停顿短到不容易感知。当引起停顿时间的98%都是在进行GC，但是结果只能得到小于2%的堆内存恢复时，就会抛出<code>java.lang.OutOfMemoryError: GC overhead limit exceeded</code>这个错误。<a href="https://plumbr.eu" target="_blank" rel="noopener">Plumbr</a>给出一个示意图：</p><p><img src="https://www.howardliu.cn/images/java/OOM-gc-overhead-limit-exceeded-1.png" alt="java.lang.OutOfMemoryError: GC overhead limit exceeded"></p><p>这个错误其实就是空闲内存与GC之间平衡的一个限制，当经过几次GC之后，只有少于2%的内存被释放，也就是很少的空闲内存，可能会再次被快速填充，这样就会触发再一次的GC。这就是一个恶性循环了，CPU大部分的时间在做GC操作，没有时间做具体的业务操作，可能几毫秒的任务需要几分钟都无法完成，整个应用程序就形同虚设了。</p><h1 id="2-示例"><a href="#2-示例" class="headerlink" title="2. 示例"></a>2. 示例</h1><p>从<a href="https://plumbr.eu" target="_blank" rel="noopener">Plumbr</a>上找的一个例子，这里直接给出。</p><pre><code class="java">class Wrapper {  public static void main(String args[]) throws Exception {    Map map = System.getProperties();    Random r = new Random();    while (true) {      map.put(r.nextInt(), &quot;value&quot;);    }  }}</code></pre><p>然后设定堆内存是100m，比如<code>java -Xmx100m -XX:+UseParallelGC Wrapper</code>。不同的系统环境可能需要设置不同的堆内存大小，否则会产生不同的OOM错误。其实也算是好理解，因为<code>java.lang.OutOfMemoryError: GC overhead limit exceeded</code>需要有两个条件：98%的时间和2%的内存。如果这两个条件有一个没有达到，结果Map对象扩容，那就可能出现<code>java.lang.OutOfMemoryError: Java heap space</code>这个错误。</p><h1 id="3-解决方法"><a href="#3-解决方法" class="headerlink" title="3. 解决方法"></a>3. 解决方法</h1><h2 id="3-1-JVM参数"><a href="#3-1-JVM参数" class="headerlink" title="3.1 JVM参数"></a>3.1 JVM参数</h2><p>JVM给出一个参数避免这个错误：<code>-XX:-UseGCOverheadLimit</code>。</p><p>但是，这个参数并不是解决了内存不足的问题，只是将错误发生时间延后，并且替换成<code>java.lang.OutOfMemoryError: Java heap space</code>。</p><h2 id="3-2-堆内存"><a href="#3-2-堆内存" class="headerlink" title="3.2 堆内存"></a>3.2 堆内存</h2><p>还有一个偷懒的方法是：增大堆内存。既然堆内存少了，那就增加堆内存即可。</p><p>但是，这个方法也不是万能的。因为程序里可能有内存泄露。这个时候即使再增大堆内存，也会有用完的时候。</p><p>所以前两个方法都只是治标不治本而已。</p><h2 id="3-3-终极方法"><a href="#3-3-终极方法" class="headerlink" title="3.3 终极方法"></a>3.3 终极方法</h2><p>其实还是有一个终极方法的，而且是治标治本的方法，就是找到占用内存大的地方，把代码优化了，就不会出现这个问题了。</p><p>怎么找到需要优化的代码呢？就是通过heap dump生产jvm快照，通过分析快照找到占用内存大的对象，从而找到代码位置。</p><p>通过设置<code>-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=heapdump</code>参数来生产快照，然后通过VisualVM或者<a href="http://www.eclipse.org/mat/" target="_blank" rel="noopener">MAT</a>等工具分析快照内容进行定位。通过这个参数是将发生OOM时的堆内存所有信息写入快照文件，也就是说，如果此时堆内存中有敏感信息的话，那就可能造成信息泄漏了。</p><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/java/gc-overhead-limit-exceeded/">java.lang.OutOfMemoryError:GC overhead limit exceeded</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/77934725" target="_blank" rel="noopener">java.lang.OutOfMemoryError:GC overhead limit exceeded</a></p>]]></content>
    
    <summary type="html">
    
      简单来说，java.lang.OutOfMemoryError\:\ GC overhead limit exceeded发生的原因是，当前已经没有可用内存，经过多次GC之后仍然没能有效释放内存。
    
    </summary>
    
    
      <category term="java" scheme="https://www.howardliu.cn/categories/java/"/>
    
      <category term="oom" scheme="https://www.howardliu.cn/categories/java/oom/"/>
    
    
      <category term="java" scheme="https://www.howardliu.cn/tags/java/"/>
    
      <category term="oom" scheme="https://www.howardliu.cn/tags/oom/"/>
    
  </entry>
  
  <entry>
    <title>中文字节长度引起的数据丢失</title>
    <link href="https://www.howardliu.cn/get-chinese-length/"/>
    <id>https://www.howardliu.cn/get-chinese-length/</id>
    <published>2017-08-30T07:43:05.000Z</published>
    <updated>2020-01-27T09:58:50.180Z</updated>
    
    <content type="html"><![CDATA[<p>最近在写一个应用监控的项目，使用netty作为数据传输。因为刚开始写，没有使用Protobuf之类的作为编码工具，只是使用的是netty自带的<code>LengthFieldBasedFrameDecoder</code>作为报文解析工具，自定义编码解码类，实现数据传输。</p><p>本来一切正常，结果在昨天测试的过程中，传输的数据体总是少16个字符，甚是奇怪。</p><a id="more"></a><p>翻来覆去查问题，后来仔细查看报文内容，才发现报文中有8个汉字。这才想到，中文字节长度不能使用<code>java.lang.String</code>的<code>length()</code>方法获取。应该使用的是<code>getBytes()</code>方法转成字节数组，在通过数组的<code>length</code>属性获取长度。</p><p>比如：</p><p><code>&quot;abcd&quot;.length()</code>的结果是：4<br><code>&quot;abcd&quot;.getBytes().length</code>的结果是：4<br><code>&quot;中国威武&quot;.length()</code>的结果是：4<br><code>&quot;中国威武&quot;.getBytes().length</code>的结果是：12<br><code>&quot;中国v5&quot;.length()</code>的结果是：4<br><code>&quot;中国v5&quot;.getBytes().length</code>的结果是：8</p><p>例子简单，但也能说明问题。在这里每个中文字节长度是3，英文字母、数字、英文标点是1。</p><p>所以在我的测试代码中，存在的8个汉字使用<code>length()</code>方法获取的长度是8，比<code>getBytes()</code>方法的字节数组长度少了16，所以在传输过程中总是少了16个字符（英文字符长度是1）。</p><p>总的来说，在对中文进行转换字节的时候一定要注意，千万不要想当然的使用<code>length()</code>方法。还是要根据具体情况多试试。<strong>特立文标记此错误</strong></p><p>下面附上代码：</p><p>编码器：MessageEncoder</p><pre><code class="java">import cn.howardliu.monitor.cynomys.net.struct.Message;import io.netty.buffer.ByteBuf;import io.netty.channel.ChannelHandlerContext;import io.netty.handler.codec.MessageToByteEncoder;import io.netty.util.CharsetUtil;public class MessageEncoder extends MessageToByteEncoder&lt;Message&gt; {    @Override    protected void encode(ChannelHandlerContext ctx, Message msg, ByteBuf out) throws Exception {        if (msg == null || msg.getHeader() == null) {            throw new IllegalArgumentException(&quot;the encode message is null.&quot;);        }        out.writeInt(msg.getHeader().getCrcCode());        out.writeInt(msg.getHeader().getLength());        out.writeInt(msg.getHeader().getOpaque());        out.writeInt(msg.getHeader().getTag().length());        out.writeCharSequence(msg.getHeader().getTag(), CharsetUtil.UTF_8);        out.writeInt(msg.getHeader().getSysName().length());        out.writeCharSequence(msg.getHeader().getSysName(), CharsetUtil.UTF_8);        out.writeInt(msg.getHeader().getSysCode().length());        out.writeCharSequence(msg.getHeader().getSysCode(), CharsetUtil.UTF_8);        out.writeByte(msg.getHeader().getType());        out.writeByte(msg.getHeader().getCode());        out.writeByte(msg.getHeader().getFlagPath());        if (msg.getBody() == null) {            out.writeInt(0);        } else {            out.writeInt(msg.getBody().getBytes().length);            out.writeCharSequence(msg.getBody(), CharsetUtil.UTF_8);        }        out.setInt(4, out.readableBytes() - 8);    }}</code></pre><p>解码器：MessageDecoder</p><pre><code class="java">import cn.howardliu.monitor.cynomys.net.struct.Header;import cn.howardliu.monitor.cynomys.net.struct.Message;import io.netty.buffer.ByteBuf;import io.netty.channel.ChannelHandlerContext;import io.netty.handler.codec.LengthFieldBasedFrameDecoder;import io.netty.util.CharsetUtil;public class MessageDecoder extends LengthFieldBasedFrameDecoder {    public MessageDecoder(int maxFrameLength, int lengthFieldOffset, int lengthFieldLength) {        super(maxFrameLength, lengthFieldOffset, lengthFieldLength);    }    @Override    protected Object decode(ChannelHandlerContext ctx, ByteBuf in) throws Exception {        ByteBuf frame = (ByteBuf) super.decode(ctx, in);        if (frame == null) {            return null;        }        Message message = new Message()                .setHeader(                        new Header()                                .setCrcCode(frame.readInt())                                .setLength(frame.readInt())                                .setOpaque(frame.readInt())                                .setTag(frame.readCharSequence(frame.readInt(), CharsetUtil.UTF_8).toString())                                .setSysName(frame.readCharSequence(frame.readInt(), CharsetUtil.UTF_8).toString())                                .setSysCode(frame.readCharSequence(frame.readInt(), CharsetUtil.UTF_8).toString())                                .setType(frame.readByte())                                .setCode(frame.readByte())                                .setFlagPath(frame.readByte())                );        if (frame.readableBytes() &gt; 4) {            message.setBody(frame.readCharSequence(frame.readInt(), CharsetUtil.UTF_8).toString());        }        return message;    }}</code></pre><p>解码器使用方式是<code>new MessageDecoder(1024 * 1024 * 100, 4, 4)</code>。</p><p>厚颜的贴上这个项目地址：</p><ul><li>项目名称：cynomys</li><li>项目地址：<a href="https://github.com/howardliu-cn/cynomys" target="_blank" rel="noopener">https://github.com/howardliu-cn/cynomys</a></li></ul><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/get-chinese-length/">中文字节长度引起的数据丢失</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/77717976" target="_blank" rel="noopener">中文字节长度引起的数据丢失</a></p>]]></content>
    
    <summary type="html">
    
      在对中文进行转换字节的时候一定要注意，千万不要想当然的使用`length()`方法。还是要根据具体情况多试试。
    
    </summary>
    
    
      <category term="bug" scheme="https://www.howardliu.cn/categories/bug/"/>
    
    
      <category term="bug" scheme="https://www.howardliu.cn/tags/bug/"/>
    
      <category term="中文字符" scheme="https://www.howardliu.cn/tags/%E4%B8%AD%E6%96%87%E5%AD%97%E7%AC%A6/"/>
    
  </entry>
  
  <entry>
    <title>ResourceManager HA 配置</title>
    <link href="https://www.howardliu.cn/hadoop/rm-ha/"/>
    <id>https://www.howardliu.cn/hadoop/rm-ha/</id>
    <published>2017-07-11T06:34:05.000Z</published>
    <updated>2020-01-27T10:02:07.547Z</updated>
    
    <content type="html"><![CDATA[<p>陆续的把<a href="https://www.howardliu.cn/hadoop/hadoop-environment-cluster-setup/">Hadoop集群部署</a>、<a href="https://www.howardliu.cn/hadoop/hdfs-ha-using-qjm/">HDFS的HA配置</a>完成，把<a href="https://www.howardliu.cn/hadoop/rm-ha/">ResourceManager的HA配置</a>好之后，Hadoop集群配置也算是完整了，可以满足小型中型生产环境Hadoop集群搭建的需要。如果真要搭建超大型的Hadoop集群，这些只能算是参考，还需要修改很多其他参数，使性能更好一些。</p><p>ResourceManager（RM）负责跟踪集群中资源使用情况，调度应用程序（比如MapReduce作业）。在Hadoop 2.4之前，ResourceManager存在单点故障，需要通过其他方式实现HA。官方给出的HA方案是Active/Standby两种状态ResourceManager的冗余方式，类似于HDFS的HA方案，也就是通过冗余消除单点故障。</p><a id="more"></a><h1 id="HA架构"><a href="#HA架构" class="headerlink" title="HA架构"></a>HA架构</h1><p>下图是ResourceManager HA方案架构图：</p><p><img src="https://www.howardliu.cn/images/hadoop/rm-ha-overview.png" alt="RM HA"></p><h2 id="RM故障转移"><a href="#RM故障转移" class="headerlink" title="RM故障转移"></a>RM故障转移</h2><p>ResourceManager HA是通过Active/Standby冗余架构实现的，在任何时间点，其中一个RM处于Active状态，其他RM处于Standby状态，Standby状态的RM就等着Active扑街或被撤。通过管理员命令或自动故障转移（需要开启自动故障转移配置），Standby就会转为Active状态，对外提供服务。</p><ul><li>手动转换和故障转移：当未启用自动故障转移时，就需要管理员手动转换。首先将Active状态的RM转为Standby状态，然后将一个Standby状态的转为Active状态。这些操作都需要通过<code>yarn rmadmin</code>命令来操作。</li><li>自动故障转移：RM可以通过内嵌的基于Zookeeper的Active/Standby选择器决定哪个RM应该是Active状态的。当Active性能下降或无响应时，一个Standby状态的RM就被推举出来，转为Active状态接管。这里不需要像HDFS的HA配置需要一个单独的ZKFS守护进程辅助完成切换，因为这个功能已经内嵌在RM中。</li></ul><p>客户端、ApplicationMaster和NodeManager在故障转移时，会轮训这些RM节点，知道找到Active状态的RM。如果Active节点性能下降，他们会重新轮训查找新的Active状态的RM。默认的轮训扩展是<code>org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider</code>。可以通过实现<code>org.apache.hadoop.yarn.client.RMFailoverProxyProvider</code>，并配置<code>yarn.client.failover-proxy-provider</code>来实现自己的逻辑。</p><h2 id="恢复RM状态"><a href="#恢复RM状态" class="headerlink" title="恢复RM状态"></a>恢复RM状态</h2><p>当启用<a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html" target="_blank" rel="noopener">ResourceManger重启状态恢复</a>之后，新的Active状态的RM会加载上一个RM状态，并根据状态尽可能的恢复之前的操作。应用程序会定期检查，以避免丢失数据。状态存储需要对Active状态和Standby状态的RM都可见。目前，<code>RMStateStore</code>有两个持久化实现，<code>FileSystemRMStateStore</code>和<code>ZKRMStateStore</code>。<code>ZKRMStateStore</code>隐式的只允许一个RM写入操作，可以没有单独的防护机制就能够避免闹裂问题，所以是HA集群推荐的状态存储方式。使用<code>ZKRMStateStore</code>时，建议不要在zookeeper集群上设置<code>zookeeper.DigestAuthenticationProvider.superDigest</code>配置，以确保zk管理员无法访问YARN的信息。</p><h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>大多数的故障转移功能可以使用各种配置进行调整，下表是必须的和重要的参数项。完整的配置和默认值参见<a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" target="_blank" rel="noopener">yarn-default.xml</a>。关于状态存储参见<a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html" target="_blank" rel="noopener">ResourceManger状态存储</a>。</p><table><thead><tr><th>配置项</th><th>描述</th></tr></thead><tbody><tr><td>yarn.resourcemanager.zk-address</td><td>zookeeper集群地址，用于状态存储和内部的leader选举</td></tr><tr><td>yarn.resourcemanager.ha.enabled</td><td>开启RM的HA</td></tr><tr><td>yarn.resourcemanager.ha.rm-ids</td><td>RM逻辑id列表，以逗号分割，比如：rm1,rm2。</td></tr><tr><td>yarn.resourcemanager.hostname.[rm-id]</td><td>对于每个rm-id，需要给出hostname或ip地址。</td></tr><tr><td>yarn.resourcemanager.address.[rm-id]</td><td>对于每个rm-id，指定host:port地址，该配置会覆盖yarn.resourcemanager.hostname.rm-id。</td></tr><tr><td>yarn.resourcemanager.scheduler.address.[rm-id]</td><td>对于每个rm-id，指定ApplicationMasters申请资源的Scheduler的host:port地址，该配置会覆盖yarn.resourcemanager.hostname.rm-id。</td></tr><tr><td>yarn.resourcemanager.resource-tracker.address.[rm-id]</td><td>对于每个rm-id，指定NodeManager连接的host:port地址，该配置会覆盖yarn.resourcemanager.hostname.rm-id。</td></tr><tr><td>yarn.resourcemanager.admin.address.[rm-id]</td><td>对于每个rm-id，指定管理命令操作的host:port地址，该配置会覆盖yarn.resourcemanager.hostname.rm-id。</td></tr><tr><td>yarn.resourcemanager.webapp.address.[rm-id]</td><td>对于每个rm-id，指定RM的web应用host:port地址，如果设置yarn.http.policy是HTTPS_ONLY，就没必要设置该参数。该参数会覆盖yarn.resourcemanager.hostname.rm-id。</td></tr><tr><td>yarn.resourcemanager.webapp.https.address.[rm-id]</td><td>对于每个rm-id，指定RM的web应用host:port地址，如果设置yarn.http.policy是HTTP_ONLY，就没必要设置该参数。该参数会覆盖yarn.resourcemanager.hostname.rm-id。</td></tr><tr><td>yarn.resourcemanager.ha.id</td><td>用于识别HA的RM，可选配置。如果设置，需要确定所有的RM都有自己的ID。</td></tr><tr><td>yarn.resourcemanager.ha.automatic-failover.enabled</td><td>启用自动故障转移; 默认情况下，仅当HA被启用时才启用。</td></tr><tr><td>yarn.resourcemanager.ha.automatic-failover.embedded</td><td>启用自动故障转移时，使用嵌入式leader选举选择Active RM。 默认情况下，仅当HA被启用时才启用。</td></tr><tr><td>yarn.resourcemanager.cluster-id</td><td>集群标志，用于保证RM不会成为另一个集群的Active节点。</td></tr><tr><td>yarn.client.failover-proxy-provider</td><td>客户端使用，用于客户端、ApplicationMaster、NodeManager连接到新的Active的RM。</td></tr><tr><td>yarn.client.failover-max-attempts</td><td>FailoverProxyProvider应该尝试的最大次数。</td></tr><tr><td>yarn.client.failover-sleep-base-ms</td><td>用于计算故障转移的休眠基准（单位是毫秒）。</td></tr><tr><td>yarn.client.failover-sleep-max-ms</td><td>故障转移休眠最长时间（单位是毫秒）。</td></tr><tr><td>yarn.client.failover-retries</td><td>尝试连接到RM的重试次数。</td></tr><tr><td>yarn.client.failover-retries-on-socket-timeouts</td><td>尝试连接到RM中可允许超时连接的次数。</td></tr></tbody></table><p>配置示例（配置承接<a href="https://www.howardliu.cn/hadoop/hadoop-environment-cluster-setup/">hadoop集群部署(yarn)</a>中的，使用s107和s108作为RM双节点）：</p><pre><code class="xml">&lt;!--Configurations for the state-store of ResourceManager--&gt;&lt;property&gt;  &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;  &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;  &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;  &lt;value&gt;10.6.3.109:2181,10.6.3.110:2181,10.6.3.111:2181&lt;/value&gt;  &lt;description&gt;ZooKeeper服务的地址，多个地址使用逗号隔开&lt;/description&gt;&lt;/property&gt;&lt;!--Configurations for HA of ResourceManager--&gt;&lt;property&gt;  &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;  &lt;value&gt;true&lt;/value&gt;  &lt;description&gt;是否启用HA，默认false&lt;/description&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;  &lt;value&gt;rm1,rm2&lt;/value&gt;  &lt;description&gt;最少2个&lt;/description&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;  &lt;value&gt;s107&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;  &lt;value&gt;s108&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;  &lt;value&gt;yarn-ha&lt;/value&gt;  &lt;description&gt;集群HA的id，用于在ZooKeeper上创建节点，区分使用同一个ZooKeeper集群的不同Hadoop集群&lt;/description&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;    &lt;value&gt;s107:8088&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;    &lt;value&gt;s108:8088&lt;/value&gt;&lt;/property&gt;</code></pre><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>可以在s107上直接通过<code>start-yarn.sh</code>启动YARN，这样在s107上会启动ResourceManager，在其他节点上会启动NodeManager。</p><p>需要注意的是s108上不会自己启动ResourceManager，需要手动启动。通过命令<code>yarn-daemon.sh start resourcemanager</code>手动启动。</p><h2 id="管理命令"><a href="#管理命令" class="headerlink" title="管理命令"></a>管理命令</h2><p>对于YARN的管理前面又说到，用的命令是<code>yarn rmadmin</code>，可以检查RM的健康状态、转换Active/Standby状态等，需要使用<code>yarn.resourcemanager.ha.rm-ids</code>参数配置的RM的id作为参数。比如，查看RM状态：</p><pre><code>$ yarn rmadmin -getServiceState rm1active$ yarn rmadmin -getServiceState rm2standby</code></pre><p>其他的命令可以通过<code>yarn rmadmin -help</code>获取。</p><h2 id="Web管理页面"><a href="#Web管理页面" class="headerlink" title="Web管理页面"></a>Web管理页面</h2><p>管理界面就是yarn.resourcemanager.webapp.address.[rm-id]配置的地址，如果访问的是Standby的RM地址，会自动重定向到Active状态的RM地址。About页面除外，可以访问About页面查看当前哪个节点是Active状态，哪个是Standby状态的。</p><hr><p>参考文章</p><ol><li><a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html" target="_blank" rel="noopener">ResourceManager High Availability</a></li></ol><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/hadoop/rm-ha/">ResourceManager HA 配置</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/74984123" target="_blank" rel="noopener">ResourceManager HA 配置</a></p>]]></content>
    
    <summary type="html">
    
      ResourceManager（RM）负责跟踪集群中资源使用情况，调度应用程序（比如MapReduce作业）。在Hadoop 2.4之前，ResourceManager存在单点故障，需要通过其他方式实现HA。官方给出的HA方案是Active/Standby两种状态ResourceManager的冗余方式，类似于HDFS的HA方案，也就是通过冗余消除单点故障。
    
    </summary>
    
    
      <category term="hadoop" scheme="https://www.howardliu.cn/categories/hadoop/"/>
    
      <category term="rm" scheme="https://www.howardliu.cn/categories/hadoop/rm/"/>
    
    
      <category term="大数据" scheme="https://www.howardliu.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="hadoop" scheme="https://www.howardliu.cn/tags/hadoop/"/>
    
      <category term="开源" scheme="https://www.howardliu.cn/tags/%E5%BC%80%E6%BA%90/"/>
    
      <category term="ha" scheme="https://www.howardliu.cn/tags/ha/"/>
    
      <category term="rm" scheme="https://www.howardliu.cn/tags/rm/"/>
    
  </entry>
  
  <entry>
    <title>YARN架构</title>
    <link href="https://www.howardliu.cn/hadoop/yarn-architecture/"/>
    <id>https://www.howardliu.cn/hadoop/yarn-architecture/</id>
    <published>2017-07-11T01:41:10.000Z</published>
    <updated>2020-01-27T10:02:25.144Z</updated>
    
    <content type="html"><![CDATA[<p>对Hadoop有过了解的都知道，Hadoop经历过很长一段时间的版本号混乱和架构调整，YARN是Hadoop 2.0（或者早期的0.23.x）提出的资源管理、任务调度框架。解决了很多Hadoop 1.0（或者0.21.x、0.22.x）时代的痛点。</p><p>随着发展，YARN不仅仅是Hadoop的资源调度框架，还成为一个通用的资源调度管理器，可以将各种各样的计算框架通过YARN管理起来，比如Strom、Spark等。</p><p>YARN的基本思想是将资源管理和作业调度/监控的功能分为独立的守护进程。分别是一个全局的 ResourceManager（RM） 和每个应用程序的 ApplicationMaster（AM）。应用程序可以是一个job作业或者一组job作业的有向无环图（DAG）。</p><p>ResourceManager负责系统中的所有应用程序的资源分配。NodeManager负责每台机器中容器代理、资源监控（cpu，内存，磁盘，网络），并将这些情况报告给ResourceManager或Scheduler。</p><p>每个应用的ApplicationMaster是一个框架特定的库，从ResourceManager协商资源，并与NodeManager共同执行监听任务。</p><p>从结构上看，YARN是主/从架构，一个ResourceManager，多个NodeManager，共同构成了数据计算框架。</p><a id="more"></a><p><img src="https://www.howardliu.cn/images/hadoop/yarn_architecture.gif" alt="YARN架构" title="https://www.howardliu.cn YARN架构"></p><p>从上面的结构图来看，YARN主要的组件包括ResourceManager、NodeManager、ApplicationMaster和Container。</p><h1 id="1-ResourceManager（RM）"><a href="#1-ResourceManager（RM）" class="headerlink" title="1. ResourceManager（RM）"></a>1. ResourceManager（RM）</h1><p>ResourceManager负责整个集群的资源管理和分配，包括处理客户端请求、启动和监控ApplicationMaster、监控NodeManager、资源的分配和调度。</p><p>ResourceManager由两个主要组件组成：Scheduler和ApplicationsManager（ASM）。</p><ul><li><p>Scheduler：Scheduler根据容量、队列等限制条件（每个队列分配多少资源、最多执行多少个作业等），向运行的应用程序分配资源。Scheduler是一个单纯的调度器，不负责监控功能或跟踪应用程序状态。另外，如果因为应用程序错误或硬件故障任务失败，它不保证重新启动任务。这些都交给ApplicationMaster完成。</p><p>资源分配单位用一个抽象概念“容器”（Container）表示，容器是一个动态资源分配单位，将诸如内存、cpu、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量</p><p>Scheduler是一个可插拔组件，可以根据自己的需要重新定义。YARN提供了已经实现了多种Scheduler，比如<a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html" target="_blank" rel="noopener">CapacityScheduler</a>和<a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/FairScheduler.html" target="_blank" rel="noopener">FairScheduler</a>。</p></li><li><p>ApplicationsManager：ApplicationsManager负责接收提交的作业，与第一个容器协商来执行应用程序对应的ApplicationMaster，并在容器失败时重启ApplicationMaster。每个应用程序对应的ApplicationMaster负责从Scheduler协商资源容器，并跟踪应用程序状态、监控执行进度。</p></li></ul><h1 id="2-NodeManager（NM）"><a href="#2-NodeManager（NM）" class="headerlink" title="2. NodeManager（NM）"></a>2. NodeManager（NM）</h1><p>NodeManager是YARN集群中每个节点上资源和任务管理器，负责当前节点程序的运行、资源的管理和监控。</p><p>NodeManager定时向ResourceManager发送本节点资源使用情况、容器运行状态等信息。同时，NodeManager需要执行ResourceManager和ApplicationMaster的命令。</p><h1 id="3-ApplicationMaster（AM）"><a href="#3-ApplicationMaster（AM）" class="headerlink" title="3. ApplicationMaster（AM）"></a>3. ApplicationMaster（AM）</h1><p>YARN运行的每个应用程序都会有一个ApplicationMaster。负责协调来自ResourceManager的资源，并通过NodeManager监控容器和资源使用（包括内存、CPU等）。</p><h1 id="4-YARN工作流程"><a href="#4-YARN工作流程" class="headerlink" title="4. YARN工作流程"></a>4. YARN工作流程</h1><ol><li>客户端向ResourceManager提交应用程序，其中包括ApplicationMaster、启动ApplicationMaster的命令、用户程序等；</li><li>ResourceManager为该应用程序分配第一个Container，并与对应NodeManager通信，要求它在这个Container中启动应用程序的ApplicationMaster；</li><li>ApplicationMaster向ResourceManager注册自己，启动成功后与ResourceManager保持心跳；</li><li>ApplicationMaster向ResourceManager申请资源；</li><li>申请资源成功后，由ApplicationMaster进行初始化，然后与NodeManager通信，要求NodeManager启动Container。然后ApplicationMaster与NodeManager保持心跳，从而对NodeManager上运行的任务进行监控和管理；</li><li>Container运行期间，向ApplicationMaster汇报自己的进度和状态信息，以便ApplicationMaster掌握任务运行状态，从而在任务失败是可以重新启动；</li><li>应用运行结束后，ApplicationMaster向ResourceManager注销自己，允许其所属的Container回收。</li></ol><hr><p>参考</p><ol><li><a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">Apache Hadoop YARN</a></li><li><a href="https://hortonworks.com/blog/introducing-apache-hadoop-yarn/" target="_blank" rel="noopener">INTRODUCING APACHE HADOOP YAR</a></li></ol><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/hadoop/yarn-architecture/">YARN架构</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/74939382" target="_blank" rel="noopener">YARN架构</a></p>]]></content>
    
    <summary type="html">
    
      YARN的基本思想是将资源管理和作业调度/监控的功能分为独立的守护进程。这样就出现了一个全局的 ResourceManager（RM） 和每个应用程序的 ApplicationMaster（AM）。应用程序可以是一个job作业或者一组job作业的有向无环图（DAG）。ResourceManager负责系统中的所有应用程序的资源分配。NodeManager负责每台机器中容器代理、资源监控（cpu，内存，磁盘，网络），并将这些情况报告给ResourceManager或Scheduler。每个应用的ApplicationMaster是一个框架特定的库，从ResourceManager协商资源，并与NodeManager共同执行监听任务。
    
    </summary>
    
    
      <category term="hadoop" scheme="https://www.howardliu.cn/categories/hadoop/"/>
    
      <category term="yarn" scheme="https://www.howardliu.cn/categories/hadoop/yarn/"/>
    
    
      <category term="大数据" scheme="https://www.howardliu.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="hadoop" scheme="https://www.howardliu.cn/tags/hadoop/"/>
    
      <category term="yarn" scheme="https://www.howardliu.cn/tags/yarn/"/>
    
      <category term="开源" scheme="https://www.howardliu.cn/tags/%E5%BC%80%E6%BA%90/"/>
    
  </entry>
  
  <entry>
    <title>使用QJM实现HDFS的HA</title>
    <link href="https://www.howardliu.cn/hadoop/hdfs-ha-using-qjm/"/>
    <id>https://www.howardliu.cn/hadoop/hdfs-ha-using-qjm/</id>
    <published>2017-07-07T08:24:23.000Z</published>
    <updated>2020-01-27T10:01:20.230Z</updated>
    
    <content type="html"><![CDATA[<p>本文是在<a href="https://www.howardliu.cn/hadoop/hadoop-environment-cluster-setup/">hadoop集群部署(yarn)</a>基础上增加的配置内容，因为那篇缺少HDFS的HA配置，在生产环境不够完整。</p><p>hadoop官方提供了两种HDFS的HA配置方案，两种方案殊途同归，但是需要的钱、精力和技术不同。</p><p>如果对HDFS架构熟悉的话（如果不熟悉，可以通过<a href="https://www.howardliu.cn/hadoop/hdfs-architecture/">HDFS架构</a>了解），就应该知道，NameNode通过FsImage和EditLog两个文件管理DataNode的数据，Secondary NameNode会定期合并EditLog，以减少NameNode启动时的安全检查。EditLog文件存储的是对文件的一条条的操作，也就是说，只要保证有另外一个NameNode的EditLog文件一直与当前正在运行的NameNode的EditLog文件是一样的，那就可以随时使用新的NameNode替换老的NameNode。官方目前给出的两种HA方案也大体是这样：</p><ul><li>QJM：the Quorum Journal Manager，翻译是法定经济管理人，实在没法想象，所以大家都亲切的称之为QJM。这种方案是通过JournalNode共享EditLog的数据，使用的是Paxos算法（没错，zookeeper就是使用的这种算法），保证活跃的NameNode与备份的NameNode之间EditLog日志一致。</li><li>NFS：Network File System 或 Conventional Shared Storage，传统共享存储，其实就是在服务器挂载一个网络存储（比如NAS），活跃NameNode将EditLog的变化写到NFS，备份NameNode检查到修改就读取过来，是两个NameNode数据一致。</li></ul><blockquote><p>客观的说，Secondary NameNode也算是对NameNode的备份，但是使用Secondary NameNode需要手动处理，不如QJM和NFS两种可以自动处理简单，所以没有被列入HA解决方案中。</p></blockquote><p>但是，这两种方案在部署方式上差别比较大。QJM需要启动几个JournalNode即可，NFS需要挂在一个共享存储。因为条件限制，我只能通过QJM的方式实现HDFS的HA，如果想看NFS方案，可以直接看<a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html" target="_blank" rel="noopener">官方文档</a>。</p><a id="more"></a><h1 id="1-hdfs-site-xml"><a href="#1-hdfs-site-xml" class="headerlink" title="1. hdfs-site.xml"></a>1. hdfs-site.xml</h1><ul><li><p>dfs.nameservices：指定nameservice的名称，这个需要自定义，可以是任意的名称。这个值需要用在后面的配置和HDFS集群管理路径中。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.nameservices&lt;/name&gt;&lt;value&gt;mycluster&lt;/value&gt;&lt;/property&gt;</code></pre></li><li><p>dfs.ha.namenodes.[nameservice ID]：指定集群中两个NameNode的id，目前只能支持最多两个NameNode，所以就需要两个id，以逗号分隔。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;&lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;</code></pre></li><li><p>dfs.namenode.rpc-address.[nameservice ID].[namenode ID]：指定NameNode的rpc地址，用于数据传输。因为有两个NameNode，所以需要给出两个节点。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;&lt;value&gt;s108:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;&lt;value&gt;s109:8020&lt;/value&gt;&lt;/property&gt;</code></pre></li><li><p>dfs.name.http-address.[nameservice ID].[namenode ID]：同3，还需要http地址。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;&lt;value&gt;s108:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;&lt;value&gt;s109:50070&lt;/value&gt;&lt;/property&gt;</code></pre></li><li><p>dfs.namenode.shared.edits.dir：需要提供JournalNode的配置地址，用于活跃NameNode向该位置写变化数据，备份NameNode从该位置读取数据应用与自身。如果配置过Kafka就应该可以理解这个。配置地址格式是：qjournal://host1:port1;hots2:port2;host3:port3/journalId，地址端口为一对，每对之间通过分号隔开，最后的journalId是为了区分不同的nameservice的。也就是说，一组JournalNode可以支撑多个NameNode的HA配置。所以，比较好的配置方式是，journalId与nameservice的名称一致。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;&lt;value&gt;qjournal://s108:8485;s109:8485;s110:8485/mycluster&lt;/value&gt;&lt;/property&gt;</code></pre></li><li><p>dfs.client.failover.proxy.provider.[nameservice ID]：HDFS客户端连接活跃NameNode的方式，配置一个Java类。因为NameNode只有一个是活跃的，也就是只有一个提供服务，另一个是备份。所以客户端需要知道哪个是活跃节点。所以需要某种方式找到这个活跃节点。这里提供一个代理类，目前Hadoop只实现了一个代理类<code>ConfiguredFailoverProxyProvider</code>，也可以自己定义：</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt;</code></pre></li><li><p>dfs.ha.fencing.methods：用于故障转移过程中，在活跃节点执行的一组脚本或Java类。HDFS集群有一条原则是：只能有一个NameNode处于活跃状态。QJM只允许一个NameNode写入JournalNode集群，所以可以避免闹裂的发生。但是故障转移过程中，还可能会有其他的问题，所以需要提供一些防护方法。需要注意的是，如果不想使用具体的防护方法，也必须提供一个脚本，比如<code>shell(/bin/true)</code>。</p><ul><li><p>sshfence：通过ssh方式连接活跃NameNode，并kill掉进程。所以还需要通过<code>dfs.ha.fencing.ssh.private-key-files</code>配置ssh key，还可以通过<code>dfs.ha.fencing.ssh.connect-timeout</code>配置ssh连接超时时间。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;&lt;value&gt;sshfence&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;&lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;&lt;value&gt;30000&lt;/value&gt;&lt;/property&gt;</code></pre><p>如果对于不是标准ssh端口或相同用户的，可以在sshfence后添加用户名和端口，格式为<code>sshfence([[username][:port]])</code>。</p></li><li><p>shell：运行任意的脚本来进行防护。我是使用sshfence方式配置的，所以下面就列出配置格式，具体信息查看<a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="noopener">官网</a>。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;&lt;value&gt;shell(/path/to/my/script.sh arg1 arg2 ...)&lt;/value&gt;&lt;/property&gt;</code></pre></li></ul></li><li><p>dfs.journalnode.edits.dir：JournalNode守护进程存储数据的本地路径。这是启动JournalNode需要配置的配置项。当然整个集群配置相同也不会有不好的影响，需要是本地绝对路径。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;&lt;value&gt;/data/hadoop/journal&lt;/value&gt;&lt;/property&gt;</code></pre></li><li><p>dfs.ha.automatic-failover.enabled：自动故障转移，该配置向需要与core-site.xml中的<code>ha.zookeeper.quorum</code>配合使用。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;</code></pre></li></ul><h1 id="2-core-site-xml"><a href="#2-core-site-xml" class="headerlink" title="2. core-site.xml"></a>2. core-site.xml</h1><ul><li><p>fs.defaultFS：这个在单点NameNode的时候配置过，这里需要再次配置，需要使用hdfs-site.xml中的nameservice名称。</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://mycluster&lt;/value&gt;&lt;/property&gt;</code></pre></li><li><p>ha.zookeeper.quorum：这个就是前面提到hdfs-site.xml中配置自动故障转移配合使用的配置项，需要提供zookeeper集群地址</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;&lt;value&gt;s109:2181,s110:2181,s111:2181&lt;/value&gt;&lt;/property&gt;</code></pre></li></ul><h1 id="3-开始启动"><a href="#3-开始启动" class="headerlink" title="3. 开始启动"></a>3. 开始启动</h1><h2 id="3-1-JournalNode"><a href="#3-1-JournalNode" class="headerlink" title="3.1 JournalNode"></a>3.1 JournalNode</h2><p>需要首先启动JournalNode，如上面配置的，需要s108/s109/s110三个节点启动JournalNode，默认端口就是8045。启动命令是<code>hadoop-daemon.sh start journalnode</code>。</p><h2 id="3-2-NameNode数据准备"><a href="#3-2-NameNode数据准备" class="headerlink" title="3.2 NameNode数据准备"></a>3.2 NameNode数据准备</h2><p>JournalNode启动完成后，因为有两个NameNode节点，就需要先同步两个NameNode节点的数据。</p><ol><li>如果是全新的HDFS集群，这个时候直接<code>hdfs namenode -format</code>格式化即可</li><li>已经格式化或是从非HA设置为HA的集群，需要把格式化后的NameNode节点的数据拷贝到为格式化节点上。未格式化NameNode节点执行<code>hdfs namenode -bootstrapStandby</code>命令。</li><li>如果是从非HA到HA的配置，需要执行<code>hdfs namenode -initializeSharedEdits</code>将原有的NameNode日志写入JournalNode中。</li></ol><h2 id="3-3-Zookeeper中的HA状态"><a href="#3-3-Zookeeper中的HA状态" class="headerlink" title="3.3 Zookeeper中的HA状态"></a>3.3 Zookeeper中的HA状态</h2><p>因为上面配置了自动故障转移，所以需要在Zookeeper中初始化HA状态。执行命令<code>hdfs zkfc -formatZK</code>。</p><h2 id="3-4-启动"><a href="#3-4-启动" class="headerlink" title="3.4 启动"></a>3.4 启动</h2><p>直接使用<code>start-dfs.sh</code>命令启动NameNode、DataNode，以及ZKFS进程，启动成功之后就可以通过s108:50070和s109:50070访问web页面查看具体哪个NameNode是Active或Standby状态的了。</p><blockquote><p>启动的时候可以注意到，启动过程没有启动Secondary NameNode，这是用为HA不会启动Secondary NameNode。也就是master配置文件配置内容无效了。</p></blockquote><h1 id="4-管理"><a href="#4-管理" class="headerlink" title="4. 管理"></a>4. 管理</h1><p>可以通过<code>hdfs haadmin</code>命令进行管理。具体查看<a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="noopener">官网</a>说明。</p><hr><p>参考</p><ol><li><a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="noopener">HDFS High Availability Using the Quorum Journal Manager</a></li><li><a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html" target="_blank" rel="noopener">HDFS High Availability</a></li></ol><hr><p>个人主页: <a href="https://www.howardliu.cn">https://www.howardliu.cn</a><br>个人博文: <a href="https://www.howardliu.cn/hadoop/hdfs-ha-using-qjm/">使用QJM实现HDFS的HA</a><br>CSDN主页: <a href="http://blog.csdn.net/liuxinghao" target="_blank" rel="noopener">http://blog.csdn.net/liuxinghao</a><br>CSDN博文: <a href="http://blog.csdn.net/liuxinghao/article/details/74910417" target="_blank" rel="noopener">使用QJM实现HDFS的HA</a></p>]]></content>
    
    <summary type="html">
    
      hadoop官方提供了两种HDFS的HA配置方案，两种方案殊途同归，但是需要的钱、精力和技术不同。如果对HDFS架构熟悉的话，就应该知道，NameNode通过FsImage和EditLog两个文件管理DataNode的数据，Secondary NameNode会定期合并EditLog，以减少NameNode启动时的安全检查。EditLog文件存储的是对文件的一条条的操作，也就是说，只要保证有另外一个NameNode的EditLog文件一直与当前正在运行的NameNode的EditLog文件是一样的，那就可以随时使用新的NameNode替换老的NameNode。官方目前给出的两种HA方案也大体是这样。QJM：the Quorum Journal Manager，翻译是法定经济管理人，实在没法想象，所以大家都亲切的称之为QJM。这种方案是通过JournalNode共享EditLog的数据，使用的是Paxos算法（没错，zookeeper就是使用的这种算法），保证活跃的NameNode与备份的NameNode之间EditLog日志一致。NFS：Network File System 或 Conventional Shared Storage，传统共享存储，其实就是在服务器挂载一个网络存储（比如NAS），活跃NameNode将EditLog的变化写到NFS，备份NameNode检查到修改就读取过来，是两个NameNode数据一致。
    
    </summary>
    
    
      <category term="hadoop" scheme="https://www.howardliu.cn/categories/hadoop/"/>
    
      <category term="hdfs" scheme="https://www.howardliu.cn/categories/hadoop/hdfs/"/>
    
    
      <category term="大数据" scheme="https://www.howardliu.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="hadoop" scheme="https://www.howardliu.cn/tags/hadoop/"/>
    
      <category term="开源" scheme="https://www.howardliu.cn/tags/%E5%BC%80%E6%BA%90/"/>
    
      <category term="hdfs" scheme="https://www.howardliu.cn/tags/hdfs/"/>
    
      <category term="ha" scheme="https://www.howardliu.cn/tags/ha/"/>
    
      <category term="QJM" scheme="https://www.howardliu.cn/tags/QJM/"/>
    
  </entry>
  
</feed>
